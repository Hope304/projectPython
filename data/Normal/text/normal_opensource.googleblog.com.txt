Google Open Source Blog opensource.google.com Menu Events Projects Programs and services Documentation About Blog Google Open Source Blog The latest news from Google on open source releases, major projects, events, and student outreach programs. Mentor organizations announced for Google Summer of Code 2024 Wednesday, February 21, 2024 We are thrilled to share that we have 195 open source projects that have been selected for Google Summer of Code (GSoC) 2024! This year we are excited to welcome 30 new organizations for their first year as part of the program. Check out our program site to view the complete list of GSoC 2024 accepted mentoring organizations. Get to know more about each organization on their GSoC program page, which includes reading through the project ideas that they are looking for GSoC contributors to work on this year. Are you interested in being a GSoC Contributor? The 2024 GSoC program is open to students and to beginners in open source software development. Contributor applications will open on Monday, March 18, 2024 at 18:00 UTC with a deadline of Tuesday, April 2, 2024 18:00 UTC to submit your application (including your project proposal). If you are eager to enhance your chances of becoming a successful contributor this year, we highly recommend beginning your preparations and initiating communication with the organizations that interest you right away. Below are some tips for prospective GSoC contributors to accomplish before the application period begins March 18th: Watch our ‘Introduction to GSoC’ video to see a quick overview of the program, and view our Community Talks or Org Highlight Videos to get inspired and learn more about some projects that contributors have worked on in the past. Check out the Contributor Guide (so much great info in here!) and Advice for Applying to GSoC doc. Review the list of accepted organizations here. We recommend finding two to four that interest you and reading through their project ideas lists. Use the filters on the site to help you narrow down based on the programming languages you are familiar with and the categories that interest you (cloud, AI, security, science, etc.). As soon as you see an idea that sparks your interest, reach out to the organization via their preferred communication methods (listed on their org page on the GSoC program site). The earlier you start the conversation, the better your chances of being accepted as a GSoC contributor. Talk with the mentors and community to determine if this project idea is something you would enjoy working on during the program. Find a project that excites you, otherwise it may be a challenging summer for you and your mentor. Use the information you received during your communications with the mentors and other org community members to write up your proposal. You can find more information about the program on our website which includes a full timeline of important dates. We also urge anyone interested in applying to read the FAQ and Program Rules and watch some of our other videos with more details about GSoC for contributors and mentors. A hearty welcome—and thank you—to all of our mentor organizations. We look forward to working with all of you during this 20th year of Google Summer of Code! By Stephanie Taylor – Google Open Source Building Open Models Responsibly in the Gemini Era Google has long believed that open technology is not only good for our company, but good for the industry, consumers, and the world. We’ve released open-source projects like Android and Chromium that transformed access to mobile and web technologies, and have done the same in AI with Transformers, TensorFlow, and AlphaFold. The release of our Gemma family of open models is a next step in how we’re deepening our commitment to open technology alongside an industry-leading safe, responsible approach. At the same time, the rapidly evolving nature of AI raises important considerations for how to enable safety-aligned open models: an approach that supports broad innovation while promoting safe uses. A benefit of open source is that once it is released, its license gives users full creative autonomy. This is a powerful guarantee of technology access for developers and end users. Another benefit is that open-source technology can be modified to fit the unique use case of the end user, without restriction. In the hands of a malicious actor, however, the lack of restrictions can raise risks. Computing has been through similar cycles before, addressing issues such as protecting users of the open internet, handling cryptography, and addressing open-source software security. We now face this challenge with AI. Below we share the approach we took to openly releasing Gemma models, and the advancements in open model safety we hope to accelerate. Providing access to Gemma open models Today, Gemma models are being released as what the industry collectively has begun to refer to as “open models.” Open models feature free access to the model weights, but terms of use, redistribution, and variant ownership vary according to a model’s specific terms of use, which may not be based on an open-source license. The Gemma models’ terms of use make them freely available for individual developers, researchers, and commercial users for access and redistribution. Users are also free to create and publish model variants. In using Gemma models, developers agree to avoid harmful uses, reflecting our commitment to developing AI responsibly while increasing access to this technology. We’re precise about the language we’re using to describe Gemma models because we’re proud to enable responsible AI access and innovation, and we’re equally proud supporters of open source. The definition of "Open Source" has been invaluable to computing and innovation because of requirements for redistribution and derived works, and against discrimination. These requirements enable cross-industry collaboration, individual innovation and entrepreneurship, and shared research to happen with exponential effects. However, existing open-source concepts can’t always be directly applied to AI systems, which raises questions on how to use open-source licenses with AI. It’s important that we carry forward open principles that have made the sea-change we’re experiencing with AI possible while clarifying the concept of open-source AI and addressing concepts like derived work and author attribution. Taking a comprehensive approach to releasing Gemma safely and responsibly Licensing and terms of use are only one part of the evaluations, technical tools, and considered decision-making that went into aligning this release with our responsible AI Principles. Our approach involved: Systematic internal review in accordance with our AI Principles: Consistent with our AI Principles, we release models only when we have determined the benefits are significant, and the risks of misuse are low or can be mitigated. We take that same approach to open models, incorporating a balance of the benefits of wider access to a particular model as well as the risks of misuse and how we can mitigate them. With Gemma, we considered the increased AI research and innovation by us and many others in the community, the access to AI technology the models could bring, and what access was needed to support these use cases. A high evaluation bar: Gemma models underwent thorough evaluations, and were held to a higher bar for evaluating risk of abuse or harm than our proprietary models, given the more limited mitigations currently available for open models. These evaluations cover a broad range of responsible AI areas, including safety, fairness, privacy, societal risk, as well as capabilities such as chemical, biological, radiological, nuclear (CBRN) risks, cybersecurity, and autonomous replication. As described in our technical report, the Gemma models exhibit state-of-the-art safety performance in human side-by-side evaluations. Responsibility tools for developers: As we release the Gemma models, we are also releasing a Responsible Generative AI Toolkit for developers, providing guidance and tools to help them create safer AI applications. We continue to evolve our approach. As we build these frameworks further, we will proceed thoughtfully and incorporate what we learn into future model assessments. We will continue to explore the full range of access mechanisms, with benefits and risk mitigation in mind, including API-based access and staged releases. Advancing open model safety together Many of today’s AI safety tools are designed for systems where the design approach assumes restricted access and redistribution, as well as auxiliary controls like query filters. Similarly, much of the AI safety research for improving mitigations takes on the design assumptions of those systems. Just as we have created unique threat models and solutions for other open technology, we are developing safety and security tools appropriate for the differences of openly available AI. As models become more and more capable, we are conducting research and investing in rigorous safety evaluation, testing, and mitigations for open models. We are also actively participating in conversations with policymakers and open-source community leaders on how the industry should approach this technology. This challenge is multifaceted, just like AI systems themselves. Model-sharing platforms like Hugging Face and Kaggle, where developers inspire each other with novel model iterations, play a critical role in efforts to develop open models safely; there is also a role for the cybersecurity community to contribute learnings and best practices. Building those solutions requires access to open models, sharing innovations and improvements. We believe sharing the Gemma models will not just help increase access to AI technology, but also help the industry develop new approaches to safety and responsibility. As developers adopt Gemma models and other safety-aligned open models, we look forward to working with the open-source community to develop more solutions for responsible approaches to AI in the open ecosystem. A global diversity of experiences, perspectives, and opportunities will help build safe and responsible AI that works for everyone. By Anne Bertucio – Sr Program Manager, Open Source Programs Office; Helen King – Sr Director of Responsibility, Google DeepMind Magika: AI powered fast and efficient file type identification Thursday, February 15, 2024 Today we are open-sourcing Magika, Google’s AI-powered file-type identification system, to help others accurately detect binary and textual file types. Under the hood, Magika employs a custom, highly optimized deep-learning model, enabling precise file identification within milliseconds, even when running on a CPU. Magika command line tool used to identify the type of a diverse set of files You can try the Magika web demo today, or install it as a Python library and standalone command line tool (output is showcased above) by using the standard command line pip install magika. Why identifying file type is difficult Since the early days of computing, accurately detecting file types has been crucial in determining how to process files. Linux comes equipped with libmagic and the file utility, which have served as the de facto standard for file type identification for over 50 years. Today web browsers, code editors, and countless other software rely on file-type detection to decide how to properly render a file. For example, modern code editors use file-type detection to choose which syntax coloring scheme to use as the developer starts typing in a new file. Accurate file-type detection is a notoriously difficult problem because each file format has a different structure, or no structure at all. This is particularly challenging for textual formats and programming languages as they have very similar constructs. So far, libmagic and most other file-type-identification software have been relying on a handcrafted collection of heuristics and custom rules to detect each file format. This manual approach is both time consuming and error prone as it is hard for humans to create generalized rules by hand. In particular for security applications, creating dependable detection is especially challenging as attackers are constantly attempting to confuse detection with adversarially-crafted payloads. To address this issue and provide fast and accurate file-type detection we researched and developed Magika, a new AI powered file type detector. Under the hood, Magika uses a custom, highly optimized deep-learning model designed and trained using Keras that only weighs about 1MB. At inference time Magika uses Onnx as an inference engine to ensure files are identified in a matter of milliseconds, almost as fast as a non-AI tool even on CPU. Magika Performance Magika detection quality compared to other tools on our 1M files benchmark Performance wise, Magika, thanks to its AI model and large training dataset, is able to outperform other existing tools by about 20% when evaluated on a 1M files benchmark that encompasses over 100 file types. Breaking down by file type, as reported in the table below, we see even greater performance gains on textual files, including code files and configuration files that other tools can struggle with. Various file type identification tools performance for a selection of the file types included in our benchmark - n/a indicates the tool doesn’t detect the given file type. Magika at Google Internally, Magika is used at scale to help improve Google users’ safety by routing Gmail, Drive, and Safe Browsing files to the proper security and content policy scanners. Looking at a weekly average of hundreds of billions of files reveals that Magika improves file type identification accuracy by 50% compared to our previous system that relied on handcrafted rules. In particular, this increase in accuracy allows us to scan 11% more files with our specialized malicious AI document scanners and reduce the number of unidentified files to 3%. The upcoming integration of Magika with VirusTotal will complement the platform's existing Code Insight functionality, which employs Google's generative AI to analyze and detect malicious code. Magika will act as a pre-filter before files are analyzed by Code Insight, improving the platform’s efficiency and accuracy. This integration, due to VirusTotal’s collaborative nature, directly contributes to the global cybersecurity ecosystem, fostering a safer digital environment. Open Sourcing Magika By open-sourcing Magika, we aim to help other software improve their file identification accuracy and offer researchers a reliable method for identifying file types at scale. Magika code and model are freely available starting today in Github under the Apache2 License. Magika can also quickly be installed as a standalone utility and python library via the pypi package manager by simply typing pip install magika with no GPU required. We also have an experimental npm package if you would like to use the TFJS version. To learn more about how to use it, please refer to Magika documentation site. Acknowledgements Magika would not have been possible without the help of many people including: Ange Albertini, Loua Farah, Francois Galilee, Giancarlo Metitieri, Luca Invernizzi, Young Maeng, Alex Petit-Bianco, David Tao, Kurt Thomas, Amanda Walker, and Zhixun Tan. By Elie Bursztein – Cybersecurity AI Technical and Research Lead and Yanick Fratantonio – Cybersecurity Research Scientist    Popular Posts Mentor organizations announced for Google Summer of Code 2023! Google Summer of Code 2023 accepted contributors announced! Rust fact vs. fiction: 5 Insights from Google's Rust journey in 2022 Google Summer of Code 2024 Celebrating our 20th Year! Magika: AI powered fast and efficient file type identification Archive ▼ 2024 (7) ▼ February (6) Mentor organizations announced for Google Summer o... Building Open Models Responsibly in the Gemini Era Magika: AI powered fast and efficient file type id... YouTube releases scripts to help partners and crea... Kubernetes 1.29 is available in the Regular channe... Announcing Google Season of Docs 2024 ► January (1) ► 2023 (44) ► December (5) ► November (6) ► October (2) ► September (3) ► August (1) ► July (2) ► June (5) ► May (5) ► April (2) ► March (6) ► February (3) ► January (4) ► 2022 (44) ► December (4) ► November (2) ► October (7) ► September (6) ► August (2) ► July (3) ► June (5) ► May (1) ► April (2) ► March (4) ► February (5) ► January (3) ► 2021 (55) ► December (3) ► November (7) ► October (4) ► September (7) ► August (5) ► June (2) ► May (2) ► April (6) ► March (6) ► February (8) ► January (5) ► 2020 (83) ► December (7) ► November (6) ► October (7) ► September (5) ► August (13) ► July (1) ► June (7) ► May (9) ► April (5) ► March (13) ► February (5) ► January (5) ► 2019 (65) ► December (6) ► November (9) ► October (8) ► September (5) ► August (3) ► July (5) ► June (4) ► May (8) ► April (3) ► March (7) ► February (4) ► January (3) ► 2018 (59) ► December (4) ► November (2) ► October (3) ► September (2) ► August (10) ► July (2) ► June (3) ► May (5) ► April (1) ► March (16) ► February (3) ► January (8) ► 2017 (73) ► December (4) ► November (5) ► October (6) ► September (7) ► August (3) ► July (3) ► June (3) ► May (5) ► April (4) ► March (13) ► February (7) ► January (13) ► 2016 (85) ► December (9) ► November (13) ► October (13) ► September (8) ► August (9) ► July (5) ► June (2) ► May (5) ► April (3) ► March (7) ► February (7) ► January (4) ► 2015 (80) ► December (5) ► November (7) ► October (6) ► September (6) ► August (4) ► July (1) ► June (6) ► May (6) ► April (10) ► March (10) ► February (11) ► January (8) ► 2014 (104) ► December (6) ► November (12) ► October (7) ► September (8) ► August (9) ► July (7) ► June (10) ► May (8) ► April (8) ► March (11) ► February (8) ► January (10) ► 2013 (100) ► December (7) ► November (10) ► October (8) ► September (9) ► August (10) ► July (7) ► June (7) ► May (8) ► April (10) ► March (9) ► February (7) ► January (8) ► 2012 (93) ► December (4) ► November (6) ► October (9) ► September (8) ► August (8) ► July (5) ► June (7) ► May (10) ► April (5) ► March (15) ► February (9) ► January (7) ► 2011 (117) ► December (7) ► November (14) ► October (13) ► September (10) ► August (6) ► July (13) ► June (11) ► May (5) ► April (11) ► March (10) ► February (10) ► January (7) ► 2010 (123) ► December (9) ► November (12) ► October (10) ► September (14) ► August (10) ► July (7) ► June (10) ► May (11) ► April (14) ► March (13) ► February (8) ► January (5) ► 2009 (124) ► December (6) ► November (5) ► October (11) ► September (11) ► August (8) ► July (13) ► June (6) ► May (11) ► April (16) ► March (17) ► February (10) ► January (10) ► 2008 (167) ► December (10) ► November (11) ► October (13) ► September (16) ► August (12) ► July (20) ► June (14) ► May (21) ► April (16) ► March (17) ► February (17) Google Privacy Terms .