A Few Thoughts on Cryptographic Engineering ‚Äì Some random thoughts about crypto. Notes from a course I teach. Pictures of my dachshunds. Skip to content Menu A Few Thoughts on Cryptographic Engineering Some random thoughts about crypto. Notes from a course I teach. Pictures of my dachshunds. Matthew Green I'm a cryptographer and professor at Johns Hopkins University. I've designed and analyzed cryptographic systems used in wireless networks, payment systems and digital content protection platforms. In my research I look at the various ways cryptography can be used to promote user privacy. My academic website Mastodon Twitter Top Posts Useful crypto resources Bitcoin tipjar Cryptopals challenges Applied Cryptography Research: A Board Journal of Cryptographic Engineering (not related to this blog) Search for: Top Posts & PagesAttack of the week: RC4 is kind of broken in TLSZero Knowledge Proofs: An illustrated primerAttack of the week: Airdrop tracingHow to choose an Authenticated Encryption modeLet's talk about PAKETo Schnorr and beyond (Part 1)To Schnorr and beyond (part 2)Useful Cryptography ResourcesWhat is the Random Oracle Model and why should you care? (Part 1)One-Time Programs Banner image by Matt Blaze Archives Archives Select Month January 2024 (1) November 2023 (1) October 2023 (1) August 2023 (1) May 2023 (2) April 2023 (1) March 2023 (1) December 2022 (1) October 2022 (1) June 2022 (1) January 2022 (1) August 2021 (1) July 2021 (1) March 2021 (1) November 2020 (1) August 2020 (1) July 2020 (1) April 2020 (1) March 2020 (1) January 2020 (1) December 2019 (1) October 2019 (1) September 2019 (1) June 2019 (1) February 2019 (1) December 2018 (1) October 2018 (1) September 2018 (1) July 2018 (2) May 2018 (1) April 2018 (3) February 2018 (1) January 2018 (2) December 2017 (1) November 2017 (1) October 2017 (2) September 2017 (1) July 2017 (1) March 2017 (1) February 2017 (1) January 2017 (1) November 2016 (1) August 2016 (2) July 2016 (1) June 2016 (1) March 2016 (2) December 2015 (1) November 2015 (1) October 2015 (1) September 2015 (1) August 2015 (1) July 2015 (1) May 2015 (1) April 2015 (2) March 2015 (1) February 2015 (3) January 2015 (1) December 2014 (1) November 2014 (1) October 2014 (3) September 2014 (1) August 2014 (1) July 2014 (1) April 2014 (2) March 2014 (1) February 2014 (1) January 2014 (1) December 2013 (4) October 2013 (1) September 2013 (4) August 2013 (1) July 2013 (1) June 2013 (2) May 2013 (1) April 2013 (2) March 2013 (2) February 2013 (3) January 2013 (2) December 2012 (1) November 2012 (1) October 2012 (4) September 2012 (3) August 2012 (4) July 2012 (2) June 2012 (3) May 2012 (5) April 2012 (6) March 2012 (4) February 2012 (7) January 2012 (8) December 2011 (11) November 2011 (13) October 2011 (7) September 2011 (8) Attack of the week: Airdrop tracing It‚Äôs been a while since I wrote an ‚Äúattack of the week‚Äù post, and the fault for this is entirely mine. I‚Äôve been much too busy writing boring posts about Schnorr signatures! But this week‚Äôs news brings an exciting story with both technical and political dimensions: new reports claim that Chinese security agencies have developed a technique to trace the sender of AirDrop transmissions. Typically my ‚Äúattack of the week‚Äù posts are intended to highlight recent research. What‚Äôs unusual about this one is that the attack is not really new; it was discovered way back in 2019, when a set of TU Darmstadt researchers ‚Äî Heinrich, Hollick, Schneider, Stute, and Weinert ‚Äî reverse-engineered the Apple AirDrop protocol and disclosed several privacy flaws to Apple. (The resulting paper, which appeared in Usenix Security 2021 can be found here.) What makes this an attack of the week is a piece of news that was initially reported by Bloomberg (here‚Äôs some other coverage without paywall) claiming that researchers in China‚Äôs Beijing Wangshendongjian Judicial Appraisal Institute have used these vulnerabilities to help police to identify the sender of ‚Äúunauthorized‚Äù AirDrop materials, using a technique based on rainbow tables. While this new capability may not (yet) be in widespread deployment, it represents a new tool that could strongly suppress the use of AirDrop in China and Hong Kong. And this is a big deal, since AirDrop is apparently one of a few channels that can still be used to disseminate unauthorized protest materials ‚Äî and indeed, that was used in both places in 2019 and 2022, and (allegedly as a result) has already been subject to various curtailments. In this post I‚Äôm going to talk about the Darmstadt research and how it relates to the news out of Beijing. Finally, I‚Äôll talk a little about what Apple can do about it ‚Äî something that is likely to be as much of a political problem as a technical one. As always, the rest of this will be in the ‚Äúfun‚Äù question-and-answer format I use for these posts. What is AirDrop and why should I care? Image from Apple. Used without permission. If you own an iPhone, you already know the answer to this question. Otherwise: AirDrop is an Apple-specific protocol that allows Apple devices to send files (and contacts and other stuff) in a peer-to-peer manner over various wireless protocols, including Bluetooth and WiFi. The key thing to know about AirDrop is that it has two settings, which can be enabled by a potential receiver. In ‚ÄúContacts Only‚Äù mode, AirDrop will accept files only from people who are in your Contacts list (address book.) When set to ‚ÄúEveryone‚Äù, AirDrop will receive files from any random person within transmit range. This latter mode has been extensively used to distribute protest materials in China and Hong Kong, as well as to distribute indecent photos to strangers all over the world. The former usage of AirDrop became such a big deal in protests that in 2022, Apple pushed a software update exclusively to Chinese users that limited the ‚ÄúEveryone‚Äù receive-from mode ‚Äî ensuring that phones would automatically switch back to ‚ÄúContacts only‚Äù after 10 minutes. The company later extended this software update to all users worldwide, but only after they were extensively criticized for the original move. Is AirDrop supposed to be private? And how does AirDrop know if a user is in their Contacts list? While AirDrop is not explicitly advertised as an ‚Äúanonymous‚Äù communication protocol, any system that has your phone talking to strangers has implicit privacy concerns baked into it. This drives many choices around how AirDrop works. Let‚Äôs start with the most important one: do AirDrop senders provide their ID to potential recipients? The answer, at some level, must be ‚Äúyes.‚Äù The reason for this is straightforward. In order for AirDrop recipients in ‚ÄúContacts only‚Äù mode to check that a sender is in their Contacts list, there must be a way for them to check the sender‚Äôs ID. This implies that the sender must somehow reveal their identity to the recipient. And since AirDrop presents a list of possible recipients any time a sending user pops up the AirDrop window, this will happen at ‚Äúdiscovery‚Äù time ‚Äî typically before you‚Äôve even decided if you really want to send a file. But this poses a conundrum: the sender‚Äôs phone doesn‚Äôt actually know which nearby AirDrop users are willing to receive files from it ‚Äî i.e., which AirDrop users have the sender in their Contacts ‚Äî and it won‚Äôt know this until it actually talks to them. But talking to them means your phone is potentially shouting at everyone around it all the time, saying something like: Hi there! My Apple ID is john.doe.28@icloud.com. Will you accept files from me!?? Now forget that this is being done by phones. Instead imagine yourself, as a human being, doing this to every random stranger you encounter on the subway. It should be obvious that this will quickly become a privacy concern, one that would scare even a company that doesn‚Äôt care about privacy. But Apple generally does care quite a bit about privacy! Thus, just solving this basic problem requires a clever way by which phones can figure out whether they should talk to each other ‚Äî i.e., whether the receiver has the sender in its Contacts ‚Äî without either side leaking any useful information to random strangers. Fortunately cryptographic researchers have thought a lot about this problem! We‚Äôve even given it a cool name: it‚Äôs called Private Set Intersection, or PSI. To make a long story short: a Private Set Intersection protocol takes a set of strings from the Sender and a set from the Receiver. It gives one (or both) parties the intersection of both sets: that is, the set of entries that appear on both lists. Most critically, a good PSI protocol doesn‚Äôt reveal any other information about either of the sets. In Apple‚Äôs case, the Sender would have just a few entries, since you can have a few different email addresses and phone numbers. The Receiver would have a big set containing its entire Contacts list. The output of the protocol would contain either (1) one or more of the Sender‚Äôs addresses, or (2) nothing. A PSI protocol would therefore solve Apple‚Äôs problem nicely. Great, so which PSI protocol does Apple use? The best possible answer to this is: üòî. For a variety of mildly defensible reasons ‚Äî which I will come back to in a moment ‚Äî Apple does not use a secure PSI protocol to solve their AirDrop problem. Instead they did the thing that every software developer does when faced with the choice of doing complicated cryptography or ‚Äúhacking something together in time for the next ship date‚Äù: they threw together their own solution using hash functions. The TU Darmstadt researchers did a nice job of reverse-engineering Apple‚Äôs protocol in their paper. Read it! The important bit happens during the ‚ÄúDiscovery‚Äù portion of the protocol, which is marked by an HTTPS POST request as shown in the excerpt below: The very short TL;DR is this: In the POST request, a sender attaches a truncated SHA-256 hash of its own Apple ID, which is contained within a signed certificate that it gets from Apple. (If the sender has more than one identifier, e.g., a phone number and an email address, this will contain hashes of each one.) The recipient then hashes every entry in its Contacts list, and compares the results to see if it finds a match. If the recipient is in Contacts Only mode and finds a match, it indicates this and accepts later file transfers. Otherwise it aborts the connection. (As a secondary issue, AirDrop also includes a very short [two byte] portion of the same hashes in its BLE advertisements. Two bytes is pretty tiny, which means this shouldn‚Äôt leak much information, since many different addresses will collide on a two-byte hash. However, some other researchers have determined that it generally does work well enough to guess identities. Or they may have, the source isn‚Äôt translating well for me.) A second important issue here is that the hash identifiers are apparently stored in logs within the recipient‚Äôs phone, which means that to obtain them you don‚Äôt have to be physically present when the transfer happens. You can potentially scoop them out of someone else‚Äôs phone after the fact. So what‚Äôs the problem? Many folks who have some experience with cryptography will see the problem immediately. But let‚Äôs be explicit. Hash functions are designed to be one-way. In theory, this means that there is should be no efficient algorithm for ‚Äúdirectly‚Äù taking the output of a hash function and turning it back into its input. But that guarantee has a huge asterisk: if I can guess a set of possible inputs that could have produced the hash, I can simply hash each one of my guesses and compare it to the target. If one input matches, then chances are overwhelming that I‚Äôve found the right input (also called a pre-image.) In its most basic form, this naive approach is called a ‚Äúdictionary attack‚Äù based on the idea that one can assemble a dictionary of likely candidates, then test every one. Since these hashes apparently don‚Äôt contain any session-dependent information (such as salt), you can even do the hashing in advance to assemble a dictionary of candidate hashes, making the attack even faster. This approach won‚Äôt work if your Apple ID (or phone number) is not guessable. The big question in exploiting this vulnerability is whether it‚Äôs possible to assemble a complete list of candidate Apple ID emails and phone numbers. The answer for phone numbers, as the Darmstadt researchers point out, is absolutely yes. Since there are only a few billion phone numbers, it is entirely possible to make a list of every phone number and have a computer grind through them ‚Äî given a not-unreasonable amount of time. For email addresses this is more complicated, but there are many lists of email addresses in the world, and the Chinese state authorities almost certainly have some good approaches to collecting and/or generating those lists. As an aside, exploiting these dictionaries can be done in three different ways: You can make a list of candidate identifiers (or generate them programmatically) and then, given a new target hash, you can hash each identifier and check for a match. This requires you to compute a whole lot of SHA256 hashes for each target you crack, which is pretty fast on a GPU or FPGA (or ASIC) but not optimal. You can pre-hash the list and make a database of hashes and identifiers. Then when you see a target hash, you just need to do a fast lookup. This means all computation is done once, and lookups are fast. But it requires a ton of storage. Alternatively, you can use an intermediate approach called a time-memory tradeoff in which you exchange some storage for some computation once the target is found. The most popular technique is called a rainbow table, and it really deserves its own separate blog post, though I will not elaborate today. The Chinese announcement explicitly mentions a rainbow table, so that‚Äôs a good indicator that they‚Äôre exploiting this vulnerability. Well that sucks. What can we, or rather Apple, do about it? If you‚Äôre worried about leaking your identifier, an immediate solution is to turn off AirDrop, assuming such a thing is possible. (I haven‚Äôt tried it, so I don‚Äôt know if turning this off will really stop your phone from talking to other people!) Alternatively you can unregister your Apple ID, or use a bizarre high-entropy Apple ID that nobody will possibly guess. Apple could also reduce their use of logging. But those solutions are all terrible. The proper technical solution is for Apple to replace their hashing-based protocol with a proper PSI protocol, which will ‚Äî as previously discussed ‚Äî reveal only one bit of information: whether the receiver has the sender‚Äôs address(es) in their Contacts list. Indeed, that‚Äôs the solution that the Darmstadt researchers propose. They even devised a Diffie-Hellman-based PSI protocol called ‚ÄúPrivateDrop‚Äù and showed that it can be used to solve this problem. But this is not necessarily an easy solution, for reasons that are both technical and political. It‚Äôs worth noting that Apple almost certainly knew from the get-go that their protocol was vulnerable to these attacks ‚Äî but even if they didn‚Äôt, they were told about these issues back in May 2019 by the Darmstadt folks. It‚Äôs now 2024, and Chinese authorities are exploiting it. So clearly it was not an easy fix. Some of this stems from the fact that PSI protocols are more computationally heavy that the hashing-based protocol, and some of it (may) stem from the need for more interaction between each pair of devices. Although these costs are not particularly unbearable, it‚Äôs important to remember that phone battery life and BLE/WiFi bandwidth is precious to Apple, so even minor costs are hard to bear. Finally, Apple may not view this as really being an issue. However in this case there is an even tougher political dimension. Will Apple even fix this, given that Chinese authorities are now exploiting it? And here we find the hundred billion dollar question: if Apple actually replaced their existing protocol with PrivateDrop, would that be viewed negatively by the Chinese government? Those of us on the outside can only speculate about this. However, the facts are pretty worrying: Apple has enormous manufacturing and sales resources located inside of China, which makes them extremely vulnerable to an irritated Chinese government. They have, in the past, taken actions that appeared to be targeted at restricting AirDrop use within China ‚Äî and although there‚Äôs no definitive proof of their motivations, it certainly looked bad. Finally, Apple has recently been the subject of pressure by the Indian government over its decision to alert journalists about a set of allegedly state-sponsored attacks. Apple‚Äôs response to this pressure was to substantially tone down its warnings. And Apple has many fewer resources at stake in India than in China, although that‚Äôs slowly changing. Hence there is a legitimate question about whether it‚Äôs politically wise for Apple to make a big technical improvement to their AirDrop privacy, right at the moment that the lack of privacy is being viewed as an asset by authorities in China. Even if this attack isn‚Äôt really that critical to law enforcement within China, the decision to ‚Äúfix‚Äù it could very well be seen as a slap in the face. One hopes that despite all these concerns, we‚Äôll soon see a substantial push to improve the privacy of AirDrop. But I‚Äôm not going to hold my breath. By Matthew Greenin Apple, attacks, privacyJanuary 11, 2024January 12, 20242,444 Words10 Comments To Schnorr and beyond (part 2) This post continues a long, wonky discussion of Schnorr signature schemes and the Dilithium post-quantum signature. You may want to start with Part 1. In the previous post I discussed the intuition behind Schnorr signatures, beginning with a high-level design rationale and ending with a concrete instantiation. As a reminder: our discussion began with this Tweet by Chris Peikert: Which we eventually developed into an abstract version of the Schnorr protocol that uses Chris‚Äôs ‚Äúmagic boxes‚Äù to realize part of its functionality: Finally, we ‚Äúfilled in‚Äù the magic boxes by replacing them with real-world mathematical objects, this time built using cyclic groups over finite fields or elliptic curves. Hopefully my hand-waving convinced you that this instantiation works well enough, provided we make one critical assumption: namely, that the discrete logarithm problem is hard (i.e., solving discrete logarithms in our chosen groups is not feasible in probabilistic polynomial time.) In the past this seemed like a pretty reasonable assumption to make, and hence cryptographers have chosen to make it all over the place. Sadly, it very likely isn‚Äôt true. The problem here is that we already know of algorithms that can solve these discrete logarithms in (expected) polynomial time: most notably Shor‚Äôs algorithm and its variants. The reason we aren‚Äôt deploying these attacks today is that we simply don‚Äôt have the hardware to run them yet ‚Äî because they require an extremely sophisticated quantum computer. Appropriate machines don‚Äôt currently exist, but they may someday. This raises an important question: Do Schnorr signatures have any realization that makes sense in this future post-quantum world? And can we understand it? That‚Äôs where I intend to go in the rest of this post. Towards a post-quantum world Cryptographers and standards agencies are not blind to the possibility that quantum computers will someday break our cryptographic primitives. In anticipation of this future, NIST has been running a public competition to identify a new set of quantum-resistant cryptographic algorithms that support both encryption and digital signing. These algorithms are designed to be executed on a classical computer today, while simultaneously resisting future attacks by the quantum computers that may come. One of the schemes NIST has chosen for standardization is a digital signature scheme called Dilithium. Dilithium is based on assumptions drawn from the broad area of lattice-based cryptography, which contains candidate ‚Äúhard‚Äù problems that have (so far) resisted both classical and quantum attacks. The obvious way to approach the Dilithium scheme is to first spend some time on the exact nature of lattices and then discuss what these ‚Äúhard problems‚Äù are (for the moment). But we‚Äôre not going to do any of that. Instead, I find that sometimes it‚Äôs helpful to just dive straight into a scheme and see what we can learn from first contact with it. As with any signature scheme, Dilithium consists of three algorithms, one for generating keys, one for signing, and one for signature verification. We can see an overview of each of these algorithms ‚Äî minus many specific details and subroutine definitions ‚Äî at the very beginning of the Dilithium specification. Here‚Äôs what it looks like: Source: Dilithium algorithm spec v3.1, arrows are mine. As you can see, I‚Äôve added some arrows pointing to important aspects of the algorithm description above. Hopefully from these highlights (and given the title of this series of post!) you should have some idea of the point I‚Äôm trying to make here. To lay things out more clearly: even without understanding every detail of this scheme, you should notice that it looks an awful lot like a standard Schnorr signature. Let‚Äôs see if we can use this understanding to reconstruct how Dilithium works. Dilithium from first principles Our first stop on the road to understanding Dilithium will begin with Chris Peikert‚Äôs ‚Äúmagic box‚Äù explanation of Schnorr signatures. Recall that his approach has five steps: Signer picks a slope. The Signer picks a random slope of a line, and inserts it into a ‚Äúmagic box‚Äù (i.e., a one-way function) to produce the public key. Signer picks a y-intercept. To conduct the interactive Identification Protocol (in this case flattened into a signature via Fiat-Shamir), the Signer picks a random ‚Äúy-intercept‚Äù for the line, and inserts that into a second magic box. Verifier challenges on some x-coordinate. In the interactive protocol, the Verifier challenges the Prover (resp. signer) to evaluate the line at some randomly-chosen x-coordinate. Alternatively: in the Fiat-Shamir realization, the Signer uses the Fiat-Shamir heuristic to pick this challenge herself, by hashing her magic boxes together with some message. Signer evaluates the line. The Signer now evaluates her line at the given coordinate, and outputs the resulting point. (The signature thus comprises one ‚Äúmagic box‚Äù and one ‚Äúpoint.‚Äù) Verifier tests that the response is on the line. The Verifier uses the magic boxes to test that the given point is on the line. If Dilithium really is a Schnorr protocol, we should be able to identify each of these stages within the Dilithium signature specification. Let‚Äôs see what we can do. Step 1: putting a ‚Äúslope‚Äù into a magic box to form the public key. Let‚Äôs take a closer look at the Dilithium key generation subroutine: A quick observation about this scheme is that it samples not one, but two secret values, s1 and s2, both of which end up in the secret key. (We can also note that these secret values are vectors rather than simple field elements, but recall that for the moment our goal is to avoid getting hung up on these details.) If we assume Dilithium is a Schnorr-like protocol, we can surmise that one of these values will be our secret ‚Äúslope.‚Äù But which one? One approach to answering this question is to go searching for some kind of ‚Äúmagic box‚Äù within the signer‚Äôs public key. Here‚Äôs what that public key looks like: pk := (A, t) The matrix A is sampled randomly. Although the precise details of that process are not given in the high-level spec, we can reasonably observe that A is not based on s1 or s2. The second value in the public key, on the other hand, is constructed by combining A with both of the secret values: t = As1 + s2. Hence we can reasonably guess that the pair (A, t) together form the first of our magic boxes. Unfortunately, this doesn‚Äôt really answer our earlier question: we still don‚Äôt know which of the two secret values will serve as the ‚Äúslope‚Äù for the Schnorr ‚Äúlinear equation.‚Äù The obvious way to solve this problem is to skip forward to the signing routine, to see if we can find a calculation that resembles that equation. Sure enough, at line (10) we find: Here only s1 is referenced, which strongly indicates that this will take the place of our ‚Äúslope.‚Äù So roughly speaking, we can view key generation as generating a ‚Äúmagic box‚Äù for the secret ‚Äúslope‚Äù value s1. Indeed, if our public key had the form (A, t := As1), this would be extremely reminiscent of the discrete logarithm realization from the previous post, where we computed (g, gm mod p) for some random ‚Äúgenerator‚Äù g. The messy and obvious question we must ask, therefore, is: what purpose is s2 serving here? We won‚Äôt answer this question just now. For the moment I‚Äôm going to leave this as the ‚ÄúChekhov‚Äôs gun‚Äù of this story. Step 2: putting a ‚Äúy-intercept‚Äù into a second magic box. If Dilithium follows the Schnorr paradigm, signing a new message should require the selection of a fresh random ‚Äúy-intercept‚Äù value. This ensures that the signer is using a different line each time she runs the protocol. If Peggy were to omit this step, she‚Äôd be continuously producing ‚Äúpoints‚Äù on a single ‚Äúline‚Äù ‚Äî which would very quickly allow any party to recover her secret slope. A quick glance at the signing algorithm reveals a good candidate for this value. Conveniently, it‚Äôs a vector named y: This vector y is subsequently fed into something that looks quite similar to the ‚Äúmagic box‚Äù we saw back in key generation. Here again we see our matrix A, which is then multiplied to produce Ay at line (8). But from this point the story proceeds differently: rather than adding a second vector to this product, as in the key generation routine, the value Ay is instead fed into a mysterious subroutine: w1 := HighBits(Ay, 2ùõÑ2) Although this looks slightly different from the magic box we built during key generation, I‚Äôm still going to go out on a limb and guess that w1 will still comprise our second ‚Äúmagic box‚Äù for y, and that this will be sent to the Verifier. Unfortunately, this elegant explanation still leaves us with three ‚Äúmysteries‚Äù, which we must attempt to resolve before going forward. Mystery #1: if w1 is the ‚Äúbox‚Äù, why doesn‚Äôt the Sign algorithm output it? If w1 is our ‚Äúmagic box,‚Äù then we should expect to see it output as part of the signature. And yet we don‚Äôt see this at all. Instead the Sign algorithm feeds w1 into a hash function H to produce a digest c. The pair (c, z) is actually what gets output as our signature. Fortunately this mystery, at least, has a simple explanation. What is happening in this routine is that we are using Fiat-Shamir to turn an interactive Identification Protocol into a non-interactive signature. And if you recall the previous post, you may remember that there are two different ways to realize Fiat-Shamir. In all cases the signer will first hash the magic box(es) to obtain a challenge. From here, things can proceed differently. In the first approach, the Signer would output the ‚Äúmagic box‚Äù (here w1) as part of the signature. The Verifier will hash the public key and ‚Äúbox‚Äù to obtain the challenge, and use the challenge (plus magic boxes) to verify the response/point z given by the Signer. In the alternative approach, the signer will only output the hash (digest) of the box (here c) along with enough material to reconstruct the magic box w1 during verification. The Verifier will then reconstruct w1 from the information it was given, then hash to see if what it reconstructed produces the digest c included in the signature. These two approaches are both roughly equivalent for security, but they have different implications for efficiency. If the value w1 is much larger than c, it generally makes sense to employ the second approach, since you‚Äôll get smaller signatures. A quick glance at the Verify algorithm confirms that Dilithium has definitely chosen to go with the second approach. Here we see that the Verifier uses the public key (A, t) as well as z from the signature to reconstruct a guess for the ‚Äúmagic box‚Äù w1. She then hashes this value to see if the result is equal to c: So that answers the easy question. Now let‚Äôs tackle the harder one: Mystery #2: what the heck does the function HighBits() do in these algorithms? This was a whole lot of words, folks. Based solely on the name (because the specification is too verbose), we can hazard a simple guess: HighBits outputs only the high-order bits of the elements of Ay. (With a whole lot more verbiage, the detailed description at right confirms this explanation.) So that answers the ‚Äúwhat.‚Äù The real question is: why? One possible explanation is that throwing away some bits of Ay will make the ‚Äúmagic box‚Äù w1 shorter, which would lead to smaller signatures. But as we discussed above, w1 is not sent as part of the signature. Instead, the Sign algorithm hashes w1 and sends only the resulting digest c, which is always pretty short. Compressing w1 is unlikely to give any serious performance benefit, except for making a fast hash function marginally faster to compute. To understand the purpose of HighBits() we need to look to a different explanation. Our biggest clue in doing this is that HighBits gets called not once, but twice: first it is called in the Sign algorithm, and then it is called a second time in Verify. A possible benefit of using HighBits() here would be apparent if, perhaps, we thought the input to these distinct invocations might be very similar but not exactly the same. If this were to occur ‚Äî i.e., if there was some small ‚Äúerror‚Äù in one of the two invocations ‚Äî then throwing away the insignificant bits might allow us to obtain the same final result in both places. Let‚Äôs provisionally speculate that this is going to be important further down the line. Mystery #3: why does the Sign algorithm have a weird ‚Äúquality check‚Äù loop inside of it? A final‚Äö and thus far unexplained, aspect of the Sign algorithm is that it does not simply output the signature after computing it. Instead it first performs a pair of ‚Äúquality checks‚Äù on the result ‚Äî and in some cases will throw away a generated signature and re-generate the whole thing from scratch, i.e., sampling a new random y and then repeating all the steps: This is another bizarre element of the scheme that sure seems like it might be important! But let‚Äôs move on. Steps 3 & 4: Victor picks a random point to evaluate on, and Peggy evaluates the line using her secret equation. As noted above, we already know that our signer will compute a Fiat-Shamir hash c and then use it to evaluate something that, at least looks like a Schnorr linear equation (although in this case it involves addition and multiplication of vectors.) Assuming the result passes the ‚Äúquality checks‚Äù mentioned above, the output of the signing algorithm is this value z as well as the hash digest c. Step 5: use the ‚Äúmagic boxes‚Äù to verify the signature is valid. In the most traditional (‚Äústandard Fiat-Shamir variant‚Äù) realization of a Schnorr protocol, the verification routine would first hash the magic boxes together with the message to re-compute c. Then we would use the magic boxes (t and w1) to somehow ‚Äútest‚Äù whether the signer‚Äôs response z satisfies the Schnorr equation. As noted above, Dilithium uses Fiat-Shamir in an alternative mode. Here the signature comprises (z, c), and verification will therefore require us re-compute the ‚Äúmagic box‚Äù w1 and hash it to see if the result matches c. Indeed, we‚Äôve already seen from the Verify routine that this is exactly what happens: All that remains now is to mindlessly slog through the arithmetic to see if any of it makes sense. Recall that in the signing routine, we computed w1 as: w1 := HighBits(Ay, 2ùõÑ2) In the Verify routine we re-compute the box (here labeled w‚Äô1) as follows. Note that I‚Äôve taken the liberty of substituting in the definitions of z and t and then simplifying: w‚Äô1 := HighBits(Az ‚Äì ct, 2ùõÑ2) = HighBits(A(y + cs1) ‚Äì c(As1 + s2), 2ùõÑ2) = HighBits(Ay ‚Äì cs2, 2ùõÑ2) As you can see, these two calculations ‚Äî that is, the inputs that are passed into the HighBits routine in both places ‚Äî do not produce precisely the same result. In the signing routine the input is Ay, and in the verification routine it is Ay ‚Äì cs2. These are not the same! And yet, for verification to work, the output of HighBits() must be equal in both cases. If you missed some extensive foreshadowing, then you‚Äôll be astounded to learn that this new problem is triggered by the presence of our mystery vector s2 inside of the public key. You‚Äôll recall that I asked you to ignore s2, but reminded you it would trip us up later. Chekov‚Äôs gun! The presence of this weird extra s2 term helps to explains some of the ‚Äúmysteries‚Äù we encountered within the Sign routine. The most notable of these is the purpose of HighBits(). Concretely: by truncating away the low-order bits of its input, this routine must ‚Äúthrow away‚Äù the bits that are influenced by the ugly additional term ‚Äúcs2‚Äù that shows up in the Verify equation. This trim ensures that signature verification works correctly, even in the presence of the weird junk vector s2 left over from our public key. Of course this just leaves us with some new mysteries! Like, for example: Mystery #4: why is the weird junk vector s2 inside of our public key in the first place!? We‚Äôll return to this one! Mystery #5: how can we be sure that the weird additive junk ‚Äúcs2‚Äù will always be filtered out by the HighBits() subroutine during signature verification? We‚Äôve hypothesized that HighBits() is sufficient to ‚Äúfilter out‚Äù the extra additive term cs2, which should ensure that the two invocations within Sign and Verify will each produce the same result (w1 and w‚Äô1 respectively.) If this is true, the re-computed hash c will match between the two routines and signature verification will succeed. Without poking into the exact nature and distribution of these terms, we can infer that the term cs2 must be ‚Äúinsignificant enough‚Äù in practice that it will be entirely removed by HighBits during verification ‚Äî at least most of the time. But how do we know that this will always be the case? For example, we could imagine a situation where most of the time HighBits clears away the junk. And yet every now and again the term cs2 is just large enough that the additive term will ‚Äúcarry‚Äù into the more significant bits. In this instance we would discover, to our chagrin, that: HighBits(Ay, 2ùõÑ) ‚â† HighBits(Ay ‚Äì cs2, 2ùõÑ) And thus even honestly-generated signatures would not verify. This ‚Äúquality check‚Äù ensures that the signature will verify correctly. The good news here is that ‚Äî provided this event does not occur too frequently ‚Äî we can mostly avoid this problem. That‚Äôs because the signer can examine each signature before it outputs a result, i.e., it can run a kind of ‚Äúquality check‚Äù on the result to see if a generated signature will verify correctly, and discard it if it does not. And indeed, this explanation partially resolves the mystery of the ‚Äúquality checks‚Äù we encountered during signature generation ‚Äî though, importantly, it explains only one of the two checks! And that leads us to our final mystery: Mystery #6: what is the second ‚Äúquality check‚Äù there for? We‚Äôve explained the first of our two quality checks as an attempt to make the scheme verify. So what does this other one do? Hopefully we‚Äôll figure that out later down the line. Leaving aside a few unanswered mysteries, we‚Äôve now come to the end of the purely mechanical explanation of Dilithium signatures. Everything else requires us to look a bit more closely at how the machinery works. What are these magic boxes? As we discussed in the previous post, the best real-life analog of a ‚Äúmagic box‚Äù is some kind of one-way function that has useful algebraic properties. In practice, we obtain these functions by identifying a ‚Äúhard‚Äù mathematical problem ‚Äî more precisely, a problem that requires infeasible time and resource-requirements for computers to solve ‚Äî and then figuring out how to use it in our schemes. Dilithium is based on a relatively new mathematical problem called Module Learning with Errors (MLWE). This problem sits at the intersection of two different subfields: lattice‚Äìbased cryptography and code-based cryptography. For a proper overview of LWE (of which MLWE is a variant), I strongly recommend you read this survey by Regev. Here in this post, my goal is to give you a vastly more superficial explanation: one that is just sufficient to help explain some of the residual ‚Äúmysteries‚Äù we noticed above. The LWE problem assumes that we are given the approximate solutions of a series of linear equations over some ring. Our goal is to recover a secret vector s given a set of non-secret coefficients. To illustrate this problem, Regev gives the following toy example: Note that if the solutions on the right-hand side were exact, then solving for the values s = (s1, ‚Ä¶, s4) could be accomplished using standard linear algebra. What makes this problem challenging is that the solutions are only approximate. More concretely, this means that the solutions we are given on the right side contain a small amount of additive ‚Äúerror‚Äù (i.e., noise.) Note that the error terms here are small: in this example, each equation could be accurate within a range of -1 to +1. Nonetheless, the addition of this tiny non-uniform error makes solving these problems vastly harder to both classical and quantum computers. Most critically, these error terms are essential to the conjectured ‚Äúhardness‚Äù of this function ‚Äî they cannot be wished away or eliminated. A second important fact is that the secret coefficients (the ones that make up s) are also small and are not drawn uniformly from the ring. In fact, if the secret terms and error were drawn uniformly from the ring, this would make solving the system of equations trivially easy ‚Äî there would potentially be many possible solutions to the system of equations. Hence it is quite important to the hardness of the (M)LWE function that these values be ‚Äúsmall.‚Äù You‚Äôll have to take my word for this, or else read deeper into the Regev survey to understand the detailed reasoning, but this will be very important to us going forward. Of course, big systems of equations are tough to look at. A more concise way to represent the above is to describe the non-secret (random) coefficients as a matrix A, and then ‚Äî using s1 to represent our secret ‚Äî the exact solution to these equations can be represented by the product As1. If we express those additive ‚Äúerror terms‚Äù as a second vector s2, the entire LWE ‚Äúfunction‚Äù can thus be succinctly expressed as: A, t = As1 + s2 For this function to be a good magic box, we require that given (A, t), it is hard to recover (at least) s1.1 You‚Äôll note that ‚Äî ignoring many of the nitty-gritty details, including the nature of the ring we‚Äôre using and the distribution of s1 and s2 ‚Äî this is essentially the structure of the public key from the Dilithium specification. So what should we learn from this? A clear implication is that the annoying error vector s2 is key to the security of the ‚Äúmagic box‚Äù used in Dilithium. If we did not include this term, then an attacker might be able to recover s1 from Dilithium‚Äôs public key, and thus could easily forge signatures. At the same time, we can observe that this error vector s2 will be ‚Äúsmall‚Äù in terms of the magnitude of its elements, which helps explain why we can so easily dispense with its effects by simply trimming away the low-order bits of the product Ay (resp. Ay ‚Äì cs2) inside of the signing and verification functions. Phew. A reasonable person might be satisfied at this point that Dilithium is essentially a variant of Schnorr, albeit one that uses very different ingredients from the classical Schnorr signatures. After all, the public key is just a ‚Äúmagic box‚Äù embedding the secret value s1, with some error thrown in to make it irreversible. The signature embeds a weird magic box computed on a value y as well as a ‚ÄúSchnorr-like‚Äù vector z = y + cs1 on some challenge point c, which can be approximately ‚Äútested‚Äù using the various boxes. What more is there to say? But there remains one last critical mystery here, one that we haven‚Äôt addressed. And that mystery lives right here, in the form of a second ‚Äúquality check‚Äù that we still have not explained: What does this second ‚Äúquality check‚Äù do? To figure out why we need this second quality check, we‚Äôll need to dive just a little bit deeper into what these values actually represent. Is Dilithium secure? If you recall the previous post, we proposed three different properties that a Schnorr-like Identification Protocol should satisfy. Specifically, we wanted to ensure that our protocols are (1) correct, (2) sound, and (3) private ‚Äî i.e., they not leak their secret key to any Verifier who sees a few transcripts. Correctness. This simply means that honestly-generated signatures will verify. I hope at this point that we‚Äôve successfully convinced ourselves that Dilithium will likely achieve this goal, provided we get out of the ‚Äúquality check‚Äù loop. (Since producing a valid signature may require multiple runs through the ‚Äúquality check‚Äù loop, we do need to have some idea of how likely a ‚Äúbad‚Äù signature is ‚Äî the Dilithium authors perform this analysis and claim that 4-7 iterations is sufficient in most cases.) Soundness. This property requires that we consider the probability that a dishonest signer (one who does not know the secret key) can produce a valid signature. This hangs on two principles: (1) the non-reversibility of the public key ‚Äúmagic box‚Äù, or more concretely: the assumed one-wayness of the MLWE function. It also requires (2) a more involved argument about the signer‚Äôs ability to compute responses. In this post we will choose to take the actual hardness of MLWE for granted (i.e., we are perfectly happy to make analyzing that function into some other cryptographer‚Äôs problem!) Hence we need only consider the second part of the argument. Specifically: let‚Äôs consider Dilithium as a pure interactive identification protocol. Let us imagine that a prover can satisfy the protocol when handed some random challenge c by the Verifier, and they can do this with high probability for many different possible values of c. If we follow the logic here, this would seem to intuitively imply that such a prover can therefore pick a value y, and then compute a response z for multiple possible c values that they may be handed. Concretely we can imagine that such a prover could produce a few values of the form: zi = y + ci s1 If a prover can compute at least two such values (all using the same value of y, but different values of c), then presumably she could then use the values to derive s1 itself ‚Äî simply by subtracting and solving for s1. What we are saying here, very informally, is that any prover who has the knowledge to successfully run the protocol this way is equivalent (up to a point) to a prover who knows the secret key. Although we will not dwell on the complete argument or the arithmetic, this does not seem like an unreasonable argument to make for Dilithium.2 Privacy (or ‚Äúzero knowledge‚Äù.) The most challenging part of the original Schnorr proof was the final argument, namely the one that holds that learning a protocol transcript (or signature) will not reveal the secret key. This privacy, or ‚Äúzero-knowledge‚Äù argument, is one of the most important things we addressed in the previous post. We can found this argument on the following claim: any number of signatures (or interactive protocol) transcripts, by themselves, do not leak any useful information that can be used by an attacker to learn about the secret key. In order to make this argument successfully for Schnorr, we came at it in a particularly bizarre way: namely, we argued that this had to be true, since any random stranger can produce a correctly-distributed Schnorr transcript ‚Äî whether or not they know the secret key. For the traditional Schnorr protocol we pointed out that it is possible to manufacture a ‚Äúfake‚Äù transcript by (1) selecting a random challenge and response, and then (2) ‚Äúmanufacturing‚Äù a new magic box to contain a new (to us unknown!) y-intercept for the line. Translating this approach to the interactive version of the Dilithium protocol, this would require us to first sample the challenge c, then sample the response z from its appropriate distribution. We would then place z into a fresh magic box (‚ÄúAz‚Äù), and compute something like this (here boxes represent MLWE functions): (Or more concretely, ‚ÄúAy‚Äù = Az ‚Äì ct. Note that t = As1 + s2 and so this gives us a slightly ‚Äúwrong‚Äù answer due to the presence of s2, but using the HighBits function on this result should strip that out and give us the correct ‚Äúbox‚Äù value.) Given the calculation above, we can use the HighBits() function to compute w1 given the ‚Äúboxed‚Äù version of y.3 The main question we would have to ask now is: is this simulated transcript statistically identical to the real transcript that would be produced by a legitimate signer? And here we run into a problem that has not been exposed throughout this post, mostly because we‚Äôve been ignoring all of the details. The map is not the territory! Up to this point we‚Äôve mostly been ignoring what‚Äôs ‚Äúinside‚Äù of each of the various vectors and matrices (y, z, A and so on.) This has allowed us to make good progress, and ignoring the details was acceptable for a high-level explanation. Unfortunately when we talk about security, these details really matter. I will make this as quick and painless as I possibly can. In Dilithium, all elements in these arrays and vectors consist of polynomials in the ring . Each ‚Äúelement‚Äù is actually a vector of coefficients representing a polynomial of the form , where every coefficient is a integer modulo q. These polynomials can be added and multiplied using standard operations built from modular arithmetic. For Dilithium, we ‚Äúconveniently‚Äù fix q = 223 ‚àí 213 + 1 and n = 256. (For an engineering-oriented overview of how to work with these elements, see this post by Filippo Valsorda. Although Filippo is describing Kyber, which uses a different q, the arithmetic is similar.) What‚Äôs important in Dilithium is how all of our various random matrices and vectors are sampled. We will note first that the matrix A consists of polynomials whose coefficients are sampled uniformly from {0, .., q-1}. However, the remaining vectors such as s1, s2 and y comprise coefficients that are not chosen this way, because the requirements of the MLWE function dictate that they cannot be sampled uniformly. And this will be critical to understanding the security arguments. Let‚Äôs take a look back at the key generation and signing routines: Notice the small subscripts used in generating y and both s1, s2. These indicate that the coefficients in these vectors are restricted to a subset of possible values, those less than some chosen parameter. In the case of the vectors s1, s2 this is an integer Œ∑ that is based on study of the MLWE problem. For the vector y the limit is based on a separate scheme parameter …£1, which is also based on the MLWE problem (and some related complexity assumptions.) At this point I may as well reveal one further detail: our hash function H does not output something as simple as a scalar or a uniform element of the ring. Instead it outputs a ‚Äúhamming ball‚Äù coefficient vector that comprises mostly 0 bits, as well as exactly r bits that contain either +1 or -1. (Dilithium recommends r=60.) This hash will be multiplied by s1 when computing z. Once you realize that Dilithium‚Äôs s1, y and c are not uniform, as they were in the original Schnorr scheme, then this has some implications for the security of the response value z := y + cs1 that the signer outputs. Consider what would happen if the signer did not add the term y to this equation, i.e., it simply output the product cs1 directly. This seems obviously bad: it this would reveal information about the secret key s1, which would imply a leak of at least some bits of the secret key (given a few signatures.) Such a scheme should obviously not be simulatable, since we would not be able to simulate it without knowing the secret key. The addition of the term y is critical to the real-world security of the scheme, since it protects the secrecy of the secret key by ‚Äúblinding it.‚Äù (Or, if you preferred the geometric security intuition from the previous post: the signer chooses a fresh ‚Äúy-intercept‚Äù each time she evaluates the protocol, because this ensures that the Verifier will not receive multiple points on the same line and thus be able to zero in on the slope value. In classical Schnorr this y-intercept is sampled uniformly from a field, so it perfectly hides the slope. Here we are weakening the logic, since both the slope and y-intercept are drawn from reduced [but non-identical] distributions!) The problem is that in Dilithium the term y is not sampled uniformly: its coefficients are relatively small. This means that we can‚Äôt guarantee that z := y + cs1 will perfectly hide the coefficients of cs1 and hence the secret key. This is a very real-world problem, not just something that shows up in the security proof! The degree of ‚Äúprotection‚Äù we get is going to be related to the relative magnitude of the coefficients of cs1 and y. If the range of the coefficients of y is sufficiently large compared to the coefficients of cs1, then the secret key may be protected ‚Äî but other times when the coefficients of cs1 are unusually large, they may ‚Äúpoke out‚Äù, like a pea poking through a too-thin mattress into a princess‚Äôs back. Here the black arrows represent the coefficients of cs1 and the red lines are the coefficients of y. Their sum forms the coefficients of z. None of this is to scale. If any coefficients poke through too much, this leaks information about the bits of the secret key! The good news is that we can avoid these bad outcomes by carefully selecting the range of the y values so that they are large enough to statistically ‚Äúcover‚Äù the coefficients of cs1, and by testing the resulting z vector to ensure it never contains any particularly large coefficients that might represent the coefficients of cs1 ‚Äúpoking through.‚Äù Concretely, note that each coefficient of s1 was chosen to be less than Œ∑, and there are at most r non-zero (+1, -1) bits set in the hash vector c. Hence the direct product cs1 will have be a vector where all coefficients are of size at most Œ≤ ‚â§ r*Œ∑. The coefficients of y, by construction, are all at most …£1. The test Dilithium uses is to reject a signature if any coefficient of the result z is greater than the difference of these values, …£1 ‚Äì Œ≤. Which is precisely what we see in the second ‚Äúquality check‚Äù of the signing algorithm: With this test in the real protocol, the privacy (zero-knowledge) argument for the Identification Protocol is now satisfied. It is always the case that as long as the vector z is chosen to be within the given ranges, the distribution of our simulated transcripts will be identical to that of the real protocol. (A more formal argument is given in Appendix B of the spec, or see this paper that introduced the ideas.) Conclusion‚Ä¶ plus everything I left out First of all, if you‚Äôre still reading this: congratulations. You should probably win a prize of some sort. This has been a long post! And even with all this detail, we haven‚Äôt managed to cover some of the more useful details, which include a number of clever efficiency optimizations that reduce the size of the public key and make the algorithms more efficient. I have also left out the ‚Äútight‚Äù security reduction that rely on some weird extra additional complexity assumptions, because these assumptions are nuts. This stuff is all described well in the main spec above, if you want the details. But more broadly: what was the point of all this? My goal in writing this post was to convince you that Dilithium is ‚Äúeasy‚Äù to understand ‚Äî after all, it‚Äôs just a Schnorr signature built using alternative ingredients, like a loaf of bread made with almond flour rather than with wheat. There‚Äôs nothing really scary about PQC signatures or lattices, if you‚Äôre willing to understand a few simple principles. And to some extent I feel like I succeeded at that. To a much greater extent, however, I feel like I convinced myself of just the opposite. Specifically, writing about Dilithium has made me aware of just how precise and finicky these post-quantum schemes are, how important the details are, particularly to the simpler old discrete logarithm setting. Maybe this will improve with time and training: as we all get more familiar using these new tools, we‚Äôll get better at specifying the the building blocks in a clear, modular way and won‚Äôt have to get so deep into various details each time we design a new protocol. Or maybe that won‚Äôt happen, and these schemes are just going to be fundamentally a bit less friendly to protocol designers than the tools we‚Äôve used in the past. In either case, I‚Äôm hoping that a generation of younger and more flexible cryptographers will deal with that problem when it comes. Notes: In practice, the LWE assumption is actually slightly stronger than this. It states that the pair (A, t = As1 + s2) is indistinguishable from a pair (A, u) where u is sampled uniformly ‚Äî i.e., no efficient algorithm can guess the difference with more than a negligible advantage over random guessing. Since these distributions are quite different, however, this indistinguishability must imply one-wayness of the underlying function. As mentioned in a footnote to the previous post, this can actually be done by ‚Äúrewinding‚Äù a prover/signer. The idea in the security proof is that if there exists an adversary (a program) that can forge the interactive protocol with reasonable probability, then we can run it up until it has output y. Then we can run it forward on a first challenge c1 to obtain z1. Finally, we can ‚Äúrewind‚Äù the prover by running it on the same inputs/random coins until it outputs y again, but this time we can challenge it on a different value c2 to obtain z2. And at this point we can calculate s1 from the pair of responses. This argument applies to the Dilithium Identification Protocol, which is a thing that doesn‚Äôt really exist (except for implicitly.) In that protocol a ‚Äútranscript‚Äù consists of the triple (w1, c, z). Since that protocol is interactive, there‚Äôs no problem with being able to fake a transcript ‚Äî the protocol only has soundness if you run it interactively. Notice that for Dilithium signatures, things are different; you should not be able to ‚Äúforge‚Äù a Dilithium signature under normal conditions. This unforgeability is enforced by the fact that c is the output of a hash function H and this will be checked during verification, and so you can‚Äôt just pick c arbitrarily. The zero-knowledge argument still holds, but it requires a more insane argument that has to do with the ‚Äúmodel‚Äù we use where in the specific context of the security proof we can ‚Äúprogram‚Äù (tamper with) the hash function H. By Matthew Greenin fundamentals, pqcNovember 30, 2023December 1, 20236,433 Words2 Comments To Schnorr and beyond (Part 1) Warning: extremely wonky cryptography post. Also, possibly stupid and bound for nowhere. One of the hardest problems in applied cryptography (and perhaps all of computer science!) is explaining why our tools work the way they do. After all, we‚Äôve been gifted an amazing basket of useful algorithms from those who came before us. Hence it‚Äôs perfectly understandable for practitioners to want to take those gifts and simply start to apply them. But sometimes this approach leaves us wondering why we‚Äôre doing certain things: in these cases it‚Äôs helpful to take a step back and think about what‚Äôs actually going on, and perhaps what was in the inventors‚Äô heads when the tools were first invented. In this post I‚Äôm going to talk about signature schemes, and specifically the Schnorr signature, as well as some related schemes like ECDSA. These signature schemes have a handful of unique properties that make them quite special among cryptographic constructions. Moreover, understanding the motivation of Schnorr signatures can help understand a number of more recent proposals, including post-quantum schemes like Dilithium ‚Äî which we‚Äôll discuss in the second part of this series. As a motivation for this post, I want to talk about this tweet: Instead of just dumping Schnorr signatures onto you, I‚Äôm going to take a more circuitous approach. Here we‚Äôll start from the very most basic building blocks (including the basic concept of an identification protocol) and then work our way gradually towards an abstract framework. Identification protocols: our most useful building block If you want to understand Schnorr signatures, the very first thing you need to understand is that they weren‚Äôt really designed to be signatures at all, at least not at first. The Schnorr protocol was designed as an interactive identification scheme, which can be ‚Äúflattened‚Äù into the signature scheme we know and love. An identification scheme consists of a key generation algorithm for generating a ‚Äúkeypair‚Äù comprising a public and secret key, as well as an interactive protocol (the ‚Äúidentification protocol‚Äù) that uses these keys. The public key represents its owners‚Äô identity, and can be given out to anyone. The secret key is, naturally, secret. We will assume that it is carefully stored by its owner, who can later use it to prove that she ‚Äúowns‚Äù the public key. The identification protocol itself is run interactively between two parties ‚Äî meaning that the parties will exchange multiple messages in each direction. We‚Äôll often call these parties the ‚Äúprover‚Äù and the ‚Äúverifier‚Äù, and many older papers used to give them cute names like ‚ÄúPeggy‚Äù and ‚ÄúVictor‚Äù. I find this slightly twee, but will adopt those names for this discussion just because I don‚Äôt have any better ideas. To begin the identification protocol, Victor must obtain a copy of Peggy‚Äôs public key. Peggy for her part will possess her secret key. The goal of the protocol is for Victor to decide whether he trusts Peggy: High level view of a generic interactive identification protocol. We‚Äôll assume the public key was generated in a previous key generation phase. (No, I don‚Äôt know why the Verifier has a tennis racket.) Note that this ‚Äúproof of ownership‚Äù does not need to be 100% perfect. We only ask that it is sound with extremely high probability. Roughly speaking, we want to ensure that if Peggy really owns the key, then Victor will always be convinced of this fact. At the same time, someone who is impersonating Peggy ‚Äî i.e., does not know her secret key ‚Äî should fail to convince Victor, except with some astronomically small (negligible) probability. (Why do we accept this tiny probability of an impersonator succeeding? It turns out that this is basically unavoidable for any identification protocol. This is because the number of bits Peggy sends to Victor must be finite, and we already said there must exist at least one ‚Äúsuccessful‚Äù response that will make Victor accept. Hence there clearly exists an adversary who just guesses the right strings and gets lucky very ocasionally. As long as the number of bits Peggy sends is reasonably large, then such a ‚Äúdumb‚Äù adversary should almost never succeed, but they will do so with non-zero probability.) The above description is nearly sufficient to explain the security goals of an identification scheme, and yet it‚Äôs not quite complete. If it was, then there would be a very simple (and yet obviously bad) protocol that solves the problem: the Prover could simply transmit its secret key to the Verifier, who can presumably test that it matches with the public key: This usually works, but don‚Äôt do this, please. If all we cared about was solving the basic problem of proving ownership in a world with exactly one Verifier who only needs to run the protocol once, the protocol above would work fine! Unfortunately in the real world we often need to prove identity to multiple different Verifiers, or to repeatedly convince the same Verifier of our identity. The problem with the strawman proposal above is that at the end of a single execution, Victor has learned Peggy‚Äôs secret key (as does anyone else who happened to eavesdrop on their communication.) This means that Victor, or any eavesdropper, will now be able to impersonate Peggy in future interactions. And that‚Äôs a fairly bad feature for an identification protocol. To deal with this problem, a truly useful identification protocol should add at least one additional security requirement: at the completion of this protocol, Victor (or an eavesdropper) should not gain the ability to mimic Peggy‚Äôs identity to another Verifier. The above protocol clearly fails this requirement, since Victor will now possess all of the secret information that Peggy once had. This requirement also helps to why identification protocols are (necessarily) interactive, or at least stateful: even if Victor did not receive Peggy‚Äôs secret key, he might still be able to record any messages sent by Peggy during her execution of the protocol with him. If the protocol was fully non-interactive (meaning, it consists of exactly one message from Peggy to Victor) then Victor could later ‚Äúreplay‚Äù his recorded message to some other Verifier, thus convincing that person that he is actually Peggy. Many protocols have suffered from this problem, including older vehicle immobilizers. The classical solution to this problem is to organize the identification protocol to have a challenge-response structure, consisting of multiple interactive moves. In this approach, Victor first sends some random ‚Äúchallenge‚Äù message to Peggy, and then Peggy then constructs her response so that it is specifically based on Victor‚Äôs challenge. Should a malicious Victor attempt to impersonate Peggy to a different Verifier, say Veronica, the expectation is that Veronica will send a different challenge value (with high probability), and so Victor will not be able to use Peggy‚Äôs original response to satisfy Veronica‚Äôs new challenge. (While interaction is generally required, in some instances we can seemingly ‚Äúsneak around‚Äù this requirement by ‚Äúextracting a challenge from the environment.‚Äù For example, real-world protocols will sometimes ‚Äòbind‚Äô the identification protocol to metadata such as a timestamp, transaction details, or the Verifier‚Äôs name. This doesn‚Äôt strictly prevent replay attacks ‚Äî replays of one-message protocols are always possible! ‚Äî but it can help Verifiers detect and reject such replays. For example, Veronica might not accept messages with out-of-date timestamps. I would further argue that, if one squints hard enough, these protocols are still interactive. It‚Äôs just that the first move of the interaction [say, querying the clock for a timestamp] is now being moved outside of the protocol.) How do we build identification schemes? Once you‚Äôve come up with the idea of an identification scheme, the obvious question is how to build one. The simplest idea you might come up with is to use some one-way function as your basic building block. The critical feature of these functions is that they are ‚Äúeasy‚Äù to compute in one direction (e.g., for some string x, the function F(x) can be computed very efficiently.) At the same time, one-way functions are hard to invert: this means that given F(x) for some random input string x ‚Äî let‚Äôs imagine x is something like a 128-bit string in this example ‚Äî it should take an unreasonable amount of computational effort to recover x. I‚Äôm selecting one-way functions because we have a number of candidates for them, including cryptographic hash functions as well as fancier number-theoretic constructions. Theoretical cryptographers also prefer them to other assumptions, in the sense that the existence of such functions is considered to be one of the most ‚Äúplausible‚Äù cryptographic assumptions we have, which means that they‚Äôre much likelier to exist than more fancy building blocks. The problem is that building a good identification protocol from simple one-way functions is challenging. An obvious starting point for such a protocol would be for Peggy to construct her secret key by selecting a random string sk (for example, a 128-bit random string) and then computing her public key as pk = F(sk). Now to conduct the identification protocol, Peggy would‚Ä¶ um‚Ä¶ well, it‚Äôs not really clear what she would do. The ‚Äúobvious‚Äù answer would be for Peggy to send her secret key sk over to Victor, and then Victor could just check that pk = F(sk). But this is obviously bad for the reasons discussed above: Victor would then be able to impersonate Peggy after she conducted the protocol with him even one time. And fixing this problem turns out to be somewhat non-trivial! There are, of course, some clever solutions ‚Äî but each one entails some limitations and costs. A ‚Äúfolklore‚Äù1 approach works like this: Instead of picking one secret string, Peggy picks N different secret strings to be her ‚Äúsecret key.‚Äù She now sets her ‚Äúpublic key‚Äù to be . In the identification protocol, Victor will challenge Peggy by asking her for a random k-sized subset of Peggy‚Äôs strings (here k is much smaller than N.) Peggy will send back the appropriate list of k secret strings. Victor will check each string against the appropriate position in Peggy‚Äôs public key. The idea here is that, after running this protocol one time, Victor learns some but not all of Peggy‚Äôs secret strings. If Victor was then to attempt to impersonate Peggy to another person ‚Äî say, Veronica ‚Äî then Veronica would pick her own random subset of k strings for Victor to respond to. If this subset is identical to the one Victor chose when he interacted with Peggy, then Victor will succeed: otherwise, Victor will not be able to answer Veronica‚Äôs challenge. By carefully selecting the values of N and k, we can ensure that this probability is very small.2 An obvious problem with this proposal is that it falls apart very quickly if Victor can convince Peggy to run the protocol with him multiple times. If Victor can send Peggy several different challenges, he will learn many more than k of Peggy‚Äôs secret strings. As the number of strings Victor learns increases, Victor‚Äôs ability to answer Veronica‚Äôs queries will improve dramatically: eventually he will be able to impersonate Peggy nearly all of the time. There are some clever ways to address this problem while still using simple one-way functions, but they all tend to be relatively ‚Äúadvanced‚Äù and costly in terms of bandwidth and computation. (I promise to talk about them in some other post.) Schnorr So far we have a motivation: we would like to build an identification protocol that is multi-use ‚Äî in the sense that Peggy can run the protocol many times with Victor (or other verifiers) without losing security. And yet one that is also efficient in the sense that Peggy doesn‚Äôt have to exchange a huge amount of data with Victor, or have huge public keys. Now there have been a large number of identity protocols. Schnorr is not even the first one to propose several of the ideas it uses. ‚ÄúSchnorr‚Äù just happens to be the name we generally use for a class of efficient protocols that meet this specific set of requirements. Some time back when Twitter was still Twitter, I asked if anyone could describe the rationale for the Schnorr protocol in two tweets or less. I admit I was fishing for a particular answer, and I got it from Chris Peikert: I really like Chris‚Äôs explanation of the Schnorr protocol, and it‚Äôs something I‚Äôve wanted to unpack for while now. I promise that all you really need to understand this is a little bit of middle-school algebra and a ‚Äúmagic box‚Äù, which we‚Äôll do away with later on. Let‚Äôs tackle it one step at a time. First, Chris proposes that Peggy must choose ‚Äúa random line.‚Äù Recalling our grade-school algebra, the equation for a line is y = mx + b, where ‚Äúm‚Äù is the line‚Äôs slope and ‚Äúb‚Äù its y-intercept. Hence, Chris is really asking us to select a pair of random numbers (m, b). (For the purposes of this informal discussion you can just pretend these are real numbers in some range. However later on we‚Äôll have them be elements of a very large finite field or ring, which will eliminate many obvious objections.) Here we will let ‚Äúm‚Äù be Peggy‚Äôs secret key, which she will choose one time and keep the same forever. Peggy will choose a fresh random value ‚Äúb‚Äù each time she runs the protocol. Critically, Peggy will put both of those numbers into a pair of Chris‚Äôs magic box(es) and send them over to Victor. Finally, Victor will challenge Peggy to evaluate her line at one specific (random) point x that he selects. This is easy for Peggy, who can compute the corresponding value y using her linear equation. Now Victor possesses a point (x, y) that ‚Äî if Peggy answered correctly ‚Äî should lie on the line defined by (m, b). He simply needs to use the ‚Äúmagic boxes‚Äù to check this fact. Here‚Äôs the whole protocol: Chris Peikert‚Äôs ‚Äúmagic box‚Äù protocol. The only thing I‚Äôve changed from his explanation is that there are now two magic boxes, one that contains ‚Äúm‚Äù and one that contains ‚Äúb‚Äú. Victor can use them together to check Peggy‚Äôs response y at the end of the protocol. Clearly this is not a real protocol, since it relies fundamentally on magic. With that said, we can still observe some nice features about it. A first thing we can observe about this protocol is that if the final check is satisfied, then Victor should be reasonably convinced that he‚Äôs really talking to Peggy. Intuitively, here‚Äôs a (non-formal!) argument for why this is the case. Notice that to complete the protocol, Peggy must answer Victor‚Äôs query on any random x that Victor chooses. If Peggy, or someone impersonating Peggy, is able to do this with high probability for any random point x that Victor might choose, then intuitively it‚Äôs reasonable that she could (in her own head, at least) compute a similar response for a second random point x‚Äô. Critically, given two separate points (x,y), (x‚Äô, y‚Äô) all on the same line, it‚Äôs easy to calculate the secret slope m ‚Äî ergo, a person who can easily compute points on a line almost certainly knows Peggy‚Äôs secret key. (This is not a proof! It‚Äôs only an intuition. However the real proof uses a similar principle.2) The question, then, is what Victor learns after running the protocol with Peggy. If we ignore the magical aspects of the protocol, the only thing that Victor ‚Äúlearns‚Äù by at end of the protocol is a single point (x, y) that happens to lie on the random line chosen by Peggy. Fortunately, this doesn‚Äôt reveal very much about Peggy‚Äôs line, and in particular, it reveals very little about her secret (slope) key. The reason is that for every possible slope value m that Peggy might have chosen as her key, there exists a value b that produces a line that intersects (x, y). We can illustrate this graphically for a few different examples: I obviously did not use a graphing tool to make this disaster. Naturally this falls apart if Victor sees two different points on the same line. Fortunately this never happens, because Peggy chooses a different line (by selecting a new b value) every time she runs the protocol. (It would be a terrible disaster if she forgot to do this!) The existence of these magic boxes obviously makes security a bit harder to think about, since now Victor can do various tests using the ‚Äúboxes‚Äù to test out different values of m, b to see if he can find a secret line that matches. But fortunately these boxes are ‚Äúmagic‚Äù, in the sense that all Victor can really do is test whether his guesses are successful: provided there are many possible values of m, this means actually searching for a matching value will take far too long to be useful. Now, you might ask: why a line? Why not a plane, or a degree-8 polynomial? The answer is pretty simple: a line happens to be one of the simplest mathematical structures that suits our needs. We require an equation for which we can ‚Äúsafely‚Äù reveal exactly one solution, without fully constraining the terms of its equation. Higher-degree polynomials and planar equations also possess this capability (indeed we can reveal more points in these structures), but each has a larger and more complex equation that would necessitate a fancier ‚Äúmagic box.‚Äù How do we know if the ‚Äúmagic box‚Äù is magic enough? Normally when people learn Schnorr, they are not taught about magic boxes. In fact, they‚Äôre typically presented with a bunch of boring details about cyclic groups. The problem with that approach is that it doesn‚Äôt teach us anything about what we need from that magic box. And that‚Äôs a shame, because there is not one specific box we can use to realize this class of protocols. Indeed, it‚Äôs better to think of this protocol as a set of general ideas that can be filled in, or ‚Äúinstantiated‚Äù with different ingredients. Hence: I‚Äôm going to try a different approach. Rather than just provide you with something that works to realize our magic box as a fait accompli, let‚Äôs instead try to figure out what properties our magical box must have, in order for it to provide us with a secure protocol. Simulating Peggy There are essentially three requirements for a secure identification protocol. First, the protocol needs to be correct ‚Äî meaning that Victor is always convinced following a legitimate interaction with Peggy. Second, it needs to be sound, meaning that only Peggy (and not an impersonator) can convince Victor to accept. We‚Äôve made an informal argument for both of these properties above. It‚Äôs important to note that each of these arguments relies primarily on the fact that our magic box works as advertised ‚Äî i.e., Victor can reliably ‚Äútest‚Äù Peggy‚Äôs response against the boxed information. Soundness also requires that bad players cannot ‚Äúunbox‚Äù Peggy‚Äôs secret key and fully recover her secret slope m, which is something that should be true of any one-way function. But these arguments don‚Äôt dwell thoroughly on how secure the boxes must be. Is it ok if an attacker can learn a few bits of m and b? Or do they need to be completely ideal. To address these questions, we need to consider a third requirement. That requirement is that that Victor, having run the protocol with Peggy, should not learn anything more useful than he already knew from having Peggy‚Äôs public key. This argument really requires us to argue that these boxes are quite strong ‚Äî i.e., they‚Äôre not going to leak any useful information about the valuable secrets beyond what Victor can get from black-box testing. Recall that our basic concern here is that Victor will run the protocol with Peggy, possibly multiple times. At the end of each run of the protocol, Victor will learn a ‚Äútranscript‚Äù. This contents of this transcript are 1) one magic box containing ‚Äúb‚Äú, 2) the challenge value x that Victor chose, and 3) the response y that Peggy answered with. We are also going to assume that Victor chose the value x ‚Äúhonestly‚Äù at random, so really there are only two interesting values that he obtained from Peggy. A question we might ask is: how useful is the information in this transcript to Victor, assuming he wants to do something creepy like pretend to be Peggy? Ideally, the answer should be ‚Äúnot very useful at all.‚Äù The clever way to argue this point is to show that Victor can perfectly ‚Äúsimulate‚Äù these transcripts without every even talking to Peggy at all. The argument thus proceeds as follows: if Victor (all by his lonesome) can manufacture a transcript that is statistically identical to the ones he‚Äôd get from talking to Peggy, then what precisely has he ‚Äúlearned‚Äù from getting real ones from Peggy at all? Implicitly the answer is: not very much. So let‚Äôs take a moment to think about how Victor might (all by himself) produce a ‚Äúfake‚Äù transcript without talking to Peggy. As a reminder, here‚Äôs the ‚Äúmagic box‚Äù protocol from up above: One obvious (wrong) idea for simulating a transcript is that Victor could first select some random value b, and put it into a brand new ‚Äúmagic box‚Äù. Then he can pick x at random, as in the real protocol. But this straightforward attempt crashes pretty quickly: Victor will have a hard time computing y = mx + b, since he doesn‚Äôt know Peggy‚Äôs secret key m. His best attempt, as we discussed, would be to guess different values and test them, which will take too long (if the field is large.) So clearly this approach does not work. But note that Victor doesn‚Äôt necessarily need to fake this transcript ‚Äúin order.‚Äù An alternative idea is that Victor can try to make a fake transcript by working through the protocol in a different order. Specifically: Victor can pick a random x, just as in the real protocol. Now he can pick the value y also at random. Note that for every ‚Äúm‚Äù there will exist a line that passes through (x, y). But now Victor has a problem: to complete the protocol, he will need to make a new box containing ‚Äúb‚Äù, such that b = y ‚Äì mx. There is no obvious way for Victor to calculate b given only the information he has in the clear. To address this third requirement, we must therefore demand a fundamentally new capability from our magic boxes. Concretely, we can imagine that there is some way to ‚Äúmanufacture‚Äù new magic boxes from existing ones, such that the new boxes contain a calculated value. This amounts to reversing the linear equation and then performing multiplication and subtraction on ‚Äúboxed‚Äù values, so that we end up with: What‚Äôs that, you say? This new requirement looks totally arbitrary? Well, of course it is. But let‚Äôs keep in mind that we started out by demanding magical boxes with special capabilities. Now I‚Äôm simply adding one more magical capability. Who‚Äôs to say that I can‚Äôt do this? Recall that the resulting transcript must be statistically identical to the ones that Victor would get from Peggy. It‚Äôs easy enough to show that the literal values (x, y, b) will all have the same distribution in both versions. The statistical distribution of our ‚Äúmanufactured magical boxes‚Äù is a little bit more complicated, because what the heck does it mean to ‚Äúmanufacture a box from another box,‚Äù anyway? But we‚Äôll just specify that the manufactured ones must look identical to the ones created in the real protocol. Of course back in the real world this matters a lot. We‚Äôll need to make sure that our magical box objects have the necessary features, which are (1) the ability to test whether a given (x, y) is on the line, and (2) the ability to manufacture new boxes containing ‚Äúb‚Äù from another box containing ‚Äúm‚Äù and a point (x, y), while ensuring that the manufactured boxes are identical to magical boxes made the ordinary way. How do we build a magical box? An obvious idea might be to place the secret values m and b each into a standard one-way function and then send over F(m) and F(b). This clearly achieves the goal of hiding the values of these two values: unfortunately, it doesn‚Äôt let us do very much else with them. Indeed, the biggest problem with simple one-way functions is that there is only one thing you can do with them. That is: you can generate a secret x, you can compute the one-way function F(x), and then you can reveal x for someone else to verify. Once you‚Äôve done this, the secret is ‚Äúgone.‚Äù That makes simple one-way functions fairly limiting. But what if F is a different type of one-way function that has some additional capabilities? In the early 1980s many researchers were thinking about such one-way functions. More concretely, researchers such as Tahir Elgamal were looking at a then-new ‚Äúcandidate‚Äù one-way function that had been proposed by Whitfield Diffie and Martin Hellman, for use in their eponymous key exchange protocol. Concretely: let p be some large non-secret prime number that defines a finite field. And let g be the ‚Äúgenerator‚Äù of some large cyclic subgroup of prime order q contained within that field.3 If these values are chosen appropriately, we can define a function F(x) as follows: The nice thing about this function is that, provided g and p are selected appropriately, it is (1) easy to compute this function in the normal direction (using square-and-multiply modular exponentiation) and yet is (2) generally believed to be hard to invert. Concretely, as long x is randomly selected from the finite field defined by {0, ‚Ä¶, q-1}, then recovering x from F(x) is equivalent to the discrete logarithm problem. But what‚Äôs particularly nifty about this function is that it has nice algebraic properties. Concretely, given F(a) and F(b) computed using the function above, we can easily compute F(a + b mod q). This is because: Similarly, given F(a) and some known scalar c, we can compute F(a \cdot c): We can also combine these capabilities. Given F(m) and F(b) and some x, we can compute F(y) where y = mx + b mod q. Almost magically means we can compute linear equations over values that have been ‚Äúhidden‚Äù inside a one-way function, and then we can compare the result to a direct (alleged) calculation of y that someone else has handed us: Implicitly, this gives us the magic box we need to realize Chris‚Äôs protocol from the previous section. The final protocol looks like this: Appropriate cyclic groups can also be constructed within certain elliptic curves, such as the NIST P-256 and secp256k1 curve (used for Schnorr signatures in Bitcoin) as well as the EdDSA standard, which is simply a Schnorr signature implemented in the Ed25519 Edwards curve. Here the exponentiation is replaced with scalar point multiplication, but the core principles are exactly the same. For most people, you‚Äôre probably done at this point. You may have accepted my claim that these ‚Äúdiscrete logarithm‚Äù-based one-way functions are sufficient to hide the values (m, b) and hence they‚Äôre magic-box-like. But you shouldn‚Äôt! This is actually a terrible thing for you to accept. After all, modular-exponentiation functions are not magical boxes. They‚Äôre real ‚Äúthings‚Äù that might potentially leak information about the points ‚Äúm‚Äù and ‚Äúb‚Äù, particularly since Victor will be given many different values to work with after several runs of the protocol. To convince ourselves that the boxes don‚Äôt leak, we must use the intuition I discussed further above. Specifically, we need to show that it‚Äôs possible to ‚Äúsimulate‚Äù transcripts without ever talking to Peggy herself, given only her public key . Recall that in the discussion above, the approach we used was to pick a random point (x, y) first, and then ‚Äúmanufacture‚Äù a box as follows: In our realized setting, this is equivalent to computing directly from and (x, y). Which we can do as follows: (If you‚Äôre picky about things, here we‚Äôre abusing division as shorthand to imply multiplication by the multiplicative inverse of the final term.) It‚Äôs easy enough to see that the implied value b = y ‚Äì mx is itself distributed identically to the real protocol as long as (x, y) are chosen randomly. In that case it holds that will be distributed identically as well, since there is a one-to-one mapping between between each b and the value in the exponent. This is an extremely convenient feature of this specific magic box. Hence we can hope that this primitive meets all of our security requirements. From ID protocols to signatures: Fiat-Shamir While the post so far has been about identification protocols, you‚Äôll notice that relatively few people use interactive ID protocols these days. In practice, when you hear the name ‚ÄúSchnorr‚Äù it‚Äôs almost always associated with signature schemes. These Schnorr signatures are quite common these days: they‚Äôre used in Bitcoin and form the basis for schemes like EdDSA. There is, of course, a reason I‚Äôve spent so much time on identification protocols when our goal was to get to signature schemes. That reason is due to a beautiful ‚Äútrick‚Äù called the Fiat-Shamir heuristic that allows us to effortlessly move from three-move identification protocols (often called ‚Äúsigma protocols‚Äù, based on the shape of the capital greek letter) to non-interactive signatures. Let‚Äôs talk briefly about how this works. The key observation of Fiat and Shamir was that Victor doesn‚Äôt really do very much within a three-move ID protocol: indeed, his major task is simply to select a random challenge. Surely if Peggy could choose a random challenge on her own, perhaps somehow based off a ‚Äúmessage‚Äù of her choice, then she could eliminate the need to interact with Victor at all. In this new setting, Peggy would compute the entire transcript on her own, and she could simply hand Victor a transcript of the protocol she ran with herself (as well as the message.) Provided the challenge value x could be bound ‚Äútightly‚Äù to a message, then this would convert an interactive protocol like the Schnorr identification protocol into a signature scheme. One obvious idea would be to take some message M and compute the challenge as x = H(M). Of course, as we‚Äôve already seen above, this is a pretty terrible idea. If Peggy is allowed to know the challenge value x, then she can trivially ‚Äúsimulate‚Äù a protocol execution transcript using the approach described in the previous section ‚Äî even if she does not know the secret key. The resulting signature would be worthless. For Peggy to pick the challenge value x by herself, therefore, she requires a strategy for generating x that (1) can only be executed after she‚Äôs ‚Äúcommitted‚Äù to her first magic box containing b, and (2) does not allow her predict or ‚Äústeer‚Äù the value x that she‚Äôll get at the end of this process. The critical observation made by Fiat and Shamir was that Peggy could do this if she possessed a sufficiently strong hash function H. Their idea was as follows. First, Peggy will generate her value b. Then she will place it into a ‚Äúmagic box‚Äù as in the normal protocol (as in the instantiation above.) Finally, she will feed her boxed value(s) for both m and b as well as an optional ‚Äúmessage‚Äù M into the hash function as follows: An evasive puzzle. Finally, she‚Äôll compute the rest of the protocol as expected, and hand Victor the transcript which he can check by re-computing the hash function on the inputs to obtain x and verifying that y is correct (as in the original protocol.) (A variant of this approach has Peggy give Victor a slightly different transcript: here she sends to Victor, who now computes and tests whether . I will leave the logic of this equation for the reader to work out. Commenter Imuli below points to a great StackExchange post that shows all the different variants of Schnorr people have built by using tricks like this.) For this entire idea to work properly, it must be hard for Peggy to identify a useful input to the hash function that provides an output that she can use to fake the transcript. In practice, this requires a hash function where the ‚Äúrelation‚Äù between input and output is what we call evasive: namely, that it is hard to find two points that have a useful relationship for simulating the protocol. In practice we often model these hash functions in security proofs as though they‚Äôre random functions, which means the output is verifiably unrelated to the input. For long and boring reasons, this model is a bit contrived. We still use it anyway. What other magic boxes might there be? As noted above, a critical requirement of the ‚Äúmagic box Schnorr‚Äù style of scheme is that the boxes themselves must be instantiated by some kind of one-way function: that is, there must be no efficient algorithm that can recover Peggy‚Äôs random secret key from within the box she produces, at least without exhaustively testing, or using some similarly expensive (super-polynomial time) attack. The cyclic group instantiation given above satisfies this requirement provided that the discrete logarithm problem (DLP) is hard in the specific group used to compute it. Assuming your attacker only has a classical computer, this assumption is conjectured to hold for sufficiently-large groups constructed using finite-field based cryptography and in certain elliptic curves. But nothing says your adversary has to be a classical computer. And this should worry us, since we happen to know that the discrete logarithm problem is not particularly hard to solve given an appropriate quantum computer. This is due to the existence of efficient quantum algorithms for solving the DLP (and ECDLP) based on Shor‚Äôs algorithm. To deal with this, cryptographers have come up with a variety of new signature schemes that use different assumptions. In my next post I‚Äôm going to talk about one of those schemes, namely the Dilithium signature scheme, and show exactly how it relates to Schnorr signatures. This post is continued in Part 2. Notes: ‚ÄúFolklore‚Äù in cryptography means that nobody knows who came up with the idea. In this case these ideas were proposed in a slightly different context (one-time signatures) by folks like Ralph Merkle. Since there are distinct subsets to pick from, the probability that Veronica will select exactly the same subset as Victor did ‚Äî allowing him to answer her challenge properly ‚Äî can be made quite small, provided N and k are chosen carefully. (For example, N=128 and k=30 gives about and so Evil Victor will almost never succeed.) Some of these ideas date back to the Elgamal signature scheme, although that scheme does not have a nice security reduction. In the real proof, we actually rely on a property called ‚Äúrewinding.‚Äù Here we can make the statement that if there exists some algorithm (more specifically, an efficient probabilistic Turing Machine) that, given only Peggy‚Äôs public key, can impersonate Peggy with high probability: then it must be possible to ‚Äúextract‚Äù Peggy‚Äôs secret value m from this algorithm. Here we rely on the fact that if we are handed such a Turing machine, we can run it (at least) twice while feeding in the same random tape, but specifying two different x challenges. If such an algorithm succeeds with reasonable probability in the general case, then we should be able to obtain two distinct points (x, y), (x‚Äô, y‚Äô) and then we can just solve for (m, b). I‚Äôm specifying a prime-order subgroup here not because it‚Äôs strictly necessary, but because it‚Äôs very convenient to have our ‚Äúexponent‚Äù values in the finite field defined by {0, ‚Ä¶, q-1} for some prime q. To construct such groups, you must select the primes q, p such that p = 2q + 1. This ensures that there will exist a subgroup of order q within the larger group defined by the field Fp. By Matthew Greenin fundamentalsOctober 6, 2023January 14, 20246,115 Words3 Comments Some rough impressions of Worldcoin Recently a reader wrote in and asked if I would look at Sam Altman‚Äôs Worldcoin, presumably to give thoughts on it from a privacy perspective. This was honestly the last thing I wanted to do, since life is short and this seemed like an obvious waste of it. Of course a project devoted to literally scanning your eyeballs was up to some bad things, duh. However: the request got me curious. Against my better judgement, I decided to spend a few hours poking around Worldcoin‚Äôs documentation and code ‚Äî in the hope of rooting out the obvious technical red flags that would lead to the true Bond-villain explanation of the whole thing. Because surely there had to be one. I mean: eyeball scanners. Right? More seriously, this post is my attempt to look at Worldcoin‚Äôs system from a privacy-skeptical point of view in order to understand how risky this project actually is. The risks I‚Äôm concerned about are twofold: (1) unintentional risks to users that could arise due to carelessness on Worldcoin‚Äôs part, and (2) ‚Äúintentional‚Äù risks that could result from future business decisions (whether they are currently planned or not.) For those who don‚Äôt love long blog posts, let me save you a bunch of time: I did not find as many red flags as I expected to. Indeed, while I‚Äôm still (slightly) leaning towards the idea that Worldcoin is the public face of some kind of moon-laser-esque evil plan, my confidence in that conclusion is lower than it was going in. Read on for the rest. What is Worldcoin and why should I care? Worldcoin is a new cryptocurrency funded by Sam Altman and run by Alex Blania. According to the project‚Äôs marketing material, the goal of the project is to service the ‚Äúglobal unbanked‚Äú, which it will do by somehow turning itself into a form of universal basic income. While this doesn‚Äôt seem like much of a business plan, it‚Äôs pretty standard fare for a cryptocurrency project. Relatively few of these projects have what sound like ‚Äúreal use cases,‚Äù and it‚Äôs pretty common for projects to engage in behavior that amounts to ‚Äúgiving things away for free in the hope that somehow this will make everyone use the thing.‚Äù The Worldcoin Orb. What sets Worldcoin apart from other projects is that the free money comes with a surprising condition: in order to join the Worldcoin system and collect free tokens, users must hand over their eyeballs. Ok, that‚Äôs obviously a bit dramatic. More seriously: the novel technical contribution of Worldcoin is a proprietary biometric sensor called ‚Äúthe orb‚Äù, which allows the system to uniquely identify users by scanning their iris, which not-coincidentally is one of the most entropy-rich biometric features that humans possess. Worldcoin uses these scans to produce a record that they refer to as a ‚Äúproof of personhood‚Äù, a credential that will, according to the project, have many unspecified applications in the future. Whatever the long term applications for this technology may be, the project‚Äôs immediate goal is to enroll hundreds of millions of eyeballs into their system, using the iris scan to make sure that no user can sign up twice. A number of people have expressed concern about the potential security and privacy risks of this plan. Even more critically, people are skeptical that Worldcoin might eventually misuse or exploit this vast biometric database it‚Äôs building. Worldcoin has arguably made themselves more vulnerable to criticism by failing to articulate a realistic-sounding business case for its technology (as I said above, something that is common to many cryptocurrency projects.) The project has instead argued that their biometric database either won‚Äôt or can‚Äôt be abused ‚Äî due to various privacy-preserving features they‚Äôve embedded into it. Worldcoin‚Äôs claims Worldcoin makes a few important claims about their system. First, they insist that the iris scan has exactly one purpose: to identify duplicate users at signup. In other words, they only use it to keep the same user from enrolling multiple times (and thus collecting duplicate rewards.) They have claimed ‚Äî though not particularly consistently, see further below! ‚Äî that your iris itself will not serve as any kind of backup ‚Äúsecret key‚Äù for accessing wallet funds or financial services. This is a pretty important claim! A system that uses iris codes only to recognize duplicate users will expose its users to relatively few direct threats. That is, the worst an attacker can do is find ways to defraud Worldcoin directly. By contrast, a system that uses biometrics to authorize financial transactions is potentially much more dangerous for users. TL;DR if iris scans can be used to steal money, the whole system is quite risky. Second, Worldcoin claims that it will not store raw images of your actual iris ‚Äî unless you ask them to. They will only store a derived value called an ‚Äúiris code‚Äù: The final claim that Worldcoin makes is that they will not tie your biometric to a substantial amount of personal or financial data, which could make their database useful for tracking. They do this by making the provision of personally identifiable information (PII) ‚Äúoptional‚Äù rather than mandatory. And further, they use technology to make it impossible for the company to tie blockchain transactions back to your iris record: There are obviously still some concerns in here: notably the claim that you ‚Äúdo not need‚Äù any personal information does leave room for people to ‚Äòvoluntarily‚Äô provide it. This could be a problem in settings where unsupervised Worldcoin contractors are collecting data in relatively poor communities. And frankly it‚Äôs a little weird that Worldcoin allows users to submit this data in the first place, if they don‚Äôt have plans to do things with it. Worldcoin in a technical nutshell NB: Much of the following is based on Worldcoin‚Äôs own documentation, as well as a review of some of their contract code. I have not verified all of this, and portions may be incorrect. Worldcoin operates two different technologies. The first is a standard EVM-based cryptocurrency token (ERC20), which operates on the ‚ÄúLayer 2‚Äù Optimism blockchain. The company can create tokens according to a limited ‚Äúinflation supply‚Äù model and then give them away to people. Mostly this part of the project is pretty boring, although there are some novel aspects to Worldcoin‚Äôs on-chain protocol that impact privacy. (I‚Äôll discuss those further below.) The novel element of Worldcoin is its biometric-based ‚Äúproof of personhood‚Äù identity verification tech. Worldcoin‚Äôs project website handwaves a lot about future applications of the technology, many of them involving ‚ÄúAI.‚Äù For the moment this technology is used for one purpose: to ensure that each Worldcoin user has registered only once for its public currency airdrop. This assurance allows Worldcoin to provide free tokens to registered users, without worrying that the same person will receive multiple rewards. To be registered into the system, users visit a Worldcoin orb scanning location, where they must verify their humanity to a real-life human operator. The orb purports to contain tamper-resistant hardware that can scan one or both of the user‚Äôs eyes, while also performing various tests to ensure that the eyeballs belong to a living, breathing human. This sensor takes high-resolution iris images, which are then then processed internally within the the orb using an algorithm selected by Worldcoin. The output of this process is a sort of ‚Äúdigest value‚Äù called an iris code. Iris codes operate like a fuzzy ‚Äúhash‚Äù of an iris: critically, one iris code can be compared to another code, such that if the ‚Äúdistance‚Äù between the pair is small enough, the two codes can be considered to derive from the same iris. That means the coding must be robust to small errors caused by the scanning process, and even to small age-related changes within the users‚Äô eyes. (The exact algorithm Worldcoin uses for this today is not clear to me, since their documentation mentions two different approaches: one based on machine learning, and one using more standard image processing techniques.) In theory the use of iris code algorithms computed within tamper-resistant hardware should mean that your raw iris scans are safe ‚Äî the orb never needs to output them, it can simply output this (hopefully) much less valuable iris code. (In practice, however, this is not quite true: Worldcoin allows users to opt in to ‚Äúdata custody‚Äú, which means that their raw iris scans will also be stored by the project. Worldcoin claims that these images will be encrypted, though presumably using a key that the project itself holds. Custody is promoted as a way to enable updates to the iris coding algorithm without forcing users to re-scan their eyes at an orb. It is not clear how many users have opted in to this custody procedure, and it isn‚Äôt great.) Once the orb has scanned a user, the iris code is uploaded to a server operated by the Altman/Blania company Tools for Humanity. The code may or may not be attached to other personal user information that Worldcoin collects, such as phone numbers and email addresses (Worldcoin‚Äôs documents are slightly vague on this point, except for noting that this data is ‚Äúoptional.‚Äù) The server now compares the new code against its library of previously-registered iris codes. If the new code is sufficiently ‚Äúdistant‚Äù from all previous codes, the system will consider this user to be a unique first-time registered user. (The documents are also slightly vague about what happens if they‚Äôre already registered, see much further below.) To complete the enrollment process, users download Worldcoin‚Äôs wallet software to generate two different forms of credential: A standard Ethereum wallet public and secret key, which is used to actually control funds in Worldcoin‚Äôs ERC20 contract. A specialized digital credential called an ‚Äúidentity commitment‚Äù, which comes with its own matching secret key material. Critically, none of this key material is derived from the user‚Äôs biometric scan. The (public) ‚Äúidentity commitment‚Äù, is uploaded to Tools for Humanity, where it is stored in the database along with the user‚Äôs iris code, while all of the secret key material is stored locally on the user‚Äôs phone (with a possibility for cloud backup*). As a final step, Tools for Humanity exports the user‚Äôs identity commitment (but not the iris code data!) into a smart-contract-managed data structure that resides on Worldcoin‚Äôs blockchain. Not shown: a user can choose to upload their iris scans and keep them in escrow. (source) Phew. Ok. Only one more thing. As mentioned previously, the entire purpose of this iris scanning business is to allow users to perform assertions on their blockchain that ‚Äúprove‚Äù they are unique human beings: the most obvious one being a request for airdropped tokens. (It is important to note that these assertions happen after the signup process, and only use the key material in your wallet: this means it‚Äôs possible to sell your identity data once you‚Äôve been scanned into the system.) From a privacy perspective, a very real concern with these assertions is that Worldcoin (aka Tools for Humanity) could monitor these on-chain transactions, and thus link the user‚Äôs wallet back to the unique biometric record it is associated with. This would instantly create a valuable source of linked biometric and financial data, which is one of the most obvious concerns about this type of system. To their credit: Worldcoin seems to recognize that this is a problem. And their protocols address those concerns in two ways. First: they do not upload the user‚Äôs Ethereum wallet address to Tools for Humanity‚Äôs servers. This means that any financial transactions a user makes should not be trivially linkable to the user‚Äôs biometric credentials. (This obviously doesn‚Äôt make linkage impossible: transactions could be linked back to a user via public blockchain analysis at some later point.) But Worldcoin does try to avoid the obvious pitfall here. This solves half the problem. However, to enable airdrops to a user‚Äôs wallet, Worldcoin must at some point link the user‚Äôs identity commitment to their wallet address. Implemented naively, this would seem to require a public blockchain transaction that would mention both a destination wallet address and an identity commitment ‚Äî which would implicitly link these (via the binding known to the Tools for Humanity server) to the user‚Äôs biometric iris code. This would be quite bad! Worldcoin avoids this issue in a clever way: their blockchain uses zero knowledge proofs to authorize the airdrop. Concretely, once an identity commitment has been placed on chain, the user can use their wallet to produce a privacy-preserving transaction that ‚Äúproves‚Äù the following statement: ‚ÄúI know the secret key corresponding to a valid identity commitment on the chain, and I have not previously made a proof based on this commitment.‚Äù Most critically, this proof does not reveal which commitment the user is referencing. These protections make it relatively more difficult for any outside party (including Tools for Humanity) to link this transaction back to a user‚Äôs identity and iris code. Worldcoin conducts these transactions using a zero-knowledge credential system called Semaphore (developed by the Ethereum project, using an architecture quite similar to Zcash.) Although there are a fabulous number of different ways this could all go wrong in practice (more on this below), from an architectural perspective Worldcoin‚Äôs approach seems well thought-out. Critically, it should serve to prevent on-chain airdrop transactions from being directly linked to the biometric identifiers that Tools for Humanity records. To summarize all of the above‚Ä¶ if Worldcoin works as advertised, the following should be true after you‚Äôve registered and used the system: Tools for Humanity will have a copy of your iris code stored in its database. Tools for Humanity may also have stored a copy of your raw iris scans, assuming you ‚Äúopted in‚Äù to data custody. (This data will be encrypted under a key TfH knows.) Tools for Humanity will store the mapping between each iris code and the corresponding ‚Äúidentity commitment‚Äù, which is essentially the public component of your ID credential. Tools for Humanity may have bound the iris code to other PII they optionally collected, such as phone numbers and email addresses. Tools for Humanity should not know your wallet address. All of your secret keys will be stored in your phone, and will not be backed up anywhere else unless you enable backups. (Worldcoin may also allow you to ‚Äúrecover‚Äù using your phone number, but this feature doesn‚Äôt work for me and so I‚Äôm not sure what it does.) If you conduct any transactions on the blockchain that reference the identity commitment, neither Worldcoin or TfH should be able to link these to your identity. Finally (and critically): no biometric information (or biometric-derived information) will be sent to the blockchain or stored on your phone. Tom Cruise changing his eyeball passwords in Minority Report. As you can see, this system appears to avoid some of the more obvious pitfalls of a biometric-based blockchain system: crucially, it does not upload raw biometric information to a volunteer-run blockchain. Moreover, iris scans are not used to derive key material or to enable spending of cryptocurrency (though more on this below.) The amount of data linked to your biometric credential is relatively minimal (except for those phone numbers!), and transactions on chain are not easily linked back to the credential. This architecture rules out many threats that might lead to your eyeballs being stolen or otherwise needing to be replaced. What about future uses of iris data? As I mentioned earlier, a system that only uses iris codes to recognize duplicate users poses relatively few threats to users themselves. By contrast, a system that uses biometrics to authorize financial transactions could potentially be much more risky. To abuse a clich√©: if iris scans can be used to steal money, then users might need to sleep with their eyes open closed. While it seems that Worldcoin‚Äôs current architecture does not authorize transactions using biometric data, this does not mean the platform will behave this way forever. The existence of a biometric database could potentially create incentives that force the company to put this data to more dangerous use. Let me be more specific. The most famous UX problem in cryptocurrency is that cryptocurrency‚Äôs ‚Äúdefining‚Äù feature ‚Äî self-custody of funds ‚Äî is incredibly difficult to use. Users constantly lose their wallet secret keys, and with them access to all of their funds. This problem is endemic even to sophisticated first-world cryptocurrency adopters who have access to bank safety-deposit boxes and dedicated hardware wallets. This is going to be an even more serious problem for a cryptocurrency that purports to be the global ‚Äúuniversal basic income‚Äù for billions of people who lack those resources. If Worldcoin is successful by its own standards ‚Äî i.e., it becomes a billion-user global cryptocurrency ‚Äî it‚Äôs going to have to deal with the fact that many users are going to lose their wallet secrets. Those folks will want to re-authorize themselves to get those funds back. Unlike most cryptocurrencies, Worldcoin‚Äôs biometric database provide the ultimate resource to make that possible. The Worldcoin white paper describes how biometrics can be used as a last-ditch account recovery mechanism. This is not currently mentioned on the Worldcoin website. At present the company cannot use biometrics to regain access to lose wallets, but they have many of the tools they need to get there. In the current system, the situation is as follows: In principle, Tools for Humanity‚Äôs servers can re-bind an existing iris code to a new identity commitment. They can then send that new commitment to the chain using a special call in the smart contract. While this could allow users to obtain a second airdrop (if Worldcoin/TfH choose to allow it), it would not (at present) allow a user to take control of an existing wallet. To enable wallet recovery, therefore, Worldcoin would need to change its on-chain contracts This would involve either replacing the ERC20 contract with one that admits ID-authorized recovery, or ‚Äî more practically ‚Äî deploying a new form of smart contract wallet that allows users to ‚Äúreset‚Äù ownership via an identity assertion. Worldcoin hasn‚Äôt done either of these things yet, and I‚Äôm curious to see how they will withstand the pressure in the long term. What other privacy concerns are there? Another obvious concern is that Tools for Humanity could use its biometric database as a kind of ‚Äústone soup‚Äù to build a future (and more privacy-invasive) biometric database, which could then be deployed for applications that have nothing to do with cryptocurrency. For example, an obvious application would be to record and authorize credit for customers who lack government-issued ID. This is the kind of thing I can see Silicon Valley VC investors getting very excited over. There is nothing, in principle, stopping the project from pivoting in this direction in the future. On the other hand, a reasonable counterpoint is that if Worldcoin really wanted to do this, they could just have done it from the get-go: enrolling lots of people into some weird privacy-friendly cryptocurrency seems like a bit of a distraction. But perhaps I am not being imaginative enough. A related concern is that Worldcoin might somehow layer further PII onto their existing database, or find other ways to make this data more valuable ‚Äî even if the on-chain data is unlinkable. Some reports indicate that Worldcoin employees do collect phone numbers and other personally-identifying information as part of the enrollment process. It‚Äôs really not clear why Worldcoin would collect this data if it doesn‚Äôt plan to use it somehow in the future. As a final matter, there is a very real possibility that Worldcoin somehow has the best intentions ‚Äî and yet will just screw everythingup. Databases can be stolen. Zero-knowledge proofs are famously difficult to get right, and even small timing-based vulnerabilities can create a huge privacy leak: for example it should not be very hard to guess at the linkage between keys and identity commitments based solely on when the two are posted on chain. Worldcoin‚Äôs system almost certainly deploys various back-end servers to assist with proof generation (e.g., to obtain Merkle paths) and the logs of these servers could further undermine privacy. This will only get worse as Worldcoin proceeds to ‚Äúdecentralize‚Äù the World ID database, whatever that entails. Conclusion I came into this post with a great deal of skepticism about Worldcoin: I was 100% convinced that the project was an excuse to bootstrap a valuable biometric database that would be used e-commerce applications, and that Worldcoin would have built their system to maximize the opportunities for data collection. After poking around the project a bit, I‚Äôm honestly still a little bit skeptical. I think Worldcoin is ‚Äî at some level ‚Äî an excuse to bootstrap a valuable biometric database for e-commerce applications. But I would say my confidence level is down to about 40%. Mostly this is because I‚Äôm not able to come up with a compelling story for what an evil-future-Worldcoin will do with a big database that consists of iris codes plus a few phone numbers. And more critically, I‚Äôm pleasantly surprised by the amount of thought that Worldcoin has put into keep transaction data unlinked from its ID database. No doubt there are still things that I‚Äôve missed here, and perhaps others will chime in with some details and corrections. In the meantime, I still would not personally stick my eyeballs into the orb, but I can‚Äôt quite tell you how it would hurt. Notes: * When you sign up for WorldApp it gives you the option to back up your keys to your own provider, like iCloud. This is bad but not really Worldcoin‚Äôs fault. You can also provide your phone number for some sort of recovery purpose. What‚Äôs happening here is very interesting and I‚Äôd like to understand it better, but the feature simply did not work for me. By Matthew Greenin cryptocurrencyAugust 21, 2023August 22, 20233,631 Words2 Comments On Ashton Kutcher and Secure Multi-Party Computation Back in March I was fortunate to spend several days visiting Brussels, where I had a chance to attend a panel on ‚Äúchat control‚Äú: the new content scanning regime being considered by the EU Commission. Among various requirements, this proposed legislation would mandate that client-side scanning technology be incorporated into encrypted text messaging applications like Signal, WhatsApp and Apple‚Äôs iMessage. The scanning tech would examine private messages for certain types of illicit content, including child sexual abuse media (known as CSAM), along with a broad category of textual conversations that constitute ‚Äúgrooming behavior.‚Äù I have many thoughts about the safety of the EU proposal, and you can read some of them here. (Or you‚Äôre interested in the policy itself, you can read this recent opinion by the EU‚Äôs Council‚Äôs Legal Service.) But although the EU proposal is the inspiration for today‚Äôs post, it‚Äôs not precisely what I want to talk about. Instead, I‚Äôd like to clear up some confusion I‚Äôve noticed around the specific technologies that many have proposed to use for building these systems. Also: I want to talk about Ashton Kutcher. Ashton Kutcher visits the EU parliament in March 2023 (photo: Roberta Metsola.) It turns out there were a few people visiting Brussels to talk about encryption this March. Only a few days before my own visit, Ashton Kutcher gave a major speech to EU Parliament members in support of the Commission‚Äôs content scanning proposal. (And yes, I‚Äôm talking about that Ashton Kutcher, the guy who played Steve Jobs and is married to Mila Kunis.) Kutcher has been very active in the technical debate around client-side scanning. He‚Äôs the co-founder of an organization called Thorn, which aims to develop cryptographic technology to enable content scanning. In March he gave an impassioned speech to the EU Parliament urging the deployment of these technologies, and remarkably he didn‚Äôt just talk about the policy side of things. When asked how to balance user privacy against the needs of scanning, he even made a concrete technical proposal: to use fully-homomorphic encryption (FHE) as a means to evaluate encrypted messages. Now let me take a breath here before my head explodes. I promise I am not one of those researchers who believes only subject-matter experts should talk about cryptography. Really I‚Äôm not! I write this blog because I think cryptography is amazing and I want everyone talking about it all the time. Seeing mainstream celebrities toss around phrases like ‚Äúhomomorphic encryption‚Äù is literally a dream come true and I wish it happened every single day. And yet, there are downsides to this much winning. I ran face-first into some of those downsides when I spoke to policy experts about Kutcher‚Äôs proposal. Terms like fully homomorphic encryption can be confusing and off-putting to non-cryptographers. When filtered through people who are not themselves experts in the technology, these ideas can produce the impression that cryptography is magical pixie dust we can sprinkle on all the hard problems in the world. And oh how I wish that were true. But in the real world, cryptography is full of tradeoffs. Solving one problem often just makes new problems, or creates major new costs, or else shifts the risks and costs to other parts of the system. So when people on various sides of the debate asked me whether ‚Äúfully-homomorphic encryption‚Äù could really do what Kutcher said it would, I couldn‚Äôt give an easy five-word answer. The real answer is something like: (scream emoji) it‚Äôs very complicated. That‚Äôs a very unsatisfying thing to have to tell people. Out here in the real world the technical reality is eye-glazing and full of dragons. Which brings me to this post. What Kutcher is really proposing is that we to develop systems that perform privacy-preserving computation on encrypted data. He wants to use these systems to enable ‚Äúprivate‚Äù scanning of your text messages and media attachments, with the promise that these systems will only detect the ‚Äúbad‚Äù content while keeping your legitimate private data safe. This is a complicated and fraught area of computer science. In what goes below, I am going to discuss at a high and relatively non-technical level the concepts behind it: what we can do, what we can‚Äôt do, and how practical it all is. In the process I‚Äôll discuss the two most powerful techniques that we have developed to accomplish this task: namely, multi-party computation (MPC) and, as an ingredient towards achieving the former, fully-homomorphic encryption (FHE). Then I‚Äôll try to clear up the relationship between these two things, and explain the various tradeoffs that can make one better than the other for specific applications. Although these techniques can be used for so many things, throughout this post I‚Äôm going to focus on the specific application being considered in the EU: the use of privacy-preserving computation to conduct content scanning. This post will not require any mathematics or much computer science, but it will require some patience. So find a comfortable chair and buckle in. Computing on private data Encryption is an ancient technology. Roughly speaking, it provides the ability to convert meaningful messages (and data) into a form that only you, and your intended recipient(s) can read. In the modern world this is done using public algorithms that everyone can look at, combined with secret keys that are held only by the intended recipients. Modern encryption is really quite excellent. So as long as keys are kept safe, encrypted data can be sent over insecure networks or stored in risky locations like your phone. And while occasionally people find a flaw in an implementation of encryption, the underlying technology works very well. But sometimes encryption can get in the way. The problem with encrypted data is that it‚Äôs, well, encrypted. When stored in this form, such data is virtually useless for practical purposes like performing calculations. Before you can compute on that data, you often need to first decrypt it and thus remove all the beautiful protections we get from encryption. If your goal is to compute on multiple pieces of data that originate from different parties, the problem can become even more challenging. Who can we trust to do the computing? An obvious solution is to decrypt all that data and hand it to one very trustworthy person, who will presumably swear not to steal it or get hacked. Finding those parties can be quite challenging. Fortunately for us all, the first academic cryptographers also happened to be computer scientists, and so this was exactly the sort of problem that excited them. Those researchers quickly devised a set of specific and general techniques designed to solve these problems, and also came up with a cool name for them: secure multi-party computation, or MPC for short. MPC: secure private computation (in six eight ten paragraphs) The setting of MPC is fairly simple: imagine that we have two (or more!) parties that each have some private data they don‚Äôt want to give to anyone else. Yet each of the parties is willing to provide their data as input to some specific computation, and are all willing to reveal the output of that computation ‚Äî either to everyone involved, or perhaps just to some agreed subset of the parties. Can these parties now perform the computation jointly, without appointing a trusted party? Let‚Äôs make this easier by using a concrete example. Imagine a group of workers all know their own salaries, but don‚Äôt know anything about anyone else‚Äôs salary. Yet they wish to compute some statistics over their salary data: for example, the average of their salaries. These workers aren‚Äôt willing to share their own salary data with anyone else, but they are willing to submit it as one input in a large calculation under the strict condition that only the final result is ever revealed. This might seem contrived to you, but it is in fact a real problem that some people have used MPC to solve. An MPC protocol allows the workers to do this, without appointing a trusted central party or revealing their inputs (and intermediate calculations) to anyone else. At the conclusion of the protocol each party will learn only the result of the calculation: The ‚Äúcloud‚Äù at the center of this diagram is actually a complicated protocol where every party sends messages to every other party. MPC protocols typically provide strong provable guarantees about their properties. The details vary, but typically speaking: no party will learn anything about the other parties‚Äô inputs. Indeed they won‚Äôt even learn any partial information that might be produced during the calculation. Even better, all parties can be assured that the result will be correct: as long as all parties submit valid inputs to the computation, none of them should be able to force the calculation to go awry. Now obviously there are caveats. In practice, using MPC is a bit like making a deal with a genie: you need to pay very careful attention to the fine print. Even when the cryptography works perfectly, this does not mean that computing your function is actually ‚Äúsafe.‚Äù In fact, it‚Äôs entirely possible to choose functions that when computed securely are still devastating to your privacy. For example: imagine that I use an MPC protocol to compute an average salary between myself and exactly one other worker. This could be a very bad idea! Note that if the other worker is curious, then she can figure out how much I make. That is: the average of our two wages reveals enough information that she find my wage given knowledge of her own input. This (obvious) caveat applies to many other uses of MPC, even when the technology works perfectly. This is not a criticism of MPC, just the observation that it‚Äôs a tool. In practice, MPC (or any other cryptographic technology) is not a privacy solution by itself, at least not in the sense of privacy that real-world human beings like to think about. It provides certain guarantees that may or may not be useful for providing privacy in the real world. What does MPC have to do with client-side scanning? We began this post by discussing client-side scanning for encrypted messaging apps. This is a perfect example of an application that fits the MPC (or two-party computation) use-case perfectly. That‚Äôs because in this setting we generally have multiple parties with secret data who want to perform some joint calculation on their inputs. In this setting, the first party is typically a client (usually a person using an encrypted messaging app like WhatsApp or Signal), who possesses some private text message or perhaps a media file they wish to send to another user. Under proposed law in the EU, their app could be legally mandated to ‚Äúscan‚Äù that image to see if it contains illegal content. According to the EU Commission, this scanning can be done in a variety of ways. For example, the device could compare an images against a secret database of known illicit content (typically using a specialized perceptual hash function.) However, while the EU plan starts there, their plans also get much more ambitious: they also intend to look for entirely new instances of illicit content as well as textual ‚Äúgrooming‚Äù conversations, possibly using machine learning (ML) models, that is, deep neural networks that will be trained to recognize data that fits these patterns. These various models must be sophisticated enough to understand entirely new images, as well as to derive meaning from complex interactive human conversation. None of this is likely to be very simple. Now most of this could be done using standard techniques on the client device, except for one major limitation. The challenge in this setting is that the provider doing the scanning usually wants to keep these hashes and/or ML models secret. There are several reasons for this. The first reason is that knowledge of the scanning model (or database of illicit content) makes it relatively easy for bad actors to evade the model. In other words, with only modest transformations it‚Äôs possible to modify ‚Äúbad‚Äù images so that they become invisible to ML scanners. Knowledge of the model can also allow for the creation of ‚Äúpoisoned‚Äù imagery: these include apparently-benign images (e.g., a picture of a cat) that trigger false positives in the scanner. (Indeed this such ‚Äúcolliding‚Äù images have been developed for some hash-based CSAM scanning proposals.) More worryingly, in some cases the hashes and neural network models can be ‚Äúreversed‚Äù to extract the imagery and textual content they were trained on: this has all kinds of horrifying implications, and could expose abuse victims to even more trauma. So here the user doesn‚Äôt want to send its confidential data to the provider for scanning, and the provider doesn‚Äôt want to hand its confidential model parameters to the user (or even to expose them inside the user‚Äôs phone, where they might be stolen by reverse-engineers.) This is exactly the situation that MPC was designed to handle: Sketch of a client-side scanning architecture that uses (two-party) MPC between the client and the Provider. The client inputs the content to be scanned, while the server provides its secret model and/or hash database. The protocol gives the provider a copy of the user‚Äôs content if and only if the model says it‚Äôs illicit content, otherwise the provider sees nothing. (Note in this variant, the output goes only to the Provider.) This makes everything very complicated. In fact, there has only been one real-world proposal for client-side CSAM scanning that has ever come (somewhat) close to deployment: that system was designed by Apple for a (now abandoned) client-side photo scanning plan. The Apple approach is cryptographically very ambitious: it uses neural-network based perceptual hashing, and otherwise exactly follows the architecture described above. However, critically: it relied on a neural-network based hash function that was not kept secret. Disastrous results ensued (see further below.) (If you‚Äôre interested in getting a sense of how complex this protocol is, here is a white paper describing how it works.) A diagram from the Apple CSAM scanning protocol. Ok, so what kind of MPC protocols are available to us? Multi-party computation is a broad category. It describes a class of protocols. In practice there are many different cryptographic techniques that allow us to realize it. Some of these (like the Apple protocol) were designed for specific applications, while others are capable of performing general-purpose computation. I promised this post would not go into the weeds, but it‚Äôs worth pointing out that general MPC techniques typically make use of (some combination of) three different techniques: secret sharing, circuit garbling, and homomorphic encryption. Often, efficient modern systems will use a mixture of two or three of those techniques, just to make everything more confusing because they‚Äôre to maximize efficiency. What is it that you need to know about these techniques? Here I‚Äôll try, in a matter of a few sentences (that will draw me endless grief) to try to summarize the strengths and disadvantages of each technique. Both secret sharing and garbling techniques share a common feature, which is that they require a great deal of data to be sent between the parties. In practice the amount of data sent between the parties will grow with (at least) the size of the inputs they‚Äôre computing on, but often will grow according to the complexity of the calculation they‚Äôre performing. For things like deep neural networks where both the data and calculation are huge, this generally results in fairly surprising amounts of data transfer. This is not usually considered to be a problem on the general Internet or within EC2 datacenters, where data transfer is cheap. It can be quite a challenge when one of those parties is using a cellphone, however. That makes any scheme using these technologies subject to some very serious limitations. Homomorphic encryption schemes take a different approach. These systems make use of specialized encryption schemes that are malleable. This means that encrypted data can be ‚Äúmodified‚Äù in useful ways without ever decrypting it. In a bit more detail: in fully-homomorphic encryption MPC systems, a first party can encrypt its data under a public key that it generates. It can then send the encrypted data to a second party. This second party can then perform calculations on the ciphertext while it is still encrypted ‚Äî adding and multiplying it together with other data (including data encrypted by the second party) to perform some calculation. Throughout this process all of the data remains encrypted. At the conclusion of this process, the second party will end up with a ‚Äúmodified‚Äù ciphertext that internally contains a final calculation result, but that it cannot read. To finish the protocol, the second party can send that ciphertext back to the first party, who can then decrypt it using its secret key and obtain the final output. The major upshot of the pure-FHE technique is that it substantially reduces the amount of data that the two parties need to transmit between them, especially compared to the other MPC techniques. The downside of this approach is‚Ä¶ well, there are several. One is that FHE calculations typically require vastly more computational effort (and hence time and carbon emissions) than the other techniques. Moreover, they may still require a good deal of data transfer ‚Äî in part because the number of calculations that one can perform on a given ciphertext is usually limited by ‚Äúnoise‚Äù that turns up within the ciphertext. Hence, calculations must either be very simple or else broken up into ‚Äúphases‚Äù, where the partial calculation result is decrypted and re-encrypted so that more computation can be done. This can be done interactively between the parties, or by the second party alone (using a technique called ‚Äúbootstrapping‚Äù) but in both cases the cost is either much more bandwidth exchanged or a great deal of extra computation. In practice, cryptographers rarely commit to a single approach. They instead combine all these techniques in order to achieve an appropriate balance of data-transfer and computational effort. These ‚Äúmixed systems‚Äù tend to have merely large amounts of data transfer and large amounts of computation, but are still amazingly efficient compared to the alternatives. For an example of this, consider this very optimized two-party MPC scheme aimed at performing neural network classification. This scheme takes (from the client) a 32√ó32 image, and evaluates a tiny 7-layer neural network held by a server in order to perform classification. As you can see, evaluating the model even on a single image requires about 8 seconds of computation and 200 megabytes of bandwidth exchange, for each image being evaluated: Source: MUSE paper, figure 8. These are the times for a 7-layer MiniONN network trained on the CIFAR-10 dataset. These numbers may seem quite high, but in fact they‚Äôre actually really impressive as these things go. Previous systems used nearly an order of magnitude more time and bandwidth to do their work. Maybe there will be further improvements in the future! Even on a pure efficiency basis there is much work to be done. What are the other risks of MPC in this setting? The final point I would like to make is that secure MPC (or MPC built using FHE as a tool) is not itself enough to satisfy the requirements of a safe content scanning system. As I mentioned above, MPC systems merely evaluate some function on private data. The question of whether that function is safe is left largely to the system designer. In the case of these content scanning systems, the safety of the resulting system really comes down to a question of whether the algorithms work well, particularly in settings where ‚Äúbad guys‚Äù can find adversarial inputs that try to disrupt the system. It also requires new techniques to ensure that the system cannot be abused. That is: there must be guarantees within the computation to ensure that the provider (or a party who hacks the provider) cannot change the model parameters to allow them to access your private content. This is a much longer conversation than I want to have in this post, because it fundamentally requires one to think about whether the entire system makes sense. For a much longer discussion of the risks, see this paper. This was nice, but I would like to learn more about each of these technologies! The purpose of this post was just to give the briefest explanation of the techniques that exist for performing all of these calculations. If you‚Äôre interested in knowing (a lot more!) about these technologies, take a look at this textbook by Evans, Kolesnikov and Rosulek. MPC is an exciting area, and one that is advancing every single (research) conference cycle. And maybe that is the lesson of this post: these technologies are still research techniques. It‚Äôs probably not quite time to put them out in the world. By Matthew Greenin academics, backdoors, fundamentals, protocolsMay 11, 2023May 12, 20233,473 Words9 Comments PRFs, PRPs and other fantastic things A few weeks ago I ran into a conversation on Twitter about the weaknesses of applied cryptography textbooks, and how they tend to spend way too much time lecturing people about Feistel networks and the boring details of AES. Some of the folks in this conversation suggested that instead of these things, we should be digging into more fundamental topics like ‚Äúwhat is a pseudorandom function.‚Äù (I‚Äôd link to the thread itself, but today‚Äôs Twitter is basically a forgetting machine.) This particular point struck a chord with me. While I don‚Äôt grant the premise that Feistel networks are useless, it is true that pseudorandom functions, also known as PRFs, are awfully fundamental. Moreover: these are concepts that get way too little coverage in (non-theory) texts. Since that seems bad for aspiring practitioners, I figured I‚Äôd spend a little time trying to explain these concepts in an intuitive way ‚Äî in the hopes that I can bring the useful parts to folks who aren‚Äôt being exposed to these ideas directly. This is going to be a high-level post and hence it will skip all the useful formalism. It‚Äôs also a little wonky, so feel free to skip it if you don‚Äôt really care. Also: since I need to be contrary: I‚Äôm going to talk about Feistel networks anyway. That bit will come later. What‚Äôs a PRF, and why should I care? Pseudorandom functions (PRFs) and pseudorandom permutations (PRPs) are two of the most fundamental primitives in modern cryptography. If you‚Äôve ever implemented any cryptography yourself, there‚Äôs an excellent chance you relied on an algorithm like AES, HMAC or ChaCha20 to implement either encryption or authentication. If you did this, then you probably relied on some security property you assumed those primitives to have. But what precisely is that security property you‚Äôre relying on? We could re-imagine this security definition from scratch every time we look at a new cipher. Alternatively, we could start from a much smaller number of general mathematical objects that provide security properties that we can reason about, and try to compare those to the algorithms we actually use. The second approach has a major advantage: it‚Äôs very modular. That is, rather than re-design every single protocol each time we come up it with a new type of cipher, all we really need to do is to analyze it with the idealized mathematical objects. Then we can realize it using actual ciphers, which hopefully satisfy these well-known properties. Two of the most common such objects are the pseudorandom function (PRF) and the pseudorandom permutation (PRP). At the highest level, these functions have two critical properties that are extremely important to cryptographers: They are keyed functions: this means that they take in a secret key as well as some input value. (This distinguishes them from some hash functions.) The output of a PRF (or PRP), when evaluated on some unique input, typically appears ‚Äúrandom.‚Äù (But explaining this rough intuition precisely will require some finesse, see below.) If a function actually can truly achieve those properties, we can use it to accomplish a variety of useful tasks. At the barest minimum, these properties let us accomplish message authentication (by building MACs), symmetric encryption by building stream ciphers, and key derivation (or ‚Äúpluralization‚Äù) in which a single key is turned into many distinct keys. We can also use PRFs and PRPs to build other, more fundamental primitives such as pseudorandom number generators and modes of operation, which happen to be useful when encrypting things with block ciphers. The how and why is a little complex, and that‚Äôs the part that will require all the explanation. Random functions There are many ideal primitives we‚Äôd love to be able to use in cryptography, but are thwarted from using due to the fact that they‚Äôre inefficient. One of the most useful of these is the random function. Computer programmers tend to get confused about functions. This is mostly because programming language designers have convinced us that functions are the same thing as the subroutines (algorithms) that we use to compute them. In the purely mathematical sense, it‚Äôs much more useful to forget about algorithms, and instead think of functions as simply being a mapping from some set of input values (the domain) to some set of output values (the range). If we must think about implementing functions, then for any function with a finite domain and range, there is always a simple way to implement it: simply draw up a giant (and yet still finite!) lookup table that contains the mapping from each input to the appropriate output value. Given such a table, you can always quickly realize an algorithm for evaluating it, simply by hard-coding the table into your software and performing a table lookup. (We obviously try not to do this when implementing software ‚Äî indeed, most of applied computer science can be summarized as ‚Äúfinding ways to avoid using giant lookup tables‚Äù.) A nice thing about the lookup table description of functions is that it helps us reason about concepts like the number of possible functions that can exist for a specific domain and range. Concretely: if a function has M distinct input values and N outputs, then the number of distinct functions sharing that profile is . This probably won‚Äôt scale very well for even modest values of M and N, but let‚Äôs put this aside for a moment. Given enough paper, we could imagine writing down each unique lookup table on a piece of paper: then we could stack those papers up and admire the minor ecological disaster we‚Äôd have just created. Now let‚Äôs take this thought-experiment one step farther: imagine that we could walk out among those huge stacks of paper we‚Äôll have just created, and somehow pick one of these unique lookup tables uniformly at random. If we could perform this trick routinely, the result would be a true ‚Äúrandom function‚Äù, and it would make an excellent primitive to use for cryptography. We could use these random functions to build hash functions, stream ciphers and all sorts of other things that would make our lives much easier. There are some problems with this thought experiment, however. A big problem is that, for functions with non-trivial domain and range, there just isn‚Äôt enough paper in the world to enumerate every possible function. Even toy examples fall apart quickly. Consider a tiny hash function that takes in (and outputs) only 4-bit strings. This gives us M=16 inputs and N=16 outputs, and hence number of (distinct) mappings is , or about 18 quintillion. It gets worse if you think about ‚Äúuseful‚Äù cryptographic functions, say those with the input/output profile of ChaCha20, which has 128-bit inputs and 512-bit outputs. There you‚Äôd need a whopping (giant) pieces of paper. Since there are only around atoms in the observable universe (according to literally the first Google search I ran on the topic), we would quickly run into shortages even if we were somehow able to inscribe each table onto a single atom. Obviously this version of the thought experiment is pretty silly. After all: why bother to enumerate every possible function if we‚Äôre going to throw most of them away? It would be much more efficient if we could sample a single random function directly without all the overhead. This also turns out to be fairly straightforward: we can write down a single lookup table with M rows (corresponding to all possible inputs); for each row, we can sample a random output from the set of N possible outputs. The resulting table will be M rows long and each row will contain bits of data. While this seems like a huge improvement over the previous approach, it‚Äôs still not entirely kosher. Even a single lookup table is still going to huge ‚Äî at least as large as the function‚Äôs entire domain. For example: if we wanted to sample a random function with the input/output profile of ChaCha20, the table would require enough paper to contain bits. And no, we are not going to be able to compress this table! It should be obvious now that a random function generated this way is basically just a file full of random data. Since it has maximal entropy, compression simply won‚Äôt work. The fact that random functions aren‚Äôt efficient for doing cryptography does not always stop cryptographers from pretending that we might use them, most famously as a way to model cryptographic hash functions in our security proofs. We have an entire paradigm called the random oracle model that makes exactly this assumption. Unfortunately, in reality we can‚Äôt actually use random functions to implement cryptographic functions ‚Äî sampling them, evaluating them and distributing their code are all fundamentally infeasible operations. Instead we ‚Äúinstantiate‚Äù our schemes with an efficient hash algorithm like SHA3, and then we pray. However, there is one important caveat. While we generally cannot sample and share large random functions in practice, we hope we can do something almost as interesting. That is, we can build functions that appear to be random: and we can do this in a very powerful cryptographic sense. Pseudorandom functions Random functions represent a single function drawn from a family of functions, namely the family that consists of every possible function that has the appropriate domain and range. As noted above, the cost of this decision is that such functions cannot be sampled, evaluated or distributed efficiently. Pseudorandom functions share a similar story to random functions. That is, they represent a single function sampled from a family. What‚Äôs different in this case is that the pseudorandom function family is vastly smaller. A benefit of this tradeoff is that we can demand that the description of the function (and its family) be compact: pseudorandom function families must possess a relatively short description, and it must be possible to both sample and evaluate them efficiently: meaning, in polynomial time. Compared to the set of all possible functions over a given domain and range, a pseudorandom function family is positively tiny. Let‚Äôs stick with the example of ChaCha20. As previously discussed, ChaCha has a 128-bit input, but it also takes in a 256-bit secret key. If we were to view ChaCha20 as a pseudorandom function family, then we could view it as a family of individual functions, where each key value selects exactly one function from the family. Now let‚Äôs be clear: is still a really big number! However: it is vastly smaller than , which is the total number of possible functions with ChaCha20‚Äôs input profile. Sampling a random 256-bit key and sharing it with to Bob is eminently feasible; indeed, your browser did something like this when you loaded this website. Sampling a ‚Äúkey‚Äù of bit-length is not. This leaves us with an important question, however. Since ChaCha20‚Äôs key is vastly smaller than the description of a random function, and the algorithmic description of ChaCha20 is also much smaller than the description of even a single random function, is it possible for small-key function family like ‚ÄúChaCha20‚Äù to be as good (for cryptographic purposes) as a true random function? And what does ‚Äúgood‚Äù even mean here? Defining pseudorandomness Mirriam-Webster defines the prefix pseudo as ‚Äúbeing apparently rather than actually as stated.‚Äù The Urban Dictionary is more colorful: it defines pseudo as ‚Äúfalse; not real; fake replication; bootleg; tomfoolery‚Äú, and also strongly hints that pseudo may be shorthand for pseudoephedrine (note: it is not.) Clearly if we can describe a function using a compact algorithmic description and a compact key, then it cannot be a true random function: it is therefore bootleg. However that doesn‚Äôt mean it‚Äôs entirely tomfoolery. What pseudorandom means, in a cryptographic sense, is that a function of this form will be indistinguishable from a truly random function ‚Äî at least to an adversary who does not know which function we have chosen from the family, and who has a limited amount of computing power. Let‚Äôs unpack this definition a bit! Imagine that I create a black box that contains one of two possible items, chosen with equal probability. Item (1) is an instance of a single function sampled at random from a purported pseudorandom function family; item (2) is a true random function sampled from the set of all possible functions. Both functions have exactly the same input/output profile, meaning they take in inputs of the same length, and produce outputs of the same length (here we are excluding the key.) Now imagine that I give you ‚Äúoracle‚Äù access to this box. What this means is: you will be allowed to submit any input values you want, and the box will evaluate your input using whichever function it contains. You will only see the output. (And no, you don‚Äôt get to time the box or measure any side channels it might compute, this is a thought experiment.) You can submit as many inputs as you want, using any strategy for choosing them that you desire: they simply have to be valid inputs, meaning that they‚Äôre within the domain of the function. We will further stipulate that you will be computationally limited: that means you will only be able to compute for a limited (polynomial in, say, the PRF‚Äôs key length) number of timesteps. At the end of the day, your goal is to guess which type of function I‚Äôve placed in the box. We say that a family of functions is pseudorandom if for every possible efficient strategy (meaning, using any algorithm that runs in time polynomial in the key size, provided these algorithms were enumerated before the function was sampled), the ‚Äúadvantage‚Äù you will have in guessing what‚Äôs in the box is very tiny (at most negligible in, say, the size of the function‚Äôs key.) A fundamental requirement of this definition is that the PRF‚Äôs key/seed (aka the selector that chooses which function to use) has to remain secret from the adversary. This is because the description of the PRF family itself cannot be kept secret: that is both good cryptographic practice (known as Kerckhoff‚Äôs principle), but also due to the way we‚Äôve defined the problem over ‚Äúall possible algorithms‚Äù, which necessarily includes algorithms that have the PRF family‚Äôs description coded inside of them. And pseudorandom functions cannot possibly be indistinguishable from random ones if the attacker can learn or guess the PRF‚Äôs secret key: this would allow the adversary to simply compute the function themselves and compare the results they get to the values that come out of the oracle (thus winning the experiment nearly 100% of the time.) There‚Äôs a corollary to this observation: since the key length of the PRF is relatively short, the pseudorandomness guarantee can only be computational in nature. For example, imagine the key is 256 bits long: an attacker with unlimited computational resources could brute-force guess its way through all possible 256-bit keys and test each one against the results coming from the oracle. If the box truly contains a PRF, then with high probability she‚Äôll eventually find a key that produces the same results as what comes out of the box; if the box contains a random function, then she probably won‚Äôt. To rule such attacks out of bounds we must assume that the adversary is not powerful enough to test a large fraction of the keyspace. (In practice this requirement is pretty reasonable, since brute forcing through an n-bit keyspace requires on the order of work, and we assume that there exist reasonable values of n for which no computing device exists that can succeed at this.) So what can we do with pseudorandom functions? As I mentioned above, pseudorandom functions are extremely useful for a number of basic cryptographic purposes. Let‚Äôs give a handful here. Building stream ciphers. One of the simplest applications of a PRF is to use it to build an efficient stream cipher. Indeed, this is exactly what the ChaCha20 function is typically used for. Let us assume for the moment that ChaCha20 is a PRF family (I‚Äôll come back to this assumption later.) Then we could select a random key and evaluate the function on a series of unique input values ‚Äî the ChaCha20 IETF proposals suggest concatenating a 64-bit block number with a counter ‚Äî and then concatenate the outputs of the function together to produce a keystream of bits. To encrypt a message we would simply exclusive-OR (XOR) this string of bits (called a ‚Äúkeystream‚Äù) with the message to be enciphered. Why is this reasonable? The argument breaks down into three steps: If we had generated the keystream using a perfect random number generator (and kept it secret, and never re-used the keystream) then the result would be a one-time pad, which is known to be perfectly secure. And indeed, had we had been computing this output using a truly random function (with a ChaCha20-like I/O profile) where each input was used exactly once, the result of this evaluation would indeed have been such a random string. Of course we didn‚Äôt do this: we used a PRF. But here we can rely on the fact that our attackers cannot distinguish PRF output from that of a random function. One can make the last argument the other way around, too. If our attacker is much better at ‚Äúbreaking‚Äù the stream cipher implemented with a PRF than they are at breaking one implemented with a random function, then they are implicitly ‚Äúdistinguishing‚Äù the two types of function with a substantial advantage ‚Äî and this is precisely what the definition of a PRF says that an attacker cannot do! Constructing MACs. A PRF with a sufficiently large range can also be used as a Message Authentication Code. Given a message M, the output of PRF(k, M) ‚Äî the PRF evaluated on a secret key k and the message M ‚Äî should itself be indistinguishable from the output of a random function. Since this output will effectively be a random string, this means that an attacker who has not previously seen a MAC on M should have a hard time guessing the appropriate MAC for a given message. (The ‚Äústrength‚Äù of the MAC will be proportional to the output length of the PRF.) Key derivation. Often in cryptography we have a single random key k and we need to turn this into several random-looking keys (k1, k2, etc.) This happens within protocols like TLS, which (at least in version 1.3) has an entire tree of keys that it derives from a single master secret. PRFs, it turns out, are an excellent for this task. To ‚Äúdiversify‚Äù a single key into multiple keys, one can simply evaluate the PRF at a series of distinct points (say, k1 = PRF(k, 1), k2 = PRF(k, 2), and so on), and the result is a set of keys that are indistinguishable from random; provided that the PRF does what it says it does. There are, of course, many other applications for PRFs; but these are some pretty important ones. Pseudorandom permutations Up until now we‚Äôve talked about pseudorandom functions (PRFs): these are functions that have output that is indistinguishable from a random function. A related concept is that of the pseudorandom permutation (PRP). Pseudorandom permutations share many of the essential properties of PRFs, with one crucial difference: these functions realize a permutation of their input space. That is: if we concentrate on a given function in the family (or, translating to practical terms, we fix one ‚Äúkey‚Äù) then each distinct input maps to a distinct output (and vice versa.) A nice feature of permutations is that they are potentially invertible, which makes them a useful model for something we use very often in cryptography: block ciphers. These ciphers take in a key and a plaintext string, and output a ciphertext of the same length as the plaintext. Most critically, this ciphertext can be deciphered back to the original plaintext. Note that a standard (pseudo)random function doesn‚Äôt necessarily allow this: for example, a PRF instance F can map multiple inputs (A, B) such that F(A) = F(B), which makes it very hard to uniquely invert either output. The definition of a pseudorandom permutation is very similar to that of a PRF: they must be indistinguishable from some idealized function ‚Äî only in this case the ideal object is a random permutation. A random permutation is simply a function sampled uniformly from the set of all possible permutations over the domain and range. (Because really, why wouldn‚Äôt it be?) There are two important mathematical features of PRPs that I should mention here: PRPs are actually PRFs (to an extent.) A well-known result in cryptography, called the ‚ÄúPRP/PRF switching lemma‚Äù demonstrates that a PRP with sufficiently-large domain and range basically ‚Äúis‚Äù a PRF. Put differently: a pseudorandom permutation placed into an oracle can be computationally indistinguishable from an oracle that contains a random function (with the same domain and range), provided the range of the function is large enough and the attacker doesn‚Äôt make too many queries. The intuition behind this result is fairly straightforward. If we consider this from the perspective of an attacker interacting with some function in an oracle, the only difference between a random permutation and a random function is that the former will never produce any collisions ‚Äî distinct inputs that produce the same output ‚Äî while the latter may (occasionally) do so. From the adversary‚Äôs perspective, therefore, the ability to distinguish whether the oracle contains a random permutation or a random function devolves to querying the oracle to see if one can observe such a collision. Clearly if it sees even one collision of the form F(A) = F(B), then it‚Äôs not dealing with a permutation. But it may take many queries for the attacker to find such a collision in a random function, or to be confident one should already have occurred (and hence it is probably interacting with a PRP.) In general the ability to distinguish the two is a function of the number of queries the attacker is allowed to make, as well as the size of the function‚Äôs range. After a single query, the probability of a collision (on a random function) is zero: hence the attacker has no certainty at all. After two queries, the probability is equal to 1/N where N is the number of possible outputs. As the attacker makes more queries this probability increases. Following the birthday argument the expected probability reaches p=0.5 after about queries. For functions like AES, which has output size , this will occur around queries. PRFs can be used to build PRPs. The above result shows us that PRPs are usually good enough to serve as PRFs ‚Äúwithout modification.‚Äù What if one has a PRF and wishes to build a PRP from it? This can also be done, but it requires more work. The standard technique was proposed by Luby and Rackoff and it involves building a Feistel network, where each ‚Äúround function‚Äù in the PRP is built using a pseudorandom function. (See the picture at right.) This is a bit more involved than I want to get in this post, so please just take away the notion that the existence of one of these objects implies the existence of the other. Why do I care about any of this? I mean, you don‚Äôt have to. However: I find that many people just getting into cryptography tend to get very involved in the deep details of particular constructions (ciphers and elliptic curves being one source of rabbit-holing) and take much longer to learn about useful analysis tools like PRFs and PRPs. Once you understand how PRPs and PRFs work, it‚Äôs much easier to think about protocols like block cipher modes of operation, or MAC constructions, or anything that involves deriving multiple keys. Take a simple example, the CBC mode of operation: this is a ‚Äúclassical‚Äù mode of operation used in many block ciphers. I don‚Äôt recommend that you use it (there are better modes) but it‚Äôs actually a very good teaching example. CBC encryption requires the sender to first select a random string called an Initialization Vector, then to chop up their message into equal-size blocks. Encryption looks something like this: From Wikipedia. The plus signs are bitwise XOR. If we‚Äôre willing to assume that the block cipher is a PRP, then analyzing the security of this construction shouldn‚Äôt be terribly hard. Provided the block size of the cipher is large enough, we can first use the PRP/PRF switching lemma to argue that a PRP is (computationally) indistinguishable from a random function. To think about the security of CBC-mode encryption, therefore, we can (mathematically) replace our block cipher with a random function of the appropriate domain and range. Now the question is whether CBC-mode is secure when realized with a random function. So if we replace the block cipher with a random function, how does the argument work? Well obviously in a real scheme both the encryptor and decryptor would need to have a copy of the same function, and we‚Äôve already covered why that‚Äôs problematic: the function would need to be fully-sampled and then communicated between the two parties. Then they would have to scan through a massive table to find each entry. But let‚Äôs put that aside for a moment. Instead let‚Äôs focus only on the encryptor. Since we don‚Äôt have to think about communicating the entire function to another party, we don‚Äôt have to sample it up front. Instead we can sample it ‚Äúlazily‚Äù for the purposes of arguing security. More specifically: means instead of sampling the entire random function in one go, we can instead imagine using an oracle that ‚Äúbuilds‚Äù the function one query at a time. The oracle works as follows: anytime the encryptor queries it on some input value, the oracle checks to see if this value has been queried before. If it has previously been queried, the oracle outputs the value it gave previously. Otherwise it samples a new (uniformly random) output string using a random number generator, then writes the input/output values down so it can check for later duplicate inputs. Now imagine that an encryptor is using CBC mode to encrypt some secret message, but instead of a block cipher they are using our ‚Äúrandom function‚Äù oracle above. The encryption of a message will work like this: To encrypt each new message, the encryptor will first choose a uniformly-random Initialization Vector (IV). She will then XOR that IV with the first block of the message, producing a uniformly-distributed string. Then she‚Äôll query the random function oracle to obtain the ‚Äúencipherment‚Äù of this string. Provided the oracle hasn‚Äôt seen this input before, it will sample and output a uniformly random output string. That string will form the first block of ciphertext. Then the encryptor will take the resulting ciphertext block and treat it as the ‚ÄúIV‚Äù for the next message block, and will repeat steps (2-4) over and over again for each subsequent block. Notice that this encryption is pretty good. As long as the oracle never gets called on the same input value twice, the output of this encryption process will be a series of uniformly-random bits that have literally nothing to do with the input message. This strongly imples that CBC ciphertexts will be very secure! Of course we haven‚Äôt really proven this: we have to consider the probability that the encryptor will query the oracle twice on the same input value. Fortunately, with a little bit of simple probability, we can show the following: since (1) each input is uniformly distributed, then (2) the probability of such a repeated input stays quite low. (In practice the probability works out to be a function of the function‚Äôs output length and the total number of plaintext blocks enciphered. This analysis is part of the reason that cryptographers generally prefer ciphers with large block sizes, and why we place official limits on the number of blocks you‚Äôre allowed to encipher with modes like CBC before you change the key. To see more of the gory details, look at this paper.) Notice that so far I‚Äôve done this analysis assuming that the block cipher (encipherment) function is a random function. In practice, it makes much more sense to assume that the block cipher is actually a pseudorandom permutation. Fortunately we‚Äôve got most of the tools to handle this switch. We need to add two final details to the intuition: (1) since a PRF is indistinguishable from a random function to all bounded adversaries, we can first substitute in a PRF for that random function oracle with only minimal improvement in the attacker‚Äôs ability to distinguish the ciphertext from random bits. Next: (2) by the PRP/PRF switching lemma we can exchange that PRF for a PRP with similarly minor impact on the adversary‚Äôs capability. This is obviously not a proof of security: it‚Äôs merely an intuition. But it helps to set up the actual arguments that would appear in a real proof. And you can provide a similar intuition for many other protocols that use keyed PRF/PRP type functions. What if the PRP/PRF key isn‚Äôt secret? One of the biggest restrictions on the PRF concept is the notion that these functions are only secure when the secret key (AKA, the choice of which ‚Äúfunction‚Äù to use from the family) is kept secret from the adversary. We already discussed why this is critical: in the PRF (resp. PRP) security game, an attacker who learns the key can instantly ‚Äúdistinguish‚Äù a pseudorandom function from a random one. In other words, knowledge of the secret key explodes the entire concept of pseudorandomness. Hence from a mathematical perspective, the security properties of a PRF are somewhere between non-existent and undefined in this setting. But that‚Äôs not very satisfying, and this kind of non-intuitive behavior only makes people ask more questions. They come back wondering: what actually happens when you learn the secret key for a PRF? Does it explode or collapse into some kind of mathematical singularity? How does a function go from ‚Äúindistinguishable from random‚Äù to ‚Äúcompletely broken‚Äù based on learning a small amount of data? And then, inevitably, they‚Äôll try to build things like hash functions using PRFs. The former questions are mainly interesting to cryptographic philosophers. However the latter question is practically relevant, since people are constantly trying to do things like build hash functions out of block ciphers. (NB: this is not actually a crazy idea. It‚Äôs simply not possible to do it based solely on the assumption that these functions are pseudorandom.) So what happens to a PRF when you learn its key? One answer to this question draws from the following (tempting, but incorrect) line of reasoning: PRFs must somehow produce statistically-‚Äúrandom looking‚Äù output all the time, whether you know the key or not. Therefore, the argument goes, the PRF is effectively as good as random even after one learns the key. This intuition is backed up by the following thought-experiment: Imagine that at time (A) I do not know the key for a PRF, but I query an oracle on a series of inputs (for simplicity, let‚Äôs say I use the values 1, 2, ‚Ä¶, q for some integer q that is polynomial in the key length.) Clearly at this point, the outputs of the PRF must be indistinguishable from those of a true random function. If the range of the function comprises -bit strings, then any statistical ‚Äúrandomness test‚Äù I run on those outputs should ‚Äúsucceed‚Äù, i.e., tell me that they look pretty random.(Putting this differently: if any test reliably ‚Äúfails‚Äù on the output of the PRF oracle, but ‚Äúsucceeds‚Äù on the output of a true random function, then you‚Äôve just built a test that lets you distinguish the PRF from a random function ‚Äî and this means the function was never a PRF in the first place! And your ‚ÄúPRF‚Äù will now disappear in a puff of logic.) Now imagine that at time (B) ‚Äî after I‚Äôve obtained the oracle outputs ‚Äî someone hands me the secret key for the PRF that was inside the oracle. Do the outputs somehow ‚Äústop‚Äù being random? Will the NIST test suite suddenly start failing? The simple answer to the last question is ‚Äúobviously no.‚Äù Any public statistical test you could have performed on the original outputs will still continue to pass, even after you learn the secret key. What has changed in this instance is that you can now devise new non-public statistical tests that are based on your knowledge of the secret key. For example, you might test to see if the values are outputs of the PRF (on input the secret key), which of course they would be ‚Äî and true random numbers wouldn‚Äôt be. So far this doesn‚Äôt seem so awful. Where things get deeply unpleasant is if the secret key is known to the attacker at the time it queries the oracle. Then the calls to the PRF can behave in ways that deviate massively from the expected behavior of a random function. For example, consider a function called ‚ÄúKaty-Perry-PRF‚Äù that generally behaves like a normal PRF most of the time, but that spews out Katy Perry lyrics when queried on specific (rare) inputs. Provided that these rare inputs are hard for any attacker to find ‚Äî meaning, the attacker will find them only with negligible probability ‚Äî then Katy-Perry-PRF will be a perfectly lovely PRF. (More concretely, we might imagine that the total number of possible input values is exponential in the key length, and the set of ‚ÄúKaty-Perry-producing‚Äù input values forms a negligible fraction of this set, distributed pseudorandomly within it, to boot.) We can also imagine that the location of these Katy-Perry-producing inputs is only listed in the secret key, which a normal PRF adversary will not have. Clearly a standard attacker (without the secret key) is unlikely to find any inputs that produce Katy Perry lyrics. Yet an attacker who knows the secret key can easily obtain the entire output of Katy Perry‚Äôs catalog: this attacker will simply look through the secret key to find the appropriate inputs, and then query them all one at a time. The behavior of the Katy-Perry function on these inputs is clearly very different from what we‚Äôd expect from a random function and yet here is a function that still satisfies the definition of a PRF. Now obviously Katy-Perry-PRF is a silly and contrived example. Who actually cares if your PRF outputs Katy Perry lyrics? But similar examples can be used to produce PRFs that enable easy ‚Äúcollisions‚Äù, which is generally a bad thing when one is trying to build things like hash functions. This is why the construction of such functions needs to either assume weaker properties (i.e., that you get only collision-resistance) or make stronger assumptions, such as the (crazy) assumption that the block cipher is actually a random function. Finally: how do we build PRFs? So far I‚Äôve been using the ChaCha function as an example of something we‚Äôd really like to imagine is a PRF. But the fact of the matter is that nobody knows how to actually prove this. Most of the practical functions we use like PRFs, which include ChaCha, HMAC-SHA(x), and many other ciphers, are constructed from a handful of simple mathematical operations such as rotations, XORs, and additions. The result is then analyzed by very smart people to see if they can break it. If someone finds a flaw in the function, we stop using it. This is theoretically less-than-elegant. Instead, it would be nice to have constructions we clearly know are PRFs. Unfortunately the world is not quite that friendly to cryptography. From a theoretical perspective, we know that PRFs can be constructed from pseudorandom generators (PRGs). We further know that PRGs can in turn be constructed from one-way functions (OWFs). The existence of the latter functions is one of the most basic assumptions we make in cryptography, which is a good way of saying we have no idea if they exist but we are hopeful. Indeed, this is the foundation of what‚Äôs called the ‚Äústandard model.‚Äù But in practice the existence of OWFs remains a stubbornly open problem, bound tightly to the P/NP problem. If that isn‚Äôt entirely satisfying to you, you might also like to know that we can also build (relatively) efficient PRFs based on the assumed hardness of a number of stronger mathematical assumptions, including things like the Decisional Diffie-Hellman assumption and various assumptions in lattices. Such things are nice mainly because they let us build cool things like oblivious PRFs. Phew! This has been a long piece, on that I‚Äôm glad to have gotten off my plate. I hope it will be helpful to a few people who are just starting out in cryptography and are itching to learn more. If you are one of these people and you plan to keep going, I urge you to take a look at a cryptography textbook like Katz/Lindell‚Äôs excellent textbook, or Goldreich‚Äôs (more theoretical) Foundations of Cryptography. Top photo by Flickr user Dave DeSandro, used under CC license. By Matthew Greenin fundamentals, provable securityMay 8, 2023May 10, 20236,195 Words1 Comment Book Review: Red Team Blues As a rule, book reviews are not a thing I usually do. So when I received an out-of-the-blue email from Cory Doctorow last week asking if I would review his latest book, Red Team Blues, it took a minute to overcome my initial skepticism. While I‚Äôm a fan of Cory‚Äôs work, this is a narrow/nerdy blog about cryptography, not a place where we spend much time on literature. Moreover, my only previous attempt to review a popular cryptography novel ‚Äî a quick sketch of Dan Brown‚Äôs abysmal Digital Fortress ‚Äî did not go very well for anyone. But Cory isn‚Äôt Dan Brown. And Red Team Blues is definitely not Digital Fortress. This became obvious in the middle of the first chapter, when a character began explaining the operation of a trusted execution environment and its various digital signing keys. While it‚Äôs always fun to read about gangsters and exploding cars, there‚Äôs something particularly nice about a book whose plot hangs around a piece of technology that most people don‚Äôt even think about. (And if that isn‚Äôt your thing, there are exploding cars and gangsters.) This still leaves the question of how a cryptography blog reviews a work of fiction, even one centered on cryptography. The answer is pretty simple: I‚Äôm not going to talk much about the story. If you want that, there are other reviews out there. While I did enjoy the book immensely and I‚Äôm hopeful Cory will write more books in this line (with hopefully more cryptography), I‚Äôll mainly focus on the plausibility of the core technical setup. But even to do that, I have to provide a few basic details about the story. (Note: minor spoilers below, but really only two chapters‚Äô worth.) The protagonist of Red Team Blues is 67-year-old Martin Hench, an expert forensic accountant with decades of experience tracing and recovering funds for some of the most powerful people in Silicon Valley. Martin is on the brink of retirement, lives in a bus named ‚Äúthe Unsalted Hash‚Äù and loves bourbon nearly as much as he despises cryptocurrency. This latter position is presumably a difficult one for someone in Martin‚Äôs line of work, and sure enough his conviction is quickly put to the test. Before long Martin is hired by his old friend Danny Lazer ‚Äî sort of a cross between Phil Zimmerman, David Chaum and (maybe) Max Levchin ‚Äî who begs him to take one last career-defining job: namely, to save his friend‚Äôs life by saving his newest project: a cryptocurrency called TrustlessCoin. TrustlessCoin is a private cryptocurrency: not terribly different from real ones like Monero or Zcash. (As a founding scientist of a private cryptocurrency, let me say that none of the things in this novel have ever happened to me, and I‚Äôm slightly disappointed in that.) Unlike standard cryptocurrencies, TrustlessCoin contains one unusual and slightly horrifying technological twist. Where standard cryptocurrencies rely on consensus algorithms to construct a public ledger (and zero-knowledge proofs for privacy), TrustlessCoin bases its integrity on the security of mobile Trusted Execution Environments (TEEs). This means that its node software runs inside of systems like Intel‚Äôs SGX, ARM‚Äôs TrustZone, or Apple‚Äôs Secure Enclave Processor. Now, this idea isn‚Äôt entirely unprecedented. Indeed, some real systems like MobileCoin, Secret Network and Intel‚Äôs PoET take a fairly similar approach ‚Äî although admittedly, these rely mainly on server-based TEEs rather than mobile ones. It is, however, an idea that makes me want to scream like a child who just found a severed human finger in his bowl of cornflakes. You see, TEEs allow you to run software (more) securely inside of your own device, which is a good and respectable thing to do. But distributed systems often require more: they must ensure that everyone else in the network is also running the software in a similarly-trustworthy environment. If some people aren‚Äôt doing so ‚Äî that is, if they‚Äôre running the software on a computer they can tamper with and control ‚Äî then that can potentially harm the security of the entire network. TEE designers have been aware of this idea for a long time, and for years have been trying to address this using secure remote attestation. Attestation systems provision each processor with a digital signing key (in turn certified by the manufacturer‚Äôs root signing key) that allows the processor to produce attestations. These signed messages ‚Äúprove‚Äù to remote parties that you‚Äôre actually running the software inside a valid TEE, rather than on some insecure VMWare image or a Raspberry Pi. Provided these systems all work perfectly, everyone in the system can communicate with everyone else and know that they are running the software on secure hardware as well. The problems crop up when that assumption breaks down. If even a single person can emulate the software inside a TEE on their own (non-trusted device or VM) then all of your beautiful assumptions may go out the window. Indeed, something very similar to this recently happened to Secret Network: clever academic researchers found a way to extract a master decryption key from (one) processor, and were then able to use that key to destroy privacy guarantees across the whole network. (Some mitigations have since been deployed.) It goes without saying that Red Team Blues is not about side-channel attacks on processors. The problem in this novel is vastly worse: Danny Lazer has gone and bribed someone to steal the secret root signing keys for every major mobile secure enclave processor: and, of course, they‚Äôve been all been stolen. Hench‚Äôs problem is to figure out whether it‚Äôs even possible to get them back. And that‚Äôs only the beginning of the story. As its name implies, Red Team Blues is a novel about the difference between offense and defense: about how much more difficult it is to secure a system than it is to attack one. This metaphor applies to just about every aspect of life, from our assumptions about computer security to the way we live our lives and build our societies. But setting all these heavy thoughts aside, mostly Red Team Blues is a quick fun read. You can get the eBook without DRM, or listen to an audiobook version narrated by Wil Wheaton (although I didn‚Äôt listen to it because I couldn‚Äôt put the book down.) By Matthew Greenin book reviews, books, infosecApril 24, 20231,040 Words1 Comment Remarks on ‚ÄúChat Control‚Äù On March 23 I was invited to participate in a panel discussion at the European Internet Services Providers Association (EuroISPA). The focus of this discussion was on recent legislative proposals, especially the EU Commission‚Äôs new ‚Äúchat control‚Äù content scanning proposal, as well as the future of encryption and fundamental rights. These are the introductory remarks I prepared. Thank you for inviting me today. I should start by making brief introduction. I am a professor of computer science and a researcher in the field of applied cryptography. On a day-to-day basis this means that I work on the design of encryption systems. Most of what I do involves building things: I design new encryption systems and try to make existing encryption technologies more useful. Sometimes I and my colleagues also break encryption systems. I wish I could tell you this didn‚Äôt happen often, but it happens much more frequently than you‚Äôd imagine, and often in systems that have billions of users and that are very hard to fix. Encryption is a very exciting area to work in, but it‚Äôs also a young area. We don‚Äôt know all the ways we can get things wrong, and we‚Äôre still learning. I‚Äôm here today to answer any questions about encryption in online communication systems. But mainly I‚Äôm here because the EU Commission has put forward a proposal that has me very concerned. This proposal, which is popularly called ‚Äúchat control‚Äù, would mandate content scanning technology be added to private messaging applications. This proposal has not been properly analyzed at a technical level, and I‚Äôm very worried that the EU might turn it into law. Before I get to those technical details, I would like to address the issue of where encryption fits into this discussion. Some have argued that the new proposal is not about encryption at all. At some level these people are correct. The new legislation is fundamentally about privacy and confidentiality, and where law enforcement interests should balance against those things. I have opinions about this, but I‚Äôm not an EU citizen. Unfortunately this is a fraught debate that Europeans will have to have among themselves. I don‚Äôt envy you. What concerns me is that the Commission does not appear to have a strong grasp on the technical implications of their proposal, and they do not seem to have considered how it will harm the security of our global communications systems. And this does affect me, because the security of our communications infrastructure is not localized to any one continent: if the 447 million citizens of the EU vote to weaken these technical systems, it could affect all consumers of computer security technology worldwide. So why is encryption so critical to this debate? Encryption matters because it is the single best tool we have for securing private data. My time here is limited, but if I thought that using all of it to convince you of this single fact was necessary, I would do that. Literally every other approach we‚Äôve ever used to protect valuable data has been compromised, and often quite badly. And because encryption is the only tool that works for this purpose, any system that proposes to scan private data must ‚Äî as a purely technical requirement ‚Äî grapple with the technical challenges it raises when that data is protected with end-to-end encryption. And those technical implications are significant. I have read the Impact Assessment authored by the Commission, and I hope I am not being rude to this audience when I say that I found it deeply naive and alarming. My impression is that the authors do not understand, at a purely technical level, that they are asking technology providers to deploy systems that none of them know how to build safely. Nor has the Commission consulted people with the technical and scientific expertise that would be needed to make this proposal viable. In order to explain my concerns, I need to give some brief background on how content scanning systems work: both historically, and in the context that the EU is proposing. Modern content scanning systems are a new creation. They have only been deployed since only about 2009, and widely deployed only after about 2011. These systems normally evaluate messages uploaded to a server, often a social network or public repository. In historical systems ‚Äî that is, older systems without end-to-end encryption ‚Äî they would process unencrypted plaintext data, usually to look for known child sexual abuse media files (or CSAM.) Upon finding such an image, they undertake various reporting: typically alerting employees at the provider, who may then escalate to the police. Historical scanning systems such as Microsoft‚Äôs PhotoDNA used a perceptual hashing algorithm to reduce each image to a ‚Äúfingerprint‚Äù that can be checked against a database of known illicit content. These databases are maintained by child safety organizations such as NCMEC. The hashing algorithms themselves are deliberately imperfect: they are designed to produce similar fingerprints for files that appear (to the human eye) to be identical, even if a user has slightly altered the file‚Äôs data. A first limitation of these systems is that their inaccuracy can be exploited. It is relatively easy, using techniques that have only been developed recently, to make new images that appear to be harmless licit media files, but that will produce a fingerprint that is identical to harmful illicit CSAM. A second limitation of these hash-based systems is that they cannot detect novel CSAM content. This means that criminals who post newly-created abuse media are effectively invisible to these scanners. Even a decade ago, the task of finding novel CSAM would have required human operators. However, recent advances in AI have made it possible to train deep neural networks on such imagery, so that these networks can try to detect new examples of it: Of course, the key word in any machine-based image recognition system is ‚Äútry.‚Äù All image recognition systems are somewhat fallible (see example at right) and even when they work well, they often fail to differentiate between licit and illicit content. Moreover these systems can be exploited by malicious users to produce surprising results. I‚Äôll come back to that in a moment. But allow me to return to the key challenge: integrating these systems with encrypted communication systems. In end-to-end encrypted systems, such as WhatsApp or Apple iMessage or Signal, server-side scanning is no longer viable. The problem here is that private data is encrypted when it reaches the server, and cannot be scanned. The Commission proposal isn‚Äôt specific about how these systems should be handled, but it hints that this scanning should be done on the user‚Äôs device before the content is encrypted. This approach is called client side scanning. There are several challenges here. First, client-side scanning represents an exception to the privacy guarantees of encrypted systems. In a standard end-to-end encrypted system, your data is private to you and your intended recipient. In a system with client-side scanning, your data is confidential‚Ä¶ with an asterisk. That is, the data itself will be private unless the scanning system determines a violation has occurred, at which point your confidentiality will be (silently) revoked and unencrypted data will be transmitted to the provider (and thus, anyone who has compromised your provider.) This ability to selectively disable encryption creates new opportunities for attacks. If an attacker can identify the conditions that will cause the model to reduce the confidentiality of youe encryption, she can generate new ‚Äî and apparently harmless ‚Äî content that will cause this to happen. This will very quickly overwhelm the scanning system, rendering it useless. But it will also seriously reduce the privacy of many users. A mirror version of this attacker exists as well: he will use knowledge of the model to evade these systems, producing new imagery and content that appear unchanged, but that these systems cannot detect at all. Your most sophisticated criminals ‚Äî most likely the ones who create this awful content in the first place ‚Äî will hide in plain sight. Finally, a more alarming possibility exists: many neural-network classifiers allow for the extraction of the images that were used to train the model. This means every complex neural network model may potentially contain images of abuse victims, who would be exposed to further harm if these models were revealed. The only known defense against all of these attacks is to tightly protect the models themselves: that is, the ensure that the complex systems of neural network weights and/or hash fingerprints are never revealed. Historical server-side systems to to great lengths to protect this data, even making their very algorithms confidential. This was feasible in server-side scanning systems because the data only exists on a centralized server. It does not work well with client-side scanning, where models must be distributed to users‚Äô phones. And so without some further technical ingredient, models cannot exist either on the server or on the user‚Äôs device. The only serious proposal that has attempted to address this technical challenge was devised ‚Äî and then subsequently abandoned ‚Äî by Apple in 2021. That proposal aimed only at detecting known content using a perceptual hash function. The company proposed to use advanced cryptography to ‚Äúsplit‚Äù the evaluation of hash comparisons between the user‚Äôs device and Apple‚Äôs servers: this ensured that the device never received a readable copy of the hash database. Apple‚Äôs proposal failed for a number of reasons, but its technical failures provided important lessons that have largely been ignored by the Commission. While Apple‚Äôs system protected the hash database, it did not protect the code of the proprietary neural-network-based hash function Apple devised. Within two weeks of the public announcement, users were able to extract this code and devise both the collision attacks and evasion attacks I mentioned above. One of the first ‚Äúmeaningful‚Äù collisions against NeuralHash, found by Gregory Maxwell. Evasion attacks against Apple‚Äôs NeuralHash, from Struppek et al. (source) The Commission‚Äôs Impact Assessment deems the Apple approach to be a success, and does not grapple with this failure. I assure you that this is not how it is viewed within the technical community, and likely not within Apple itself. One of the most capable technology firms in the world threw all their knowledge against this problem, and were embarrassed by a group of hackers: essentially before the ink was dry on their proposal. This failure is important because it illustrates the limits of our capabilities: at present we do not have an efficient means for evaluating complex neural networks in a manner that allows us to keep them secret. And so model extraction is a real possibility in all proposed client-side scanning systems today. Moreover, as my colleagues and I have shown, even ‚Äútraditional‚Äù perceptual hash functions like Microsoft‚Äôs PhotoDNA are vulnerable to evasion and collision attacks, once their code becomes available. These attacks will proliferate, if only because 4chan is a thing: and because some people on the Internet love nothing more than hurting other Internet users. From Prokos et al. (source) This example shows how a neural-network based hash function (NeuralHash) can be misled, by making imperceptible changes to an image. In practice, the Commission‚Äôs proposal ‚Äî if it is implemented in production systems ‚Äî invites a range of technical attacks that we simply do not comprehend today, and that scientists have barely begun to think about. Moreover, the Commission is not content to restrain themselves to scanning for known CSAM content as Apple did. Their desire to target previously unknown content as well as textual content such as ‚Äúgrooming behavior‚Äù poses risks from many parties and requires countermeasures against abuse and surveillance that are completely undeveloped. Worse: the ‚Äúgrooming behavior‚Äù requirement implies that untested, perhaps not-yet-developed AI language models will be a core part of tomorrow‚Äôs security systems. This is worrisome, since these models have failure modes and exploit opportunities that we are only beginning to explore. In my discussion so far I have only scratched the surface of this issue. My analysis today does not consider even more basic issues, such as how we can trust that the purveyors of these opaque models are honest, and that the model contents have not been altered: perhaps by insider attack or malicious outside hackers. Each of these threats was once theoretical, and I have seen them all occur in just the last several years. Nor does it consider how the scope of these systems might be increased by future governments, and how this infrastructure will make future abuses more likely. In conclusion, I hope that the Commission will rethink its hurried schedule and give this proposal enough time to be evaluated by scientists and researchers here in Europe and around the world. We should seek to understand these technical details as a precondition for mandating new technologies, rather than attempting to ‚Äúbuild the airplane while we are flying in it‚Äù, which is very much what this proposal will encourage. Thank you for your time. By Matthew Greenin backdoors, messagingMarch 23, 2023March 23, 20232,143 Words3 Comments Why encrypted backup is so important You might have seen the news today that Apple is announcing a raft of improvements to Macs and iOS devices aimed at improving security and privacy. These include FIDO support, improvements to iMessage key verification, and a much anticipated announcement that the company is abandoning their plans for (involuntary) photo scanning. While every single one of these is exciting, one announcement stands above the others. This is Apple‚Äôs decision to roll out (opt-in) end-to-end encryption for iCloud backups. While this is only one partial step in the right direction, it‚Äôs still a huge and decisive step ‚Äî one that I think will substantially raise the bar for cloud security across the whole industry. If you‚Äôre looking for precise details on all of these features, see Apple‚Äôs description here or their platform security guide. Others will no doubt have the time to do deep-dive explanations on each one. (I was given a short presentation by Apple today, and was provided the opportunity to ask a bunch of questions that their representative answered thoughtfully. But this is no substitute for a detailed look at the technical specs.) In the rest of this post I want to zero in on end-to-end encrypted iCloud backup, and why I think this announcement is such a big deal. Smartphones and cloud backup: the biggest consumer privacy compromise you never heard of If you‚Äôre the typical smartphone or tablet user, your devices have become the primary repository for your private papers, photos and communications. Imagine some document that your grandparents would have kept on a shelf or inside of a locked drawer in their home. Today the equivalent document probably resides in one of your devices. This data is the most personal stuff in a human life: your private family photos, your mail, your financial records, even a history of the books you read and which pages you found meaningful. Of course, it also includes new types of information that are unimaginably more valuable and invasive than anything your grandparents could have ever imagined. But this is only half the story. If you‚Äôre the typical user, you don‚Äôt only keep this data in your device. An exact duplicate exists in a data center hundreds or thousands of miles away from you. Every time you snap a photo, each night while you sleep, this doppelganger is scrupulously synchronized through the tireless efforts of cloud backup software ‚Äî usually the default software built into your device‚Äôs operating system. It goes without saying that you, dear reader, might not be the typical user. You might be one of the vanishingly small fraction of users who change their devices‚Äô default backup policies. You might be part of the even smaller fraction who back up their phone to a local computer. If you‚Äôre one of those people, congratulations: you‚Äôve made good choices. But I would beg you to get over it. You don‚Äôt really matter. The typical user does not make the same choices as you did. The typical user activates cloud backup because their device urges them to do at setup time and it‚Äôs just so easy to go along. The typical user sends their most personal photos to Apple or Google, not because they‚Äôve thought deeply about the implications, but because they can‚Äôt afford to lose a decade of family memories when their phone or laptop breaks down. The typical user can‚Äôt afford to shell out an extra $300 to purchase extra storage capacity, so they buy a base-model phone and rely on cloud sync to offload the bulk of their photo library into the cloud (for a small monthly fee), so their devices can still do useful things. And because the typical user does these things, our society does these things. I am struggling to try to find an analogy for how crazy this is. Imagine your country held a national referendum to decide whether most citizens should be compelled to photocopy their private photos and store them in a centralized library ‚Äî one that was available to both police and motivated criminals alike. Would anyone vote in favor of that, even if there was technically an annoying way to opt out? As ridiculous as this sounds, it‚Äôs effectively what we‚Äôve done to ourselves over the past ten years: but of course we didn‚Äôt choose any of it. A handful of Silicon Valley executives made the choice for us, in pursuit of adoption metrics and a ‚Äúmagical‚Äù user experience. What‚Äôs done is done, and those repositories now exist. And that should scare you. It terrifies me, because these data repositories are not only a risk to individual user privacy, they‚Äôre effectively a surveillance super-weapon. However much damage as we‚Äôve done to our privacy with search engines and cellphone location data, the private content of our papers is the final frontier in the battle for our privacy. And in less than a decade, we‚Äôve already lost the war. Apple‚Äôs slow motion battle to encrypt your backups To give credit where it‚Äôs due, I think the engineers at Apple and Google were the first to realize what they‚Äôd unleashed ‚Äî maybe even before many of us on the outside were even aware of the scale of the issue. In 2016, Apple began quietly deploying new infrastructure designed to secure user encryption keys in an ‚Äúend-to-end‚Äù fashion: this means that keys would only be accessible only to the user who generated them. The system Apple deployed was called the ‚ÄúiCloud Key Vault‚Äú, and it is consists of hundreds of specialized devices called Hardware Security Modules (HSMs) that live in the company‚Äôs data centers. The devices store user encryption keys. Those keys are in turn gated by a user-chosen passcode, which is typically the same passcode you use daily to unlock your device. A user who knows their passcode can ask for a copy of their key. An attacker who can‚Äôt guess that passcode (in a small number of attempts) cannot. Most critically: Apple counts themselves in the category of people who might be attackers. This means they went to some trouble to ensure that even they cannot (be forced to) bypass this system. When it comes to encrypted backup there is essentially one major problem: how to store keys. I‚Äôm not saying this is literally the only issue, far from it. But once you‚Äôve found a way for users to securely store and recover their keys, every other element of the system can be hung from that. The remaining problems are still important! There are still, of course, reasonable concerns that some users will forget their device passcode and thus lose access to backups. You need a good strategy when this does happen. But even if solving these problems took some time and experimentation, it should only have been a matter of time until Apple activated end-to-end encryption for at least a portion of their user base. Once broadly deployed, this feature would have sent a clear signal to motivated attackers that future abuse of cloud backup repositories wasn‚Äôt a good place to invest resources. But this is not quite what happened. What actually happened is unclear, and Apple refuses to talk about it. But the outlines of what we do know tells a story that is somewhere between ‚Äúmeh‚Äù and ‚Äúugh‚Äú. Specifically, reporting from Reuters indicates that Apple came under pressure from government agencies: these agencies wished Apple to maintain the availability of cleartext backup data, since this is now an important law enforcement priority. Whatever the internal details, the result was not so much a retreat but a rout: Once the decision was made, the 10 or so experts on the Apple encryption project ‚Äî variously code-named Plesio and KeyDrop ‚Äî were told to stop working on the effort, three people familiar with the matter told Reuters. For what it‚Äôs worth, some have offered alternative explanations. John Gruber wrote a post that purports to push back on this reporting, arguing that the main issues were with users who got locked out of their own backups. (Apple has recently addressed this by deploying a feature that allows you to set another user as your recovery assistant.) However even that piece acknowledges that government pressure was likely an issue ‚Äî a key dispute is about whether the FBI killed the plan, or whether fear of angering the FBI caused Apple to kill its own plan. Whatever caused it, this setback did not completely close the door on end-to-end encrypted backups, of course. Despite Apple‚Äôs reticence, other companies ‚Äî notably Google and Meta‚Äôs WhatsApp ‚Äî have continued to make progress by deploying end-to-end encrypted systems very similar to Apple‚Äôs. At present, the coverage is partial: Google‚Äôs system may not encrypt everything, and WhatsApp‚Äôs backups are opt-in. Selective encryption and client-side scanning: a road not taken As of July 2021 the near-term deployment of end-to-end encrypted backups seemed inevitable to me. In the future, firms would finally launch the technology and demonstrate that it works ‚Äî at least for some users. This would effectively turn us back towards the privacy world of 2010 and give users a clear distinction between private data and non-private user data. There was another future where that might not happen, but I thought that was unlikely. One thing I did not foresee was a third possible future: one where firms like Apple rebuilt their encryption so we could have both end-to-end encryption ‚Äî and governments could have their surveillance too. In August of last year, Apple proposed such a vision. In a sweeping announcement, the company unveiled a plan to deploy ‚Äúclient-side image scanning‚Äù to 1.75 billion iCloud users. The system, billed as part of the company‚Äôs ‚ÄúChild Safety‚Äù initiative, used perceptual hashing and cryptography to scan users‚Äô private photo libraries for the presence of known child sexual abuse media, or CSAM. This would allow Apple to rapidly identify non-compliant users and, subject to an internal review process, report violators to the police. Apple‚Äôs proposal was not the first system designed to scan cloud-stored photos for such imagery. It was the first system capable of working harmoniously with end-to-end encrypted backups. This fact is due to the specific way that Apple proposed to conduct the scanning. In previous content scanning systems, user files are scanned on a server. This required that content must be uploaded in plaintext, i.e., unencrypted form, so that the server can process it. Apple‚Äôs system, on the other hand, performed the necessary hashing and scanning on the user‚Äôs own device ‚Äî before the data was uploaded. The technical implications of this design are critical: Apple‚Äôs scanning would continue to operate even if Apple eventually flipped the switch to activate end-to-end encryption for your private photos (as they did today.) And let‚Äôs please not be dense about this. While Apple‚Äôs system did not yet encrypt cloud-stored photos last year (that‚Äôs the new announcement Apple made today), encryption plans were the only conceivable reason one would deploy a client-side scanning system. There was no other reasonable explanation. Users have a difficult time understanding even simple concepts around encryption. And that‚Äôs not their fault! Firms constantly say things like ‚Äúyour files are encrypted‚Äù even when they store the decryption keys right next to the encrypted data. Now try explaining the difference between ‚Äúencryption‚Äù and ‚Äúend-to-end encryption‚Äù along with forty-six variants of ‚Äúend-to-end encryption that has some sort of giant asterisk in which certain types of files can be decrypted by your cloud provider and reported to the police.‚Äù Who even knows what privacy guarantees those systems would offer you ‚Äî and how they would evolve. To me it felt like the adoption of these systems would signal the end of a meaningful concept of user-controlled data. Yet this came very close to happening. It could still happen. It didn‚Äôt though. And to this day I‚Äôm not entire sure why. Security and privacy researchers told the company exactly how dangerous the idea was. Apple employees reacted negatively to the proposal. But much to my surprise, the real clincher was the public‚Äôs negative reaction: as much as people hate CSAM, people really seemed to hate the idea that their private data might be subject to police surveillance. The company delayed the feature and eventually abandoned it, with today‚Äôs result being the end of the saga. I would love to be a fly on the wall to understand how this went down inside of Apple. I doubt I‚Äôll ever learn what happened. I‚Äôm just glad that this is where we wound up. What‚Äôs next? I wish I could tell you that Apple‚Äôs announcement today is the end of the story, and now all of your private data will be magically protected ‚Äî from hackers, abusive partners and the government. But that is not how things work. Apple‚Äôs move today is an important step. It hardens certain walls: very important, very powerful walls. It will send a clear message to certain attackers that deeper investment in cloud attacks is probably not worthwhile. Maybe. But there is still a lot of work to do. For one thing, Apple‚Äôs proposal (which rolls out in a future release) is opt-in: users will have to activate ‚ÄúAdvanced Protection‚Äù features for their iCloud account. With luck Apple will learn from this early adoption, and find ways to make it easier to encourage more users to adopt this feature. But that‚Äôs a ways off. And even if Apple does eventually move most of their users into end-to-end encrypted cloud backups, there will always be other ways to compromise someone‚Äôs data. Steal their phone, guess their password, jailbreak a partner‚Äôs phone, use sophisticated targeted malware. And of course a huge fraction of the world will still live under repressive governments that don‚Äôt need to trouble with breaking into cloud providers. But none of these attacks will be quite as easy as attacks on non-E2E cloud backup, and none will offer quite the same level convenience and scale. Today‚Äôs announcement makes me optimistic that we seem to be heading ‚Äî in fits and starts ‚Äî to a world where your personal data will belong to you. Cover photo by Scott Robinson, used under CC license. By Matthew Greenin AppleDecember 7, 2022December 8, 20222,335 Words1 Comment One-Time Programs One of the things I like to do on this blog is write about new research that has a practical angle. Most of the time (I swear) this involves writing about other folks‚Äô research: it‚Äôs not that often that I write about work that comes out of my own lab. Today I‚Äôm going make an exception to talk about a new paper that will be appearing at TCC ‚Äô22. This is joint work with my colleagues Abhishek Jain and Aarushi Goel along with our students Harry Eldridge and Max Zinkus. This paper is fun for three reasons: (1) it addresses a cool problem, (2) writing about it gives me a chance to cover a bunch of useful, general background that fits the scope of this blog [indeed, our actual research won‚Äôt show up until late in the post!], and most critically (3) I want people to figure out how to do it better, since I think it would be neat to make these ideas more practical. (Note, if you will, that TCC stands for Theory of Cryptography conference, which is kind of a weird fit.) Our work is about realizing a cryptographic primitive called the One-Time Program (OTP). This is a specific kind of cryptographically obfuscated computer program ‚Äî that is, a program that is ‚Äúencrypted‚Äù but that you can mail (literally) to someone who can run it on any untrusted computer, using input that the executing party provides. This ability to send ‚Äúsecure, unhackable‚Äù software to people is all by itself of a holy grail of cryptography, since it would solve so many problems both theoretical and practical. One-time programs extend these ideas with a specific property that is foreshadowed by the name: the executing computer can only run a OTP once. OTPs are one of the cooler ideas to pop out of our field, since they facilitate so many useful things. Aside from the obvious dark-side implications (perfect DRM! scary malware!) they‚Äôre useful for many constructive applications. Want to send a file encrypted under a weak password, but ensure nobody can just brute-force the thing? Just send a OTP that checks the password and outputs the file. Want to send a pile of sensitive data and let users compute statistical functions over it using differential privacy? Sure. Time-lock encryption? Why not. In fact, OTPs are powerful enough that you can re-invent many basic forms of cryptography from them‚Ä¶ provided you‚Äôre not too worried about how efficient any of it will be. As we‚Äôll see in a minute, OTPs might be nice, but they are perhaps a little bit too good to be true. Most critically they have a fundamental problem: building them requires strong model-breaking assumptions. Indeed, many realizations of OTPs require the program author to deliver some kind of secure hardware to the person who runs the program.* This hardware can be ridiculously simple and lightweight (much simpler than typical smartcards) but the need to have some of it represents a very big practical limitation. This is likely why we don‚Äôt have OTPs out in the world ‚Äî the hardware required by scientists does not actually exist. In this work we tried to answer a very similar question. Specifically: can we use real hardware (that already exists inside of phones and cloud services) to build One-Time Programs? That‚Äôs the motivation at a high level. Now for the details‚Ä¶ and boy are there a lot of them. One-Time Programs One-Time Programs (OTPs) were first proposed by Goldwasser, Kalai and Rothblum (GKR) back in CRYPTO 2008. At a surface level, the idea is quite simple. Let‚Äôs imagine that Alice has some (secret) computer program P that takes in some inputs, cogitates for a bit, and then produces an output. Alice wishes to mail a copy of P to her untrustworthy friend Bob who will then be able to run it. However Alice (and Bob) have a few very strict requirements: Bob can run the program on any input he wants, and he will get a correct output. Bob won‚Äôt learn anything about the program beyond what he learns from the output (except, perhaps, an upper-bound on its size/runtime.) Bob can run the program exactly once. Let‚Äôs use a very specific example to demonstrate how these programs might work. Imagine that Alice wants to email Bob a document encrypted under a relatively weak password such as a 4-digit PIN. If Alice employed a traditional password-based encryption scheme, this would be a very bad idea! Bob (or anyone else who intercepts the message before it reaches Bob) could attempt to decrypt the document by systematically testing each of the (10,000) different passwords until one worked correctly. Using a one-time program, however, Alice can write a program with code that looks like this, and turn it into an OTP: Program P: takes an argument "password" 1. if password != "<secret password>", output "BAD" 2. else output <secret document> I don‚Äôt know if the kids even get my meme choices anymore. When Bob receives an OTP of the program above, he can then run it on some password input he chooses ‚Äî even if Alice is no longer around to help him. More critically, because it‚Äôs a one-time program, Bob will only be able to try a single password guess. Assuming Bob knows the right password this is just fine. But a recipient who does not know the password will have to guess it correctly the first time. The nice implication is that even a ‚Äúweak‚Äù 4-digit PIN reasonably to safe to use as a password. (Of course if Alice is worried about Bob innocently fat-fingering his password, she can send him a few different copies of the same program. Put differently: one-time programs trivially imply N-time programs.) One-time programs have many other useful applications. Once I can make ‚Äúunhackable‚Äù limited-use software, I can send you all sorts of useful functionalities based on secret intellectual property or private data rather than keeping that stuff locked up on my own server. But before we can do those things, we need to actually build OTPs. Why hardware? If you spend a few minutes thinking about this problem, it should become obvious that we can‚Äôt build OTPs using (pure) software: at least not the kind of software that can run on any general-purpose computer. The problem here stems from the ‚Äúcan only run once‚Äù requirement. Imagine that you send me a pure software version of a (purported) One-Time Program. (By this I mean: a piece of software I can run on a standard computer, whether that‚Äôs a Macbook or a VM/emulator like QEMU.) I‚Äôm supposed to be able to run the program once on any input I‚Äôd like, and then obtain a valid output. The program is never supposed to let me run it a second time on a different input. But of course if I can run the software once that way, we run into the following obvious problem: What stops me from subsequently wiping clean my machine (or checkpointing my VM) and then re-installing a fresh copy of the same software you sent‚Äîand running it a second time on a different input? Sadly the answer is: nothing can prevent any of this. If you implement your purported ‚ÄúOTP‚Äù using (only) software then I can re-run your program as many times as I want, and each time the program will ‚Äúbelieve‚Äù it‚Äôs running for the very first time. (In cryptography this is sometimes called a ‚Äúreset attack‚Äù.) Keanu experiences a reset attack. For those who are familiar with multi-party computation (MPC), you‚Äôll recognize that such attacks can be thwarted by requiring some interaction between the sender and recipient each time they want to run the program. What‚Äôs unique about OTPs is that they don‚Äôt require any further interaction once Alice has sent the program: OTPs work in a ‚Äúfire and forget‚Äù model. In their original paper, GKR noted this problem and proposed to get around it by changing the model. Since pure software (on a single machine) can‚Äôt possibly work, they proposed to tap the power of tamper-resistant hardware. In this approach, the program author Alice sends Bob a digital copy of the OTP along with a physical tamper-resistant hardware token (such as a USB-based mini-HSM). The little token would be stateful and act like one of those old copy-protection dongles from the 1990s: that is, it would contains cryptographic key material that the program needs in order to run. To run the program, Bob would simply pop this ‚Äúhardware token‚Äù into his computer. A single USB token might contain thousands of ‚Äúone-time memories.‚Äù Now you might object: doesn‚Äôt using hardware make this whole idea kind of trivial? After all, if you‚Äôre sending someone a piece of specialized tamper-resistant hardware, why not just pop a general-purpose CPU into that thing and run the whole program on its CPU? Why use fancy cryptography in the first place? The answer here has to do with what‚Äôs inside that token. Running general programs on tamper-proof hardware would require a token with a very powerful and expensive (not to mention complex) general-purpose CPU. This would be costly and worse, would embed a large attack software and hardware attack surface ‚Äî something we have learned a lot about recently thanks to Intel‚Äôs SGX, which keeps getting broken by researchers. By contrast, the tokens GKR propose are absurdly weak and simple: they‚Äôre simple memory devices that spit out and erase secret keys when asked. The value of the cryptography here is that Bob‚Äôs (untrusted) computer can still do the overwhelming share of the actual computing work: the token merely provides the ‚Äúicing‚Äù that makes the cake secure. But while ‚Äúabsurdly weak and simple‚Äù might lower the hardware barrier to entry, this is not the same thing as having actual tokens exist. Indeed it‚Äôs worth noting that GKR proposed their ideas way back in 2008, it is now 2022 and nobody (to my knowledge) has ever built the necessary token hardware to deploy the in the world. (One could prototype such hardware using an open HSM platform, but how secure would that actually be ‚Äî compared to a proper engineering effort by a major company like Apple, Google or Amazon?) And yet One-Time Programs are neat. It would be useful to be able to write and run them on real devices! For many years I‚Äôve run into problems that would melt away if we could deploy them easily on consumer devices. Wouldn‚Äôt it be great if we could build them using some hardware that already exists? How to build a (GKR) One-Time Program In order to explain the next part, it‚Äôs necessary to give an extremely brief overview of the GKR construction for One-Time Programs, and more specifically: their specialized tokens. This construction is based on garbled circuits and will make perfect sense if you‚Äôre already familiar with that technique. If not, it will require a little bit more explanation. GKR‚Äôs idea is to rely on many individual tokens called One-Time Memories (OTM). An invidual OTM token works like this: When a program author (Alice) sets one up, she gets to install two different strings into it: let‚Äôs call them K0 and K1. She can then mail the token to Bob. When Bob receives the token and wants to use it, he can ask the token for either of the two strings (0/1). The token will hand Bob the desired string (either K0 or K1.) Once the token has given Bob the string he asked for, it permanently erases the other string. The strings themselves need not be very long: 128 bits is ideal. To use these tokens for building One-Time Programs, Alice might need to set up a few hundred or a few thousand of these ‚Äútokens‚Äù (which can technically all be glommed together inside a single USB device) and send them to Bob. Once you assume these tokens, the GKR style of building One-Time Programs is pretty straightforward if you‚Äôre a cryptographer. Summarized to someone who is familiar with garbled circuits: the basic idea is to take the classical Yao two-party computation (2PC) scheme and replace the (interactive) Oblivious Transfer portion by sending the evaluator a set of physical One-Time Memory tokens. If that doesn‚Äôt work for you, a slightly more detailed explanation is as follows: Alice first converts her program P into a boolean circuit, like the one below: Having done that, she then assigns two random cryptographic keys (annoyingly called ‚Äòlabels‚Äô) to every single wire in the circuit. One key/label corresponds to the ‚Äú0‚Äù value on that wire, and the other to the ‚Äú1‚Äù bit. Notice that the input wires (top-left) also count here: they each get their own pair of keys (labels) that correspond to any input bit. Alice next ‚Äúgarbles‚Äù the circuit (encrypting each gate) using a clever approached devised by Andrew Yao, which I won‚Äôt describe precisely here but Vitalik Buterin nicely explains it in this blog post. The result is that each table is replaced with an encrypted table of ciphertexts: anyone who has two of the appropriate labels going into that gate will be able to evaluate it, and in return they will receive the appropriate label for the gate‚Äôs output wire. Hence all Bob needs is the appropriate keys/labels for the value Bob wishes to feed into the input wires at the top-left of the circuit ‚Äî and then he can then recursively evaluate the circuit all the way to the output wires. From Bob‚Äôs perspective, therefore, all he needs to do is obtain the labels that correspond to the input arguments he wants to feed into the circuit. He will also need a way to translate the labels on the output wires into actual bits. (Alice can send Bob a lookup table to help him translate those labels into actual bits, or she can just make the circuit output raw binary values instead of labels on those wires.) A critical warning is that Bob must receive only one input label for each wire: if he had more than that, he could run the circuit on two ‚Äúdifferent inputs.‚Äù (And in this case, many other bad things would probably happen.) Hence the high-level problem is how to deliver the appropriate input labels to Bob, while ensuring that he gets exactly one label for each wire and never more. And this is where the One-Time Memory tokens fit in perfectly. Alice will set up exactly one OTM token for each input wire: it will contain both labels for that wire. She‚Äôll send it to Bob. Bob can then query each token to obtain exactly one label for that wire, and then use those labels to evaluate the rest of the circuit. The OTM token will destroy all of the unused labels: this ensures that Bob can only run the program on exactly one input. And that‚Äôs the ballgame.** So where do I pick myself up some One-Time Memories? You buy them at your local One-Time Memory store, obviously. Seriously, this is a huge flaw in the hardware-based OTP concept. It would be awfully useful to have OTPs for all sorts of applications, assuming we had a bunch of One-Time Memory tokens lying around and it was easy to ship them to people. It would be even more awesome if we didn‚Äôt need to ship them to people. For example: Imagine there was a publicly-accessible cloud provider like Google or Apple that had lots of One-Time Memories sitting in your data center that you could rent. Alice could log in to Google and set up a bunch of OTM ‚Äútokens‚Äù remotely, and then simply send Bob the URLs to access them (and hence evaluate a OTP that Alice mails him.) As long as the cloud provider uses really good trusted hardware (for example, fancy HSMs) to implement these memories, then even the cloud provider can‚Äôt hack into the secrets of Alice‚Äôs One-Time Programs or run them without Bob‚Äôs input. Alternatively, imagine we had a bunch of One-Time Memories built into every smartphone. Alice couldn‚Äôt easily send these around to people, but she could generate One-Time Programs that she herself (or someone she sends the phone to) could later run. For example, if Alice could build a sophisticated biometric analysis program that uses inputs from the camera to unlock secrets in her app, and she could ensure that the program stays safe and can‚Äôt be ‚Äúbrute-forced‚Äù through repeated execution. Even if someone stole Alice‚Äôs phone, they would only be able to run the program (once) on some input, but they would never be able to run it twice. The problem is that, of course, cloud providers and phone manufacturers are not incentivized to let users build arbitrary ‚Äúunhackable‚Äù software, nor are they even thinking about esoteric things like One-Time Memories. No accounting for taste. And yet cloud providers and manufacturers increasingly are giving consumers access to specialized APIs backed by secure hardware. For example, every single iPhone ships with a specialized security chip called the Secure Enclave Processor (SEP) that performs specific cryptographic operations. Not every Android phone has such processor, but most include a small set of built-in TrustZone ‚Äútrustlets‚Äù ‚Äî these employ processor virtualization to implement ‚Äúsecure‚Äù mini-apps. Cloud services like Google, Apple iCloud, Signal and WhatsApp have begun to deploy Hardware Security Modules (HSMs) in their data centers ‚Äî these store encryption keys for consumers to use in data backup, and critically, are set up in such a way that even the providers can‚Äôt hack in and get the keys. Unfortunately none of the APIs for these hardware services offer anything as powerful as One-Time Programs or (even) One-Time Memories. If anything, these services are locked down specifically to prevent people from doing cool stuff: Apple‚Äôs SEP supports a tiny number of crypto operations. TrustZone does support arbitrary computation, but today your applet must be digitally signed by a phone manufacturer or a TEE-developer like Qualcomm before you can touch it. Consumer-facing cloud providers definitely don‚Äôt expose such powerful functionality (for that you‚Äôd need something like AWS Nitro.) The same goes for ‚Äúsimple‚Äù functionalities like One-Time Memories: they may be ‚Äúsimple‚Äù, but they are also totally weird and why would any consumer manufacturer bother to support them? But let‚Äôs never forget that cryptography is a sub-field of computer security. In that field when someone doesn‚Äôt let you run the machine you want to run, you figure out how to build it. What hardware APIs do manufacturers and cloud providers support? Not very many, unfortunately. The Keymaster meets the Gatekeeper. I think there was also a dog in this movie? If you hunt around the Apple Developer Documentation for ways to use the iPhone SEP, for example, you‚Äôll find APIs for generating public keys and using them, as well as ways to store keys under your passcode. Android (AOSP) provides similar features via the ‚ÄúGatekeeper‚Äù and ‚ÄúKeymaster‚Äù trustlets. Consumer-facing cloud services provide HSM-backed services that also let you store your keys by encrypting them under a password. None of this stuff is a ‚ÄúOne-Time Memory,‚Äù unfortunately. Most of it isn‚Äôt even stateful‚Ä¶ with one exception. ‚ÄúCounter lockboxes‚Äù Reading through documentation reveals one functionality that isn‚Äôt exactly what we need, but at least has some promise. In its documentation, Apple calls this function a ‚Äúcounter lockbox‚Äú, and it‚Äôs designed to store encryption keys that are protected with a user-selected password. The same person (or someone else) can later retrieve the key by sending in the right passcode. In the event that the entered passcode is not the right one, the lockbox increments an ‚Äúattempt counter‚Äù that caps the number of incorrect guesses that will be permitted before the key is locked forever. And I do mean forever. When the user exceeds the maximum number of guesses, something interesting happens (here is Apple‚Äôs version): Source: Apple Platform Security guide. While I‚Äôm going to use Apple‚Äôs terminology, lockboxes are not just an Apple thing: a nearly-identical functionality exists inside every phone and in basically all of the consumer-facing cloud services I mentioned above. Since counter lockboxes erase things, and they‚Äôre pretty much ubiquitous, this gives us hope. Even though they are not OTMs, maybe we can somehow use them to build OTMs. To explain how this works, we first need to give a clear description of how a basic a counter lockbox works. Here‚Äôs a simplified version:*** To set up a fresh lockbox, Alice provides an encryption key K and a password P as well as a ‚Äúmaximum attempt counter‚Äù M. The initial attempt counter is set to A := 0. The token stores (K, P, A, M). When Bob wants to access the lockbox, he sends in a ‚Äúguess‚Äù P‚Äô. 1. If P‚Äô == P (i.e., the guess is correct), the token returns K and resets A := 0. 2. Else if P‚Äô != P (the guess is incorrect), the token sets A := A + 1 and returns ‚Äúbad password‚Äù. If A == M (i.e., the maximum attempts have been reached) then the token wipes its memory completely. If you remember our description of a one-time memory (OTM) further above, you‚Äôll notice that a lockbox stores only one string (key), but that an OTM has two different strings stored in it. Moreover, the OTM will always give us one or the other string, but a counter lockbox only gives us its single string when we enter a password. But perhaps we can simulate OTMs by using multiple lockboxes. Imagine that Alice has access to two different (fresh) counter lockboxes, as illustrated below. Let‚Äôs assume she configures both lockboxes to permit exactly one password attempt (M=1). Next, she stores the string K0 into the lockbox on the left, and sets it up to use password ‚Äú0‚Äù. She then places the string K1 into the lockbox on the right and set it up with password ‚Äú1‚Äù, as follows: The picture above shows you what‚Äôs inside each lockbox. But you need to remember that to anyone other than Alice, lockboxes are kind of a black box. That is, they don‚Äôt reveal the key or password they‚Äôve set up with ‚Äî until the lockbox is queried on a specific password. To exploit this fact, Alice can shuffle the two lockboxes‚Äô and send them to Bob in a random ordering. Until he tries to access one, these lockboxes will look like this to Bob: So how does Bob ‚Äúopen‚Äù the lockboxes to reliably obtain one of the two strings? For a moment let‚Äôs assume that Bob is honest, and wants to get either K0 or K1. He does not know which of the two lockboxes contains the string he wants. He can, however, employ a simple strategy to obtain it. Assuming Bob‚Äôs choice is ‚Äú0‚Äù, he can simply query both lockboxes on password ‚Äú0‚Äù. One of the two lockboxes will output K0, and the other will output ‚Äúbad password‚Äù. (If his choice is ‚Äú1‚Äù, then he can do the same using that as the password.) This strategy works perfectly. In either case Bob always obtains the string he wanted, and the other string ends up getting erased: just like a One-Time Memory! The problem with this approach is that Bob might not be honest. A cheating Bob can query the first lockbox on password ‚Äú0‚Äù, and if that lockbox hands him K0, he can switch strategies and query the remaining lockbox on password ‚Äú1‚Äù. If this works, he will end up with both of the strings ‚Äî something that should never happen in a One-Time Memory. And this is very bad! Imagine we use such broken ‚Äúmemories‚Äù to build GKR one-time programs and Bob pulls this off for even a single input wire in our garbled circuit, then he will be able to query the circuit on multiple distinct inputs. (And it gets worse: if Bob obtains two labels on the same input wire, the garbled circuit construction will often unravel like a cheap sweater.) The good news here is that Bob‚Äôs strategy doesn‚Äôt always work. He will sometimes get both strings, but sometimes he won‚Äôt. Half the time he‚Äôll query the first lockbox on ‚Äú0‚Äù and it will say ‚Äúbad password‚Äù‚Ä¶ and the key it holds will be gone forever. (Bob can still go on to get the other string from the untouched lockbox, but that‚Äôs not a problem.) This means we have at best a 1/2 probability of thwarting a cheating Bob. That‚Äôs good‚Ä¶ but can we reduce Bob‚Äôs chances even more? The most obvious approach is to use more lockboxes. Instead of two lockboxes, imagine Alice sets up, say, 80 different shuffled lockboxes divided into two sets. The first forty lockboxes can be programmed with the ‚Äú0‚Äù password, and the other forty can be programmed with password ‚Äú1‚Äù: then all will be mixed together. Our goal in this is to ensure that Bob will obtain K0 only if he guesses where all of the ‚Äú0‚Äù lockboxes are. To ensure this, Alice will use secret sharing to split K0 into 40 different ‚Äúshares‚Äù, with the property that all of the shares are needed to obtain the original string. Each share can be placed into one of the 40 boxes. (A similar process will be used to protect K1.) Of course Bob can still cheat and try to guess his way through this maze Alice has built, but he will need to correctly guess where each of the ‚Äú0‚Äù-password lockboxes are located without making a single mistake: since even a single wrong guess will doom his chances to get both strings. Such a lucky guessing strategy is extremely challenging for Bob to pull off. (The analysis is not quite equivalent to Bob flipping a coin ‚Äúheads‚Äù forty times in a row [1/240] but it results in a probability that‚Äôs similarly low.) By adjusting the number of lockboxes she uses, Alice can carefully tune the security level of each ‚Äúvirtual one-time memory‚Äù so that cheating (almost) never works. This sure seems like a lot of lockboxes Well, I said this paper is appearing in the Theory of cryptography conference, didn‚Äôt I? Obviously the proposal above is not the end of the story. In that solution, for an desired S-bit security level the ‚Äúnaive‚Äù approach requires Alice to set up 2S lockboxes (or O(S) if you don‚Äôt like constants.) To build GKR one-time programs from this, Alice would need to use that O(S) lockboxes for every input bit Bob might want to feed into the program. Surely we can do better. The rest of our paper looks at ways to reduce the number of lockboxes required, mainly focusing on the asymptotic case. Our second proposal reduces the number of lockboxes to about O(1) lockboxes for every input bit (when there are many input wires) and the final proposal removes the dependence on the number of input bits entirely. This means in principle we can run programs of arbitrary input length using some reasonable fixed number of lockboxes. I can‚Äôt possibly do justice to the full details here, except to note that we rely on some very cool techniques proposed by others. Our first transform relies on a ‚Äòrobust garbling‚Äô technique devised by Almashaqbeh, Benhamouda, Han, Jaroslawicz, Malkin, Nicita, Rabin, Shah and Tromer. The second uses an underappreciated tool called ‚ÄúLaconic OT‚Äù that was proposed by Cho, Dottling, Garg, Gupta, Miao and Polychroniadou. (Sadly, Laconic OT constructions are not quite ‚Äúconcretely‚Äù practical yet, so I hope these new applications will motivate some more practical research into that area.) The upshot is that we can, in principle, manufacture programs that run on user inputs of arbitrary size with a fixed cost of at most several thousand lockboxes. While this is still quite a large number in practice(!), it isn‚Äôt not infeasible. Such a quantity of lockboxes could be obtained by, for example, setting up boatloads of fake iCloud or Google accounts and then using Apple‚Äôs iCloud Key Vault or Google‚Äôs Titan Backup service to store a ‚Äúbackup key‚Äù for each of them. (I do not recommend you actually do any of this: I suspect both Apple and Google find such things irritating, and will make your life unpleasant if you try it.) Are there any downsides to this research? Maybe. There are many applications of obfuscated programs (including one-time programs) that are extremely bad for the world. One of those applications is the ability to build extremely gnarly ransomware and malware. This is presumably one of the reasons that systems like TrustZone and Intel SGX require developers to possess a certificate in order to author code that can run in those environments. For example: a ransomware author could encrypt a whole system and the store the keys inside of a One-Time Program that would, in turn, need to be executed on a valid chunk of a blockchain in order to release the keys. This blockchain would then incorporate a fragment that proves that the system owner has paid some sum of money to the malware developer. This system would be fully ‚Äòautonomous‚Äô in the sense that your computer, once infected, would never need to phone home to any ‚Äúcommand and control‚Äù infrastructure operated by the ransomware author. This would make malware networks harder to take down. If systems designers are worried about malware on SGX, our research shows that (eventually) those designers may also need to think about the availability of ‚Äúlockbox‚Äù-type functionalities as well. Or to give a shorter summary: don‚Äôt let the bad guys get more than a few thousand lockboxes, or they gain the power of secure computation and all that comes with it. And the exact number they need could be much smaller than the results we achieve in this paper. Perhaps at a higher level, the main lesson of this research is that computation is much easier than you‚Äôd expect it to be. If someone wants to do it badly enough, they‚Äôll always find a way. Notes: * There is a second line of work that uses very powerful cryptographic assumptions and blockchains to build such programs. I am very enthusiastic about this work [and my co-authors and I have also written about this], but this post is going to ignore those ideas and stick with the hardware approach. ** Of course that‚Äôs not really the ballgame. As with all simple descriptions of complex protocol papers, this simple explanation omits all the subtle details that matter for the security proof, but that aren‚Äôt terribly relevant for the purposes of this blog. These include all sorts of interesting questions about how to prove security in an environment where an adversary can query the tokens in a weird, arbitrary order. It‚Äôs a good paper! *** Some implementations will pick the key K for you. Others fix the maximum attempt counter at some constant (usually 10 attempts) rather than supporting any amount. All of these details can be worked around in practice. By Matthew Greenin ransomware, secure computation, security researchOctober 27, 2022November 7, 20225,101 Words5 Comments Posts navigation Older posts Create a website or blog at WordPress.com Menu Menu Home About Me Bitcoin Tipjar Top Posts All Posts Useful Cryptography Resources A Few Thoughts on Cryptographic Engineering Create a website or blog at WordPress.com Subscribe Subscribed A Few Thoughts on Cryptographic Engineering Join 930 other subscribers Sign me up Already have a WordPress.com account? Log in now. A Few Thoughts on Cryptographic Engineering Customize Subscribe Subscribed Sign up Log in Report this content View site in Reader Manage subscriptions Collapse this bar Loading Comments... Write a Comment... Email (Required) Name (Required) Website