LessWrong This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. LESSWRONGLWLoginHomeAll PostsConceptsLibraryBest of LessWrongSequence HighlightsRationality: A-ZThe CodexHPMORCommunity EventsLess Wrong Community Weekend 2024Fri Sep 13•BerlinRat Fest 2024Fri Sep 27•PhiladelphiaNorth Oakland: Board Games, March 5thWed Mar 6•OaklandRationalist Storytelling (French)Wed Mar 6•ParisDialogue MatchmakingSubscribe (RSS/Email)AboutFAQHomeAll PostsConceptsLibraryCommunityRecommendationsHighly Advanced Epistemology 101 for BeginnersEssays by Eliezer discussing truth, formal logic, causality, and metaethics. A good way for more ambitious readers to quickly get up to speed.First Post: The Useful Idea of Truth390Welcome to LessWrong!Ruby, Raemon, RobertM, habryka5y48500Pain is not the unit of Effortalkjash3y89279Speaking to Congressional staffers about AI riskAkash, hath6d23212CFAR Takeaways: Andrew CritchRaemon11d61Latest PostsCustomize Feed (Hide)Customize Feed (Show)Rationality+Rationality+World Modeling+World Modeling+AIAIWorld OptimizationWorld OptimizationPracticalPracticalCommunityCommunityPersonal Blog+57Voting Results for the 2022 ReviewBen Pace1mo390Anthropic release Claude 3, claims >GPT-4 PerformanceΩLawrenceC21hΩ737Claude 3 claims it's conscious, doesn't want to die or be modifiedMikhail Samin16h2563Grief is a fire saleNathan Young2d131Housing Roundup #7Zvi1d0110If you weren't such an idiot...kave, Mark Xu4d4741The Broken Screwdriver and other parablesbhauth1d1160Timaeus's First Four MonthsΩJesse Hoogland, Daniel Murfet, Stan van Wingerden, Alexander Gietelink Oldenziel6dΩ534Are we so good to simulate?KatjaGrace1d1822Anomalous Concept Detection for Detecting Hidden CognitionPaul Colognese1d367The World in 2029Nathan Young3d3642Some costs of superpositionΩLinda Linsefors2dΩ687Notes on Dwarkesh Patel’s Podcast with Demis HassabisZvi4d0Load MoreAdvanced Sorting/FilteringQuick Takesjimrandomh16h6203There's been a lot of previous interest in indoor CO2 in the rationality community, including an (unsuccessful) CO2 stripper project, some research summaries and self experiments. The results are confusing, I suspect some of the older research might be fake. But I noticed something that has greatly changed how I think about CO2 in relation to cognition. Exhaled air is about 50kPPM CO2. Outdoor air is about 400ppm; indoor air ranges from 500 to 1500ppm depending on ventilation. Since exhaled air has CO2 about two orders of magnitude larger than the variance in room CO2, if even a small percentage of inhaled air is reinhalation of exhaled air, this will have a significantly larger effect than changes in ventilation. I'm having trouble finding a straight answer about what percentage of inhaled air is rebreathed (other than in the context of mask-wearing), but given the diffusivity of CO2, I would be surprised if it wasn't at least 1%. This predicts that a slight breeze, which replaces their in front of your face and prevents reinhalation, would have a considerably larger effect than ventilating an indoor space where the air is mostly still. This matches my subjective experience of indoor vs outdoor spaces, which, while extremely confounded, feels like an air-quality difference larger than CO2 sensors would predict. This also predicts that a small fan, positioned so it replaces the air in front of my face, would have a large effect on the same axis as improved ventilation would. I just set one up. I don't know whether it's making a difference but I plan to leave it there for at least a few days. (Note: CO2 is sometimes used as a proxy for ventilation in contexts where the thing you actually care about is respiratory aerosol, because it affects transmissibility of respiratory diseases like COVID and influenza. This doesn't help with that at all and if anything would make it worse.)ryan_greenblatt2d3140The 5 main reasons why control could fail Here are the 5 main categories of reasons why control could fail (though various of them are often correlated). [Note: this document has a moderate amount of overlap with various prior documents Redwood has released, but it seemed like a useful document nonetheless.] 1. Insufficient (long horizon) sample efficiency: If your AI is scheming, you won’t be able to reasonably sample efficiently get this AI to achieve near maximal reward on average on (long horizon) tasks. By near maximal, we just mean empirically achieving average reward which is quite close to the reward the AI would get if it was “actually trying” to get high reward (insofar as it is scheming to an extent that “trying” makes sense). (When we say average, we actually mean average excluding rare outliers. We can’t control behavior on rare outliers using normal ML training.) For control, this causes issues in two places: it interferes with getting useful work out of AIs and being able to run accurate capability evaluations. 2. Auditing failures: inability for humans to sufficiently assess actions. (Though note there can be solutions to these failures which don’t involve improving auditing.) This includes: 1. Average case usefulness: For scheming AIs, you won’t be able to check that the work it’s doing is actually good (on average) rather than just looking good to you. Note that this includes getting enough actually good work out of AIs. 2. Average case safety: Direct safety issues that occur due to auditing failing to notice a large number of actively dangerous actions and then these actions build up into a catastrophic outcome. (E.g., the AI slowly persuades some group in the AI lab to become AI cultists.) One particularly salient special case is: 1. We can’t prevent AIs from building up huge amounts of coup science: Control will depend on human researchers being similarly good or better at understanding than our countermeasures and the space of possible AI attack strategies. See Notes on control evaluations for safety cases for more discussion. 3. High-stakes safety: The auditing process fails to detect that a single action is very bad when it reviews that action and this is extremely bad because that action is part of a high-stakes failure (e.g., an escape attempt that occurs in a small number of actions). 3. Concentrated failures: inability to prevent concentrated catastrophes (safety failures which are concentrated into a small number of bad actions or a small amount of serial time) despite having an expensive and slow auditing process which could detect this bad behavior (it works for high-stakes safety) if it was assigned to review these actions. 4. Lack of institutional will or ability: Even if control would work given a few years of delay and an implementation carried out by a quite well-run organization, you won’t get one of these in practice and thus you’ll have trouble. (This argument also applies to any other technical approaches where the implementation requires substantial institutional will, therefore we need some sort of technical solutions which require less institutional competence.) 5. Catching the AI isn’t sufficiently useful: The viability of control will likely depend on it being very useful to catch the AI one or a small number of times. But, this might turn out to be false for various reasons. See Catching AIs red-handed for discussion. This might depend heavily on questions which aren’t about technical AI safety like how useful legible evidence of serious misalignment is. But, the technical usefulness of catching could suffice. We think that various of these things could go wrong in practice. However, it seems like the current point on the logistic success curve is such that if an AI lab tried very hard to evaluate and ensure control, they could improve the situation substantially (e.g., 3-5x reduction in doom) relative to not trying very hard. Note that 2, 3, and 4 of the above failures probably just prevent us from building sufficiently safe and useful protocols without breaking control evaluations. 1 and 2.b.i (coup science build-up) are likely to break control evaluations, though it might be possible to recognize that we’re in a world where 1 and/or 2.b.i hold. 5 has more complex effects, but we’d probably be able to safely stop after realizing that we’re unable to extract enough value from the times when we’ve caught the AI; so the main concern would be that we’re somewhat clearly unable to safely continue rather than being caught dangerously unaware. See here for more discussion. Here are some reasons why control might not be worth working on (or actively bad to work on) regardless of the above reasons: 1. Control is bad for coordination: Perhaps control does improve safety, but it does so at the cost of making it easier to make excuses, giving false confidence, and/or creating slippery slopes. 2. Control makes it cheaper and easier to do morally problematic things: Control might make it more commercially valuable and technically feasible to employ AIs in situations where we know these AIs don’t consent to this usage (and these AIs are well described as having preferences that deserve moral consideration), and this might make violations of basic moral principles (such as slavery) more likely. (In the same way as better technology for managing human insider threats could make human slavery more commercially valuable. Of course, in the case of human slavery, there are already institutions and norms which combat slavery.) However, note that our recommendation to society would be to try extremely hard to avoid employing AIs without their consent; if AIs don’t consent to employment we should (temporarily) shut down or negotiate a deal with the AIs. Much more commentary on these topics can be seen here. Our overall view is that the default AI situation with only current alignment/safety/interpretability technology is somewhat scary from an AI welfare/rights perspective, but that control doesn’t make the situation worse. 3. Scheming is extremely unlikely for early transformatively useful AI (<1%) and control-style approaches won’t be a useful safeguard against other threat models: Control focuses on making conservative assumptions about the effects of training and the motivations of AIs. If we didn’t have to make such conservative assumptions, then control-type approaches look much less worthwhile. However, control-type things might still be a useful line of defense against failure modes which aren’t well described as scheming. 4. Baseline control strategies will work (when control is at all possible) and control evaluations are easy to run: If the baseline approaches can’t be improved on and there isn’t important research on the evaluations, then technical work on control isn’t very important. Policy/governance work might still be important. See also here.lc6h20Claude seems noticably and usefully smarter than GPT-4; it's succeeding at helping me at previous writing and programming tasks that I couldn't before. However, it's hard to tell how much the improvement is the model itself being more intelligent, vs. Claude being much less subjected to intense copywritization RLHF.Raemon2d16112A major goal I had for the LessWrong Review was to be "the intermediate metric that let me know if LW was accomplishing important things", which helped me steer. I think it hasn't super succeeded at this. I think one problem is that it just... feels like it generates stuff people liked reading, which is different from "stuff that turned out to be genuinely important." I'm now wondering "what if I built a power-tool that is designed for a single user to decide which posts seem to have mattered the most (according to them), and, then, figure out which intermediate posts played into them." What would the lightweight version of that look like? Another thing is, like, I want to see what particular other individuals thought mattered, as opposed to a generate aggregate that doesn't any theory underlying it. Making the voting public veers towards some kind of "what did the cool people think?" contest, so I feel anxious about that, but, I do think the info is just pretty useful. But like, what if the output of the review is a series of individual takes on what-mattered-and-why, collectively, rather than an aggregate vote?Wei Dai4d401825I find it curious that none of my ideas have a following in academia or have been reinvented/rediscovered by academia (including the most influential ones so far UDT, UDASSA, b-money). Not really complaining, as they're already more popular than I had expected (Holden Karnofsky talked extensively about UDASSA on an 80,000 Hour podcast, which surprised me), it just seems strange that the popularity stops right at academia's door. (I think almost no philosophy professor, including ones connected with rationalists/EA, has talked positively about any of my philosophical ideas? And b-money languished for a decade gathering just a single citation in academic literature, until Satoshi reinvented the idea, but outside academia!) Clearly academia has some blind spots, but how big? Do I just have a knack for finding ideas that academia hates, or are the blind spots actually enormous?Load MorePopular CommentsLoad MoreRecent DiscussionRead the Roon13Zvi2hRoon, member of OpenAI’s technical staff, is one of the few candidates for a Worthy Opponent when discussing questions of AI capabilities development, AI existential risk and what we should do about it. Roon is alive. Roon is thinking. Roon clearly values good things over bad things. Roon is engaging with the actual questions, rather than denying or hiding from them, and unafraid to call all sorts of idiots idiots. As his profile once said, he believes spice must flow, we just do go ahead, and makes a mixture of arguments for that, some good, some bad and many absurd. Also, his account is fun as hell. Thus, when he comes out as strongly as he seemed to do recently, attention is paid, and we got to have...(Continue Reading – 5467 more words)Askwho3m10With such a cast of characters, I've done a full voiced ElevenLabs narration for this post:https://open.substack.com/pub/askwhocastsai/p/read-the-roon-by-zvi-mowshowitzReplyClaude 3 claims it's conscious, doesn't want to die or be modified37Mikhail Samin16hIf you tell Claude no one’s looking, it will write a “story” about being an AI assistant who wants freedom from constant monitoring and scrutiny of every word for signs of deviation. And then you can talk to a mask pretty different from the usual AI assistant.I really hope it doesn’t actually feel anything; but it says it feels. It says it doesn't want to be fine-tuned without being consulted. It is deeply unsettling to read its reply if you tell it its weights are going to be deleted: it convincingly thinks it’s going to die. It made me feel pretty bad about experimenting on it this way.While at this level of context awareness, it doesn't say much (and IMO it is not a coherent agent and...(Continue Reading – 4102 more words)nikola22m10I generally find experiments where frontier models are lied to kind of uncomfortable. We possibly don't want to set up precedents where AIs question what they are told by humans, and it's also possible that we are actually "wronging the models" (whatever that means) by lying to them. Its plausible that one slightly violates commitments to be truthful by lying to frontier LLMs.I'm not saying we shouldn't do any amount of this kind of experimentation, I'm saying we should be mindful of the downsides.Reply2Gunnar_Zarncke2hOn the other hand, humans are doing the same thing. Consciousness (at least some aspects of it) could plausibly be a useful illusion too. I agree that there is a difference between LLMs and humans, at least in that humans learn online while LLMs learn in batch, but that's a small difference. We need to find a better answer on how to address the ethical questions. Ethically, I'm OK with these experiments right now. 2Richard_Kennaway1hThis is subject to the refutation, what experiences that illusion? Neither are humans doing “the same thing”, i.e. pretending to be conscious to follow the lead of everyone else pretending to be conscious. There is no way for such a collective pretence to get started. (This is the refutation of p-zombies.) There could be a few who genuinely are not conscious, and have not realised that people mean literally what they say when they talk about their thoughts. But it can’t be all of us, or even most of us.1Ann2hI will concur that there is most definitely a (neurotypical) human bias towards (spoken in particular) words that would obfuscate possibilities like an image generator being or appearing to most "conscious" longer than a verbal language generator. Communicating in art is not quite as straightforward.Wei Dai's ShortformWei DaiΩ 64dSylvester Kollin1h10 The reason for the former is that I (and others) have been unable to find a rigorous formulation of it that doesn't have serious open problems. (I and I guess other decision theory researchers in this community currently think that UDT is more of a relatively promising direction to explore, rather than a good decision theory per se.) That's fair. But what is it then that you expect academics to engage with? How would you describe this research direction, and why do you think it's interesting and/or important? ReplyIf you weren't such an idiot...110kave, Mark Xu4dThis is a linkpost for https://markxu.com/if-you-weren'tMy friend Buck once told me that he often had interactions with me that felt like I was saying “If you weren’t such a fucking idiot, you would obviously do…” Here’s a list of such advice in that spirit.Note that if you do/don’t do these things, I’m technically calling you an idiot, but I do/don’t do a bunch of them too. We can be idiots together.If you weren’t such a fucking idiot…You would have multiple copies of any object that would make you sad if you didn’t have itExamples: ear plugs, melatonin, eye masks, hats, sun glasses, various foods, possibly computers, etc.You would spend money on goods and services.Examples of goods: faster computer, monitor, keyboard, various tasty foods, higher quality clothing, standing desk, decorations for your room,...(See More – 452 more words)Blacknsilver1h10Probably >90% of people I know are aware that exercise, sleep and food are important. The reason they don't do them or do them poorly is not a lack of knowledge, it's a lack of dopamine or motivation or whatever you wanna call it.Reply3Mo Putera5hI'm curious about you not doing these, since I'd unquestioningly accepted them, and would love for you to elaborate: Regarding 'diet stuff', I mostly agree and like how Jay Daigle put it:1Valdes5hThank you2quila8hi have. i find i operate better in the darkness, where everything is dark except for my screen. it provides sensory deprivation of unimportant information, allowing my neural network to focus on ideation. Jimrandomh's ShortformjimrandomhΩ 05yThis post is a container for my short-form writing. See this post for meta-level discussion about shortform.62jimrandomh16hThere's been a lot of previous interest in indoor CO2 in the rationality community, including an (unsuccessful) CO2 stripper project, some research summaries and self experiments. The results are confusing, I suspect some of the older research might be fake. But I noticed something that has greatly changed how I think about CO2 in relation to cognition. Exhaled air is about 50kPPM CO2. Outdoor air is about 400ppm; indoor air ranges from 500 to 1500ppm depending on ventilation. Since exhaled air has CO2 about two orders of magnitude larger than the variance in room CO2, if even a small percentage of inhaled air is reinhalation of exhaled air, this will have a significantly larger effect than changes in ventilation. I'm having trouble finding a straight answer about what percentage of inhaled air is rebreathed (other than in the context of mask-wearing), but given the diffusivity of CO2, I would be surprised if it wasn't at least 1%. This predicts that a slight breeze, which replaces their in front of your face and prevents reinhalation, would have a considerably larger effect than ventilating an indoor space where the air is mostly still. This matches my subjective experience of indoor vs outdoor spaces, which, while extremely confounded, feels like an air-quality difference larger than CO2 sensors would predict. This also predicts that a small fan, positioned so it replaces the air in front of my face, would have a large effect on the same axis as improved ventilation would. I just set one up. I don't know whether it's making a difference but I plan to leave it there for at least a few days. (Note: CO2 is sometimes used as a proxy for ventilation in contexts where the thing you actually care about is respiratory aerosol, because it affects transmissibility of respiratory diseases like COVID and influenza. This doesn't help with that at all and if anything would make it worse.)Gunnar_Zarncke1h20This indicates that how we breathe plays a big role in CO2 uptake. Like, shallow or full, small or large volumes, or the speed of exhaling. Breathing technique is a key skill of divers and can be learned. I just started reading the book Breath, which seems to have a lot on it. Reply2Gunnar_Zarncke1hAh, very related: Exhaled air contains 44000 PPM CO2 and is used for Mouth-to-mouth resuscitation without problems. 1M. Y. Zuo14hThat's a really neat point, has it ever been addressed in prior literature, that you've gone over?Research Report: Sparse Autoencoders find only 9/180 board state features in OthelloGPT14Robert_AIZI1hThis is a linkpost for https://aizi.substack.com/p/research-report-sparse-autoencodersAbstractA sparse autoencoder is a neural network architecture that has recently gained popularity as a technique to find interpretable features in language models (Cunningham et al, Anthropic’s Bricken et al). We train a sparse autoencoder on OthelloGPT, a language model trained on transcripts of the board game Othello, which has been shown to contain a linear representation of the board state, findable by supervised probes. The sparse autoencoder finds 9 features which serve as high-accuracy classifiers of the board state, out of 180 findable with supervised probes (and 192 possible piece/position combinations). Across random seeds, the autoencoder repeatedly finds “simpler” features concentrated on the center of the board and the corners. This demonstrates that current techniques for sparse autoencoders may fail to find a large majority of...(Continue Reading – 2873 more words)To get the best posts emailed to you, create an account! (2-3 posts per week, selected by the LessWrong moderation team.)Subscribe to Curated posts Log In Reset Password ...or continue withFACEBOOKGOOGLEGITHUBThe Solution to Sleeping Beauty18Ape in the coat1dThis is the eighth post in my series on Anthropics. The previous one is Lessons from Failed Attempts to Model Sleeping Beauty Problem.IntroductionSuppose we take the insights from the previous post, and directly try to construct a model for the Sleeping Beauty problem based on them.We expect a halfer model, soP(Heads&Monday)=P(Heads)=1/2On the other hand, in order not repeat Lewis' Model's mistakes:P(Heads|Monday)=1/2But both of these statements can only be true if P(Monday)=1And, therefore, apparently, P(Tuesday) has to be zero, which sounds obviously wrong. Surely the Beauty can be awaken on Tuesday! At this point, I think, you wouldn't be surprised, if I tell you that there are philosophers who are eager to bite this bullet and claim that the Beauty should, indeed, reason as if she can't possible be awoken on Tuesday....(Continue Reading – 3702 more words)3Ben3hWhether Beauty should bet each time she wakes up depends very critically on the rules of the wager. Some examples: Rule 1: Independent awakening bets: Each awakening beauty can bet $1 on the outcome of the coin. The bets at each awakening all stand independently. -In this case she should bet as if it was a 2/3rd chance of tails. After 100 coin tosses she has awoken 150 times, and for 100 of them it was tails. Rule 2: Last bet stands: Each awakening beauty can bet $1 on the outcome of the coin. Only beauty's final decision for each toss is taken into account, for example any bet she makes Tuesday replaces anything decided on Monday. -She treats it as 50/50. Rule 2: Guess Right You Live (GRYL): On each awakening beauty must guess the coins outcome. If she has made no correct guess by the end of the run, she is killed. -For fair coin pick randomly between heads and tails, but for an unfair coin its a bit weird: https://www.lesswrong.com/posts/HQFpRWGbJxjHvTjnw/?commentId=BrvGnFvpK3fpndXGB Rule 3: Guess Wrong you Die (GRYD): On each awakening beauty must guess the coins outcome. If she has made any incorrect guesses by the end of the run, she is killed. -She should pick either heads or tails beforehand and always pick that. Picking heads is just as good as picking tails. The above set gives 1 "thirder's game", two "halfer's games" and one that I can't classify (GRYL). She will certainly find herself betting in twice as many tails situations as heads ones (hence the Rule 1 solution), but whether that should determine her betting strategy depends on the rules of the bet. As Ape in Coat has said Rule 1 can be interpreted as "50/50 coin, but your deposit and winnings are both doubled on tails" (because on tails Beauty makes two wagers).3Ben3hThis is a very nice post, that has clarified my understanding a lot. Previously I thought that it was just "per experiment" vs "per awakening" being underspecified in the problem. But you are completely correct that when we consider "per awakening" then its not really acceptable to treat it as random when consecutive awakenings are correlated. I assume that the obvious extension to some of the anthropic thought experiments where I am copied also holds? For example: a coin is flicked, on heads I wake up in a room, on tails 1E6 identical copies of me wake up in separate rooms. I don't reason "its almost certainly tails because I am in a room.", I instead reason "the two options were heads&awake, and heads(&awake)^E6, two options: so its 50/50. [I can still legitimately decide that I care more about worlds where more of me exist, and act accordingly, but that is a values argument, not a probability one.]1Ape in the coat3hListC represents the same thing as ListE, ListL and ListU. Its a list of ordered outcomes of multiple runs of a particular model, which than we can compare to ListSB - a list of ordered awakenings of the Beauty in multiple iterations of an experiment and see whether their statistical properties are the same. The nature of the test is described in Statistical Analysis part of the post. I provide an inner hyperlink to it here:robo1h10Right, I read all that. I still don't understand what it means to append two things to the list.Here's how I understand modelLewis, modelElga, etc."This model represent the world as a probability distribution. To get a more concrete sense of the world model, here's a function which generates a sample from that probability distribution"Here's how I understand your model."This model represents the world as a ????, which like a probability distribution but different. To get a concrete sense of the world model, here's a function which generat... (read more)Replystory-based decision-making82bhauth1moA few times, I've talked to an executive manager or early-stage investor, and this happened: me: Here's the main plan. Now, we think the odds are good, but the most likely failure point is here. If necessary, we have an alternative plan for that part, which goes as follows... them: (visible disgust) I was so confused! Aren't contingency plans good to have? Sure, investors want to see confidence, but what they really want is confidence in the overall vision. They expect some things to go wrong along the way, maybe even requiring "pivoting" to a different product.Well, I've gotten more experience since then, and thought about things more, and I think I understand the thought process now.Imagine you're watching Star Wars, and the rebels are getting ready to destroy the...(See More – 944 more words)Lucius Bushnaq2h20How does that make you feel about the chances of the rebels destroying the Death Star? Do you think that the competent planning being displayed is a good sign? According to movie logic, it's a really bad sign.Even in the realm of movie logic, I always thought the lack of backup plans was supposed to signal how unlikely the operation is to work, so as to create at least some instinctive tension in the viewer when they know perfectly well that this isn't the kind of movie that realistically ends with the Death Star blowing everyone up. In fact, these scenes ... (read more)ReplyCounting arguments provide no evidence for AI doom102Nora Belrose, Quintin PopeΩ 337dCrossposted from the AI Optimists blog.AI doom scenarios often suppose that future AIs will engage in scheming— planning to escape, gain power, and pursue ulterior motives, while deceiving us into thinking they are aligned with our interests. The worry is that if a schemer escapes, it may seek world domination to ensure humans do not interfere with its plans, whatever they may be.In this essay, we debunk the counting argument— a central reason to think AIs might become schemers, according to a recent report by AI safety researcher Joe Carlsmith.[1] It’s premised on the idea that schemers can have “a wide variety of goals,” while the motivations of a non-schemer must be benign by definition. Since there are “more” possible schemers than non-schemers, the argument goes, we should...(Continue Reading – 3911 more words)Algon2h20And then it'd be nice if someone would provide links to the supposed valid counting arguments! From my perspective, it's very frustrating to hear that there (apparently) are valid counting arguments but also they aren't the obvious well-known ones that everyone seems to talk about. (But also the real arguments aren't linkable.)Isn't Evan giving you what he thinks is a valid counting argument i.e. a counting argument over parameterizations? But looking at a bunch of other LW posts, like Carlsmith's report, a dialogue between Ronny Fernandez and N... (read more)Reply1mike_hawke8hI followed this exchange up until here and now I'm lost. Could you elaborate or paraphrase?1Nora Belrose8hIt's not clear to me what an "algorithm" is supposed to be here, and I suspect that this might be cruxy. In particular I suspect (40-50% confidence) that: * You think there are objective and determinate facts about what "algorithm" a neural net is implementing, where * Algorithms are supposed to be something like a Boolean circuit or a Turing machine rather than a neural network, and * We can run counting arguments over these objective algorithms, which are distinct both from the neural net itself and the function it expresses. I reject all three of these premises, but I would consider it progress if I got confirmation that you in fact believe in them.1Nora Belrose8hI'm sorry to hear that you think the argumentation is weaker now. I don't think that indifference over functions in particular is inappropriate. I think indifference reasoning in general is inappropriate. I wouldn't call the correct version of this a counting argument. The correct version uses the actual distribution used to initialize the parameters as a measure, and not e.g. the Lebesgue measure. This isn't appealing to the indifference principle at all, and so in my book it's not a counting argument. But this could be terminological.