DZone: Programming & DevOps news, tutorials & tools Thanks for visiting DZone today, Edit Profile Manage Email Subscriptions How to Post to DZone Article Submission Guidelines Sign Out View Profile Post Post an Article Manage My Drafts Over 2 million developers have joined DZone. Log In / Join Refcards Trend Reports Events Video Library Refcards Trend Reports Events View Events Video Library DZone Spotlight Tuesday, March 5 View All Articles » The Rise of the Platform Engineer: How to Deal With the Increasing Complexity of Software By Mirco Hering Editor's Note: The following is an article written for and published in DZone's 2024 Trend Report, The Modern DevOps Lifecycle: Shifting CI/CD and Application Architectures. DevOps — ✓DevSecOps — ✓Platform engineering — ? Is platform engineering just another term used for a specialization of DevOps, or is it something different? The truth is probably somewhere in the middle. DevOps and its associated DevXOps flavors have a strong cultural spice that puts the individual teams at the center. Unfortunately, in many places, DevOps has led to new problems like tool proliferation and a lack of harmonization across the enterprise. One could say that in response to the very strict silos and strong centralization of the past, DevOps has pushed the pendulum too far toward federation — and, hence, a suboptimization at the team level — to the detriment of the organization. This has been felt most by the larger, more complex enterprises that have to deal with different technology stacks and differing levels of maturity across the organization. Platform engineering has evolved as a response to this enterprise-wide challenge. Platform engineering is not a replacement for DevOps. Instead, platform engineering complements DevOps to address enterprise-wide challenges and provide a tooling platform that makes it easier for individual teams to do the right thing rather than break things while trying to maintain consistency across the organization. IT delivery has increased in complexity over the last few years, given that more applications are moving at a faster pace. This means organizations cannot rely on individuals to control the complexity; they require systemic answers supported by the proper tooling. This is the problem statement that platform engineering has the ambition to address. With this, platform engineers have become crucial for organizations, as their role holds the keys to enabling security and engineering standards. What Is a Platform Engineer? The role of the platform engineer has three different parts. Figure 1. Role of the platform engineer The most obvious one is the role of a technical architect as they have to build an engineering platform that connects all tools and enables processes. The second aspect is a community enabler, which is similar to developer relations roles at technical tooling companies. The third part is a product manager; the competing interests and demands from the developer community need to be prioritized against the technical needs of the platform (consider things like security hardening and patching of outdated components). Platform Engineer as Technical Architect In organizations with moderate or high complexity within their technology stack, the number of tools required to build, release, and maintain software is at least a dozen, sometimes more. Integrating these tools and enabling the measurement of meaningful metrics is about as tricky as integrating business applications. After all, the challenges are very similar: Different processes need to be aligned, data models need to be transformed to make them usable, and integration points need to be connected to enable the end-to-end process. The systems that run the software side of the business have become similarly challenging. The role of the platform engineer here is to look after the architecture of the tools that run the software side — the goal being to make the tools "disappear" and make the build and release of software appear easy. Platform Engineer as Community Enabler Software engineers tend to think their solutions are better than those from someone else. As such, the adoption of engineering platforms is a challenge to overcome. Telling engineers to use a specific tool has often been met with resistance. The platform engineer must be a community enabler who works with the engineers to promote the platform and convince them of the benefits. Communication goes both ways in this part of the role as the platform engineer also must listen to the problems and challenges of the platform and identify new features that are high in demand. This leads to the third part of the role. Platform Engineer as Product Manager Competing demands on the platform come from the engineers of an organization and other stakeholders like security and, of course, the platform engineers. Prioritizing these demands in a meaningful way is a difficult task as you have to find a balance between all the competing interests, especially as funding for the platform is often a challenge in itself, so speed to value is critical for the ongoing support of the platform. The platform engineer requires good negotiation skills to navigate these challenges. Overview of Platform Engineering Architecture We spoke about the role of the platform engineer, but what is in that platform that the platform engineer is building and maintaining? It is easiest to think about three layers and one target environment: The top layer is the developer experience. These are the tools the developer directly engages with — tools that drive the overall workflow, like an Agile lifecycle management tool, a service management tool, and the developer IDE, fit into this. The bottom layer comprises the infrastructure components that must be combined to build application environments. This can be from the public or private cloud and includes traditional data center technologies. In the middle is where most of the complexity sits — the software engineering platform. Here, all the processes that are required to create and deliver software are being orchestrated: CI/CD, security scanning, environment provisioning, and release management. Figure 2. Platform structure Making the Switch: How to Adopt Platform Engineering Across DevOps Teams So where should you start? One successful adoption pattern focuses on identifying developer journeys to define a minimum viable platform. Which capabilities are required to enable a developer journey to achieve an outcome? Think of a task like provisioning an environment, deploying a new API to production, or running a performance test suite. Each is a valid developer journey with multiple touchpoints that potentially require numerous tools. Once you have created the minimum viable platform for the first set of applications or technologies, adoption follows three dimensions: More applications (once the required capabilities are available), more capabilities, and more maturity, thus increasing the levels of automation and/or performance. Besides worrying about building out the platform with a reasonable approach, three other aspects should be addressed early on: Community engagement Funding Measuring outcomes from the platform Defining a community engagement strategy can be very helpful. This strategy should contain how the information will be shared with the developer community, how feature requests can be made, and how the platform's benefits will be communicated. Defining the forums, the communications, and their respective frequency is also helpful. Funding can quickly become a bottleneck, so a funding strategy should be agreed upon early in the platform engineer adoption. This can be one of several strategies, such as dedicated funding, funding for the services provided, or a service tax on all software development. Each has its own benefits and challenges, a discussion of which is beyond the scope of this article. What is essential is to have a sustainable long-term funding strategy that does not depend on stakeholders' goodwill. Last but not least, the platform engineer needs to be able to show results, which means we need to measure meaningful metrics that showcase why the company is better off with the platform in place. This is often forgotten or an afterthought. Understanding the organization's priorities and aligning the measurement framework to it can help achieve ongoing support. Unfortunately, this usually requires data alignment across multiple tools and is easiest to accomplish when thought about upfront — it becomes increasingly difficult the longer the data models of individual tools remain isolated. Conclusion Platform engineering is still pretty new, yet there is already a lot of content on it, which shows how quickly it has gained interest from organizations. There is even a dedicated conference for it, which began in 2022 and has thousands of participants. It's the early days, but current indications show that platform engineering has quickly found market adoption and a passionate community. And while this is happening, the role of the platform engineer will steadily increase in importance, which is already showing up in salaries too. Hopefully, platform engineering will continue to help organizations reduce complexity for their developers while delivering on the DevOps promise: to provide better solutions faster and more securely. This is an excerpt from DZone's 2024 Trend Report, The Modern DevOps Lifecycle: Shifting CI/CD and Application Architectures.For more: Read the Report More The Enterprise Journey to Cloud Adoption By Harshavardhan Nerella "Migrate" comes from the Latin "migratio," meaning to move from one place to another. In information technology, migration entails understanding new systems' benefits, identifying current system shortfalls, planning, and transferring selected applications. Not all IT assets must be moved; migration can mean moving a part of them. This article will delve into the details of transferring IT assets to public clouds like AWS, Azure, or GCP. Many factors can influence the decision to switch to the cloud, such as expiring data center leases, the high costs of data center management, outdated hardware, software license renewals, geographical compliance needs, market growth, and the need to adjust resources to match demand quickly. Executive backing is crucial for a company to begin its cloud migration journey. This support is the cornerstone for any large-scale migration success. Leadership must unify their teams for the journey, as collaboration is essential. Attempts by isolated teams can lead to problems. Regular leadership meetings, whether weekly or bi-weekly, can overcome hurdles and keep the migration process on track. A pivotal step in achieving successful cloud migration is the formation of a Cloud Steering Committee. This team unites technical experts to forge the initial cloud adoption patterns. The optimal team structure includes infrastructure, security, application, and operations engineers alongside a lead architect, all steered by a Cloud Steering Committee leader. Together, they establish objectives for security, availability, reliability, scalability, data cleansing, and compliance. Embarking on the cloud migration journey can seem daunting when confronted with the vast array of applications and servers awaiting transition. There's no universal solution for migration; each enterprise faces unique challenges and opportunities. However, many organizations have successfully navigated their way to the cloud by exploring established frameworks and diverse migration strategies. Framework Evaluate and Discover Enterprises must establish a strong business case by aligning their objectives with an understanding of their current applications' age, architecture, and limitations. Leadership engagement, clear communication, and a defined purpose are essential to unify the organization and set feasible goals and timelines for the migration. In addition, a comprehensive portfolio analysis is critical, including discovering the applications, mapping interdependencies, and formulating migration strategies and priorities. This stage determines the complexity and business impact of applications, guiding the migration sequence. Starting with non-critical, simpler applications helps the team to understand the new platform and understand the gaps. Design and Migrate After classifying the applications identified in the discovery phase—whether they are web, mobile, or database systems—a standard blueprint must be created for each type. Each application requires careful design, migration, and validation following one of six migration strategies, which will be discussed later in this paper. An ethos of continuous improvement is advised, involving regular assessments of the blueprint to identify and rectify gaps. Modernize Migration isn't just about moving applications; it's about optimizing them. This means decommissioning outdated systems and steadily refining the operational model. Consider this as an evolving synergy among people, processes, and technology that improves incrementally throughout the migration journey. Migration Strategies Retain Some applications or segments of IT assets remain on-premises because they aren't suited for the cloud, don't deliver business value, or are not prepared for cloud migration. This could be due to dependencies on on-premises systems or data sovereignty issues. Retire Applications that no longer yield value to the business can be phased out. By retiring these systems, resources can be reallocated to more impactful business areas. Rehost Commonly known as "lift and shift," this strategy is popular among enterprises for its ability to facilitate a swift migration to the cloud. It requires minimal alterations to the code, database, or architecture, allowing for a more straightforward transition. Replatform Often termed "lift-tinker-and-shift," this process involves minor optimizations to applications for cloud efficiency, such as software updates, configuration tweaks, or the use of cloud-native services like Kubernetes-as-a-Service or Database-as-a-Service. An example includes transitioning from traditional database services to a cloud-based option like Amazon RDS. Repurchase This strategy comes into play when an application isn't suitable for cloud deployment or existing licensing agreements don't support a Bring-Your-Own-License (BYOL) model in the cloud. It involves switching to a Software-as-a-Service (SaaS) platform or working with Independent Software Vendors (ISVs). For example, replacing an on-premises customer relationship management (CRM) system with Salesforce. Refactor/Re-Architect This more intensive approach requires redesigning and rebuilding the application from scratch to fully exploit cloud-native features, significantly enhancing agility, scalability, and performance. Though it's the most costly and time-intensive strategy, it positions businesses to seamlessly integrate future cloud innovations with minimal effort. A typical scenario is transforming an application from a monolithic architecture to microservices. Conclusion While there is no one-size-fits-all solution for cloud migration, enterprises can benefit from analyzing successful migration journeys. Organizations can optimize their approach by emulating effective strategies and adapting them to their unique requirements. Taking the time to understand the new platform, learning from past missteps thoroughly, and refining processes are key steps toward meaningful outcomes. Moreover, it's strategic to prioritize migration projects based on business needs, considering factors such as complexity, revenue impact, operational criticality, and the timing of hardware upgrades. Additionally, investing in training for staff to master new tools and technologies is essential for a smooth transition to the cloud. More Trend Report The Modern DevOps Lifecycle While DevOps is here to stay, as the years pass, we must continuously assess and seek improvements to our existing software processes, systems, and culture — and DevOps is no exception to that rule. With business needs and customer demands constantly shifting, so must our technology, mindsets, and architecture in order to keep pace.Now is the time for this movement that's all about "shifting left" to essentially shift.In our annual DevOps Trend Report, we explore both its fundamental principles as well as the emerging topics, methodologies, and challenges surrounding the engineering ecosystem. Within our "Key Research Findings" and featured articles from our expert community members, readers will find information on core DevOps topics as well as new insights on what's next for DevOps in 2024 and beyond. Join us to learn about the state of CI/CD pipelines, the impact of technical debt, patterns for supply chain management<>DevOps, the rise of platform engineering, and even more! Download Refcard #393 Getting Started With Large Language Models By Tuhin Chattopadhyay CORE Download Refcard #392 Software Supply Chain Security By Justin Albano CORE Download More Articles Semantic Search With Weaviate Vector Database In a previous blog, the influence of the document format and the way it is embedded in combination with semantic search was discussed. LangChain4j was used to accomplish this. The way the document was embedded has a major influence on the results. This was one of the main conclusions. However, a perfect result was not achieved. In this post, you will take a look at Weaviate, a vector database that has a Java client library available. You will investigate whether better results can be achieved. The source documents are two Wikipedia documents. You will use the discography and list of songs recorded by Bruce Springsteen. The interesting part of these documents is that they contain facts and are mainly in a table format. Parts of these documents are converted to Markdown in order to have a better representation. The same documents were used in the previous blog, so it will be interesting to see how the findings from that post compare to the approach used in this post. The sources used in this blog can be found on GitHub. Prerequisites The prerequisites for this blog are: Basic knowledge of embedding and vector stores Basic Java knowledge, Java 21 is used Basic knowledge of Docker The Weaviate starter guides are also interesting reading material. How to Implement Vector Similarity Search 1. Installing Weaviate There are several ways to install Weaviate. An easy installation is through Docker Compose. Just use the sample Docker Compose file. YAML version: '3.4' services: weaviate: command: - --host - 0.0.0.0 - --port - '8080' - --scheme - http image: semitechnologies/weaviate:1.23.2 ports: - 8080:8080 - 50051:50051 volumes: - weaviate_data:/var/lib/weaviate restart: on-failure:0 environment: QUERY_DEFAULTS_LIMIT: 25 AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true' PERSISTENCE_DATA_PATH: '/var/lib/weaviate' DEFAULT_VECTORIZER_MODULE: 'none' ENABLE_MODULES: 'text2vec-cohere,text2vec-huggingface,text2vec-palm,text2vec-openai,generative-openai,generative-cohere,generative-palm,ref2vec-centroid,reranker-cohere,qna-openai' CLUSTER_HOSTNAME: 'node1' volumes: weaviate_data: Start the Compose file from the root of the repository. Shell $ docker compose -f docker/compose-initial.yaml up You can shut it down with CTRL+C or with the following command: Shell $ docker compose -f docker/compose-initial.yaml down 2. Connect to Weaviate First, let’s try to connect to Weaviate through the Java library. Add the following dependency to the pom file: XML <dependency> <groupId>io.weaviate</groupId> <artifactId>client</artifactId> <version>4.5.1</version> </dependency> The following code will create a connection to Weaviate and display some metadata information about the instance. Java Config config = new Config("http", "localhost:8080"); WeaviateClient client = new WeaviateClient(config); Result<Meta> meta = client.misc().metaGetter().run(); if (meta.getError() == null) { System.out.printf("meta.hostname: %s\n", meta.getResult().getHostname()); System.out.printf("meta.version: %s\n", meta.getResult().getVersion()); System.out.printf("meta.modules: %s\n", meta.getResult().getModules()); } else { System.out.printf("Error: %s\n", meta.getError().getMessages()); } The output is the following: Shell meta.hostname: http://[::]:8080 meta.version: 1.23.2 meta.modules: {generative-cohere={documentationHref=https://docs.cohere.com/reference/generate, name=Generative Search - Cohere}, generative-openai={documentationHref=https://platform.openai.com/docs/api-reference/completions, name=Generative Search - OpenAI}, generative-palm={documentationHref=https://cloud.google.com/vertex-ai/docs/generative-ai/chat/test-chat-prompts, name=Generative Search - Google PaLM}, qna-openai={documentationHref=https://platform.openai.com/docs/api-reference/completions, name=OpenAI Question & Answering Module}, ref2vec-centroid={}, reranker-cohere={documentationHref=https://txt.cohere.com/rerank/, name=Reranker - Cohere}, text2vec-cohere={documentationHref=https://docs.cohere.ai/embedding-wiki/, name=Cohere Module}, text2vec-huggingface={documentationHref=https://huggingface.co/docs/api-inference/detailed_parameters#feature-extraction-task, name=Hugging Face Module}, text2vec-openai={documentationHref=https://platform.openai.com/docs/guides/embeddings/what-are-embeddings, name=OpenAI Module}, text2vec-palm={documentationHref=https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings, name=Google PaLM Module} The version is shown and the modules that were activated, this corresponds to the modules activated in the docker compose file. 3. Embed Documents In order to query the documents, the documents need to be embedded first. This can be done by means of the text2vec-transformers module. Create a new Docker Compose file with only the text2vec-transformers module enabled. You also set this module as DEFAULT_VECTORIZER_MODULE, set the TRANSFORMERS_INFERENCE_API to the transformer container and you use the sentence-transformers-all-MiniLM-L6-v2-onnx image for the transformer container. You use the ONNX image when you do not make use of a GPU. YAML version: '3.4' services: weaviate: command: - --host - 0.0.0.0 - --port - '8080' - --scheme - http image: semitechnologies/weaviate:1.23.2 ports: - 8080:8080 - 50051:50051 volumes: - weaviate_data:/var/lib/weaviate restart: on-failure:0 environment: QUERY_DEFAULTS_LIMIT: 25 AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true' PERSISTENCE_DATA_PATH: '/var/lib/weaviate' DEFAULT_VECTORIZER_MODULE: 'text2vec-transformers' ENABLE_MODULES: 'text2vec-transformers' TRANSFORMERS_INFERENCE_API: http://t2v-transformers:8080 CLUSTER_HOSTNAME: 'node1' t2v-transformers: image: semitechnologies/transformers-inference:sentence-transformers-all-MiniLM-L6-v2-onnx volumes: weaviate_data: Start the containers: Shell $ docker compose -f docker/compose-embed.yaml up Embedding the data is an important step that needs to be executed thoroughly. It is therefore important to know the Weaviate concepts. Every data object belongs to a Class, and a class has one or more Properties. A Class can be seen as a collection and every data object (represented as JSON-documents) can be represented by a vector (i.e. an embedding). Every Class contains objects which belong to this class, which corresponds to a common schema. Three markdown files with data of Bruce Springsteen are available. The embedding will be done as follows: Every markdown file will be converted to a Weaviate Class. A markdown file consists out of a header. The header contains the column names, which will be converted into Weaviate Properties. Properties need to be valid GraphQL names. Therefore, the column names have been altered a bit compared to the previous blog. E.g. writer(s) has become writers, album details has become AlbumDetails, etc. After the header, the data is present. Ever row in the table will be converted to a data object belonging to a Class. An example of a markdown file is the Compilation Albums file. Markdown | Title | US | AUS | CAN | GER | IRE | NLD | NZ | NOR | SWE | UK | |----------------------------------|----|-----|-----|-----|-----|-----|----|-----|-----|----| | Greatest Hits | 1 | 1 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 1 | | Tracks | 27 | 97 | — | 63 | — | 36 | — | 4 | 11 | 50 | | 18 Tracks | 64 | 98 | 58 | 8 | 20 | 69 | — | 2 | 1 | 23 | | The Essential Bruce Springsteen | 14 | 41 | — | — | 5 | 22 | — | 4 | 2 | 15 | | Greatest Hits | 43 | 17 | 21 | 25 | 2 | 4 | 3 | 3 | 1 | 3 | | The Promise | 16 | 22 | 27 | 1 | 4 | 4 | 30 | 1 | 1 | 7 | | Collection: 1973–2012 | — | 6 | — | 23 | 2 | 78 | 19 | 1 | 6 | — | | Chapter and Verse | 5 | 2 | 21 | 4 | 2 | 5 | 4 | 3 | 2 | 2 | In the next sections, the steps taken to embed the documents are explained in more detail. The complete source code is available at GitHub. This is not the most clean code, but I do hope it is understandable. 3.1 Basic Setup A map is created, which contains the file names linked to the Weaviate Class names to be used. Java private static Map<String, String> documentNames = Map.of( "bruce_springsteen_list_of_songs_recorded.md", "Songs", "bruce_springsteen_discography_compilation_albums.md", "CompilationAlbums", "bruce_springsteen_discography_studio_albums.md", "StudioAlbums"); In the basic setup, a connection is set up to Weaviate, all data is removed from the database, and the files are read. Every file is then processed one by one. Java Config config = new Config("http", "localhost:8080"); WeaviateClient client = new WeaviateClient(config); // Remove existing data Result<Boolean> deleteResult = client.schema().allDeleter().run(); if (deleteResult.hasErrors()) { System.out.println(new GsonBuilder().setPrettyPrinting().create().toJson(deleteResult.getResult())); } List<Document> documents = loadDocuments(toPath("markdown-files")); for (Document document : documents) { ... } 3.2 Convert Header to Class The header information needs to be converted to a Weaviate Class. Split the complete file row by row. The first line contains the header, split it by means of the | separator and store it in variable tempSplittedHeader. The header starts with a | and therefore the first entry in tempSplittedHeader is empty. Remove it and store the remaining part of the row in variable splittedHeader. For every item in splittedHeader (i.e. the column names), a Weaviate Property is created. Strip all leading and trailing spaces from the data. Create the Weaviate documentClass with the class name as defined in the documentNames map and the just created Properties. Add the class to the schema and verify the result. Java // Split the document line by line String[] splittedDocument = document.text().split("\n"); // split the header on | and remove the first item (the line starts with | and the first item is therefore empty) String[] tempSplittedHeader = splittedDocument[0].split("\\|"); String[] splittedHeader = Arrays.copyOfRange(tempSplittedHeader,1, tempSplittedHeader.length); // Create the Weaviate collection, every item in the header is a Property ArrayList<Property> properties = new ArrayList<>(); for (String splittedHeaderItem : splittedHeader) { Property property = Property.builder().name(splittedHeaderItem.strip()).build(); properties.add(property); } WeaviateClass documentClass = WeaviateClass.builder() .className(documentNames.get(document.metadata("file_name"))) .properties(properties) .build(); // Add the class to the schema Result<Boolean> collectionResult = client.schema().classCreator() .withClass(documentClass) .run(); if (collectionResult.hasErrors()) { System.out.println("Creation of collection failed: " + documentNames.get(document.metadata("file_name"))); } 3.3 Convert Data Rows to Objects Every data row needs to be converted to a Weaviate data object. Copy the rows containing data in variable dataOnly. Loop over every row, a row is represented by variable documentLine. Split every line by means of the | separator and store it in variable tempSplittedDocumentLine. Just like the header, every row starts with a |, and therefore, the first entry in tempSplittedDocumentLine is empty. Remove it and store the remaining part of the row in variable splittedDocumentLine. Every item in the row becomes a property. The complete row is converted to properties in variable propertiesDocumentLine. Strip all leading and trailing spaces from the data. Add the data object to the Class and verify the result. At the end, print the result. Java // Preserve only the rows containing data, the first two rows contain the header String[] dataOnly = Arrays.copyOfRange(splittedDocument, 2, splittedDocument.length); for (String documentLine : dataOnly) { // split a data row on | and remove the first item (the line starts with | and the first item is therefore empty) String[] tempSplittedDocumentLine = documentLine.split("\\|"); String[] splittedDocumentLine = Arrays.copyOfRange(tempSplittedDocumentLine, 1, tempSplittedDocumentLine.length); // Every item becomes a property HashMap<String, Object> propertiesDocumentLine = new HashMap<>(); int i = 0; for (Property property : properties) { propertiesDocumentLine.put(property.getName(), splittedDocumentLine[i].strip()); i++; } Result<WeaviateObject> objectResult = client.data().creator() .withClassName(documentNames.get(document.metadata("file_name"))) .withProperties(propertiesDocumentLine) .run(); if (objectResult.hasErrors()) { System.out.println("Creation of object failed: " + propertiesDocumentLine); } String json = new GsonBuilder().setPrettyPrinting().create().toJson(objectResult.getResult()); System.out.println(json); } 3.4 The Result Running the code to embed the documents prints what is stored in the Weaviate vector database. As you can see below, a data object has a UUID, the class is StudioAlbums, the properties are listed and the corresponding vector is displayed. Shell { "id": "e0d5e1a3-61ad-401d-a264-f95a9a901d82", "class": "StudioAlbums", "creationTimeUnix": 1705842658470, "lastUpdateTimeUnix": 1705842658470, "properties": { "aUS": "3", "cAN": "8", "gER": "1", "iRE": "2", "nLD": "1", "nOR": "1", "nZ": "4", "sWE": "1", "title": "Only the Strong Survive", "uK": "2", "uS": "8" }, "vector": [ -0.033715352, -0.07489116, -0.015459526, -0.025204511, ... 0.03576842, -0.010400549, -0.075309984, -0.046005197, 0.09666792, 0.0051724687, -0.015554721, 0.041699238, -0.09749843, 0.052182134, -0.0023900834 ] } 4. Manage Collections So, now you have data in the vector database. What kind of information can be retrieved from the database? You are able to manage the collection, for example. 4.1 Retrieve Collection Definition The definition of a collection can be retrieved as follows: Java String className = "CompilationAlbums"; Result<WeaviateClass> result = client.schema().classGetter() .withClassName(className) .run(); String json = new GsonBuilder().setPrettyPrinting().create().toJson(result.getResult()); System.out.println(json); The output is the following: Shell { "class": "CompilationAlbums", "description": "This property was generated by Weaviate\u0027s auto-schema feature on Sun Jan 21 13:10:58 2024", "invertedIndexConfig": { "bm25": { "k1": 1.2, "b": 0.75 }, "stopwords": { "preset": "en" }, "cleanupIntervalSeconds": 60 }, "moduleConfig": { "text2vec-transformers": { "poolingStrategy": "masked_mean", "vectorizeClassName": true } }, "properties": [ { "name": "uS", "dataType": [ "text" ], "description": "This property was generated by Weaviate\u0027s auto-schema feature on Sun Jan 21 13:10:58 2024", "tokenization": "word", "indexFilterable": true, "indexSearchable": true, "moduleConfig": { "text2vec-transformers": { "skip": false, "vectorizePropertyName": false } } }, ... } You can see how it was vectorized, the properties, etc. 4.2 Retrieve Collection Objects Can you also retrieve the collection objects? Yes, you can, but this is not possible at the moment of writing with the java client library. You will notice, when browsing the Weaviate documentation, that there is no example code for the java client library. However, you can make use of the GraphQL API which can also be called from java code. The code to retrieve the title property of every data object in the CompilationAlbums Class is the following: You call the graphQL method from the Weaviate client. You define the Weaviate Class and the fields you want to retrieve. You print the result. Java Field song = Field.builder().name("title").build(); Result<GraphQLResponse> result = client.graphQL().get() .withClassName("CompilationAlbums") .withFields(song) .run(); if (result.hasErrors()) { System.out.println(result.getError()); return; } System.out.println(result.getResult()); The result shows you all the titles: Shell GraphQLResponse( data={ Get={ CompilationAlbums=[ {title=Chapter and Verse}, {title=The Promise}, {title=Greatest Hits}, {title=Tracks}, {title=18 Tracks}, {title=The Essential Bruce Springsteen}, {title=Collection: 1973–2012}, {title=Greatest Hits} ] } }, errors=null) 5. Semantic Search The whole purpose of embedding the documents is to verify whether you can search the documents. In order to search, you also need to make use of the GraphQL API. Different search operators are available. Just like in the previous blog, 5 questions are asked about the data. on which album was “adam raised a cain” originally released?The answer is “Darkness on the Edge of Town”. what is the highest chart position of “Greetings from Asbury Park, N.J.” in the US?This answer is #60. what is the highest chart position of the album “tracks” in canada?The album did not have a chart position in Canada. in which year was “Highway Patrolman” released?The answer is 1982. who produced “all or nothin’ at all”?The answer is Jon Landau, Chuck Plotkin, Bruce Springsteen and Roy Bittan. In the source code, you provide the class name and the corresponding fields. This information is added in a static class for each collection. The code contains the following: Create a connection to Weaviate. Add the fields of the class and also add two additional fields, the certainty and the distance. Embed the question using a NearTextArgument. Search the collection via the GraphQL API, limit the result to 1. Print the result. Java private static void askQuestion(String className, Field[] fields, String question) { Config config = new Config("http", "localhost:8080"); WeaviateClient client = new WeaviateClient(config); Field additional = Field.builder() .name("_additional") .fields(Field.builder().name("certainty").build(), // only supported if distance==cosine Field.builder().name("distance").build() // always supported ).build(); Field[] allFields = Arrays.copyOf(fields, fields.length + 1); allFields[fields.length] = additional; // Embed the question NearTextArgument nearText = NearTextArgument.builder() .concepts(new String[]{question}) .build(); Result<GraphQLResponse> result = client.graphQL().get() .withClassName(className) .withFields(allFields) .withNearText(nearText) .withLimit(1) .run(); if (result.hasErrors()) { System.out.println(result.getError()); return; } System.out.println(result.getResult()); } Invoke this method for the five questions. Java askQuestion(Song.NAME, Song.getFields(), "on which album was \"adam raised a cain\" originally released?"); askQuestion(StudioAlbum.NAME, StudioAlbum.getFields(), "what is the highest chart position of \"Greetings from Asbury Park, N.J.\" in the US?"); askQuestion(CompilationAlbum.NAME, CompilationAlbum.getFields(), "what is the highest chart position of the album \"tracks\" in canada?"); askQuestion(Song.NAME, Song.getFields(), "in which year was \"Highway Patrolman\" released?"); askQuestion(Song.NAME, Song.getFields(), "who produced \"all or nothin' at all?\""); The result is amazing, for all five questions the correct data object is returned. Shell GraphQLResponse( data={ Get={ Songs=[ {_additional={certainty=0.7534831166267395, distance=0.49303377}, originalRelease=Darkness on the Edge of Town, producers=Jon Landau Bruce Springsteen Steven Van Zandt (assistant), song="Adam Raised a Cain", writers=Bruce Springsteen, year=1978} ] } }, errors=null) GraphQLResponse( data={ Get={ StudioAlbums=[ {_additional={certainty=0.803815484046936, distance=0.39236903}, aUS=71, cAN=—, gER=—, iRE=—, nLD=—, nOR=—, nZ=—, sWE=35, title=Greetings from Asbury Park,N.J., uK=41, uS=60} ] } }, errors=null) GraphQLResponse( data={ Get={ CompilationAlbums=[ {_additional={certainty=0.7434340119361877, distance=0.513132}, aUS=97, cAN=—, gER=63, iRE=—, nLD=36, nOR=4, nZ=—, sWE=11, title=Tracks, uK=50, uS=27} ] } }, errors=null) GraphQLResponse( data={ Get={ Songs=[ {_additional={certainty=0.743279218673706, distance=0.51344156}, originalRelease=Nebraska, producers=Bruce Springsteen, song="Highway Patrolman", writers=Bruce Springsteen, year=1982} ] } }, errors=null) GraphQLResponse( data={ Get={ Songs=[ {_additional={certainty=0.7136414051055908, distance=0.5727172}, originalRelease=Human Touch, producers=Jon Landau Chuck Plotkin Bruce Springsteen Roy Bittan, song="All or Nothin' at All", writers=Bruce Springsteen, year=1992} ] } }, errors=null) 6. Explore Collections The semantic search implementation assumed that you knew in which collection to search the answer. Most of the time, you do not know which collection to search for. The explore function can help in order to search across multiple collections. There are some limitations to the use of the explore function: Only one vectorizer module may be enabled. The vector search must be nearText or nearVector. The askQuestion method becomes the following. Just like in the previous paragraph, you want to return some additional, more generic fields of the collection. The question is embedded in a NearTextArgument and the collections are explored. Java private static void askQuestion(String question) { Config config = new Config("http", "localhost:8080"); WeaviateClient client = new WeaviateClient(config); ExploreFields[] fields = new ExploreFields[]{ ExploreFields.CERTAINTY, // only supported if distance==cosine ExploreFields.DISTANCE, // always supported ExploreFields.BEACON, ExploreFields.CLASS_NAME }; NearTextArgument nearText = NearTextArgument.builder().concepts(new String[]{question}).build(); Result<GraphQLResponse> result = client.graphQL().explore() .withFields(fields) .withNearText(nearText) .run(); if (result.hasErrors()) { System.out.println(result.getError()); return; } System.out.println(result.getResult()); } Running this code returns an error. A bug is reported, because a vague error is returned. Shell GraphQLResponse(data={Explore=null}, errors=[GraphQLError(message=runtime error: invalid memory address or nil pointer dereference, path=[Explore], locations=[GraphQLErrorLocationsItems(column=2, line=1)])]) GraphQLResponse(data={Explore=null}, errors=[GraphQLError(message=runtime error: invalid memory address or nil pointer dereference, path=[Explore], locations=[GraphQLErrorLocationsItems(column=2, line=1)])]) GraphQLResponse(data={Explore=null}, errors=[GraphQLError(message=runtime error: invalid memory address or nil pointer dereference, path=[Explore], locations=[GraphQLErrorLocationsItems(column=2, line=1)])]) GraphQLResponse(data={Explore=null}, errors=[GraphQLError(message=runtime error: invalid memory address or nil pointer dereference, path=[Explore], locations=[GraphQLErrorLocationsItems(column=2, line=1)])]) GraphQLResponse(data={Explore=null}, errors=[GraphQLError(message=runtime error: invalid memory address or nil pointer dereference, path=[Explore], locations=[GraphQLErrorLocationsItems(column=2, line=1)])]) However, in order to circumvent this error, it would be interesting to verify whether the correct answer returns the highest certainty over all collections. Therefore, for each question every collection is queried. The complete code can be found here, below only the code for question 1 is shown. The askQuestion implementation is the one used in the Semantic Search paragraph. Java private static void question1() { askQuestion(Song.NAME, Song.getFields(), "on which album was \"adam raised a cain\" originally released?"); askQuestion(StudioAlbum.NAME, StudioAlbum.getFields(), "on which album was \"adam raised a cain\" originally released?"); askQuestion(CompilationAlbum.NAME, CompilationAlbum.getFields(), "on which album was \"adam raised a cain\" originally released?"); } Running this code returns the following output. Shell GraphQLResponse(data={Get={Songs=[{_additional={certainty=0.7534831166267395, distance=0.49303377}, originalRelease=Darkness on the Edge of Town, producers=Jon Landau Bruce Springsteen Steven Van Zandt (assistant), song="Adam Raised a Cain", writers=Bruce Springsteen, year=1978}]}, errors=null) GraphQLResponse(data={Get={StudioAlbums=[{_additional={certainty=0.657206118106842, distance=0.68558776}, aUS=9, cAN=7, gER=—, iRE=73, nLD=4, nOR=12, nZ=11, sWE=9, title=Darkness on the Edge of Town, uK=14, uS=5}]}, errors=null) GraphQLResponse(data={Get={CompilationAlbums=[{_additional={certainty=0.6488107144832611, distance=0.7023786}, aUS=97, cAN=—, gER=63, iRE=—, nLD=36, nOR=4, nZ=—, sWE=11, title=Tracks, uK=50, uS=27}]}, errors=null) The interesting parts here are the certainties: Collection Songs has a certainty of 0.75 Collection StudioAlbums has a certainty of 0.62 Collection CompilationAlbums has a certainty of 0.64 The correct answer can be found in the collection of songs that has the highest certainty. So, this is great. When you verify this for the other questions, you will see that the collection containing the correct answer, always has the highest certainty. Conclusion In this post, you transformed the source documents in order to fit in a vector database. The semantic search results are amazing. In the previous posts, it was kind of a struggle to retrieve the correct answers to the questions. By restructuring the data and by only using a vector semantic search a 100% score of correct answers has been achieved. By Gunter Rotsaert CORE JS Toolbox 2024: Runtime Environments and Package Management Series Introduction Staying ahead of the curve in JavaScript development requires embracing the ever-evolving landscape of tools and technologies. As we navigate through 2024, the landscape of JavaScript development tools will continue to transform, offering more refined, efficient, and user-friendly options. This "JS Toolbox 2024" series is your one-stop shop for a comprehensive overview of the latest and most impactful tools in the JavaScript ecosystem. Across the series, we'll delve into various categories of tools, including runtime environments, package managers, frameworks, static site generators, bundlers, and test frameworks. It will empower you to wield these tools effectively by providing a deep dive into their functionalities, strengths, weaknesses, and how they fit into the modern JavaScript development process. Whether you're a seasoned developer or just starting, this series will equip you with the knowledge needed when it comes to selecting the right tools for your projects in 2024. The series consists of 3 parts: Runtime Environments and Package Management (this article): In this first installment, we explore the intricacies of runtime environments, focusing on Node and Bun. You'll gain insights into their histories, performance metrics, community support, and ease of use, supported by relevant case studies.The segment on package management tools compares npm, yarn, and pnpm, highlighting their performance and security features. We provide tips for choosing the most suitable package manager for your project. Frameworks and Static Site Generators: This post provides a thorough comparison of popular frameworks like React, Vue, Angular, Svelte, and HTMX, focusing on their unique features and suitability for different project types.The exploration of static site generators covers Astro, Nuxt/Next, Hugo, Gatsby, and Jekyll, offering detailed insights into their usability, performance, and community support, along with success stories from real-world applications. Bundlers and Test Frameworks: We delve into the world of bundlers, comparing webpack, build, vite, and parcel 2. This section aims to guide developers through the nuances of each bundler, focusing on their performance, compatibility, and ease of use.The test frameworks section provides an in-depth look at MochaJS, Jest, Jasmine, Puppeteer, Selenium, and Playwright. It includes a comparative analysis emphasizing ease of use, community support, and overall robustness, supplemented with case studies demonstrating their effectiveness in real-world scenarios. Part 1: Runtime Environments and Package Management JavaScript is bigger than ever, and the ecosystem is nothing short of overwhelming. In this JS toolbox 2024 series, we’ve selected and analyzed the most noteworthy JS tools, so that you don’t have to. Just as any durable structure needs a solid foundation, successful JavaScript projects rely heavily on starting with the right tools. This post, the first in our JS Toolbox 2024 series, explores the core pillars of the JavaScript and TypeScript ecosystem: Runtime environments, package management, and development servers. In this post: Runtime environments Node.js Deno Bun 2. Comparing JS runtimes Installation Performance, stability, and security Community 3. Package managers NPM Yarn pnpM Bun 4. What to choose Runtime Environments In JavaScript development, runtimes are the engines that drive advanced, server-centric projects beyond the limitations of a user's browser. This independence is pivotal in modern web development, allowing for more sophisticated and versatile applications. The JavaScript runtime market is more dynamic than ever, with several contenders competing for the top spot. Node.js, the long-established leader in this space, now faces formidable competition from Deno and Bun. Deno is the brainchild of Ryan Dahl, the original creator of Node.js. It represents a significant step forward in runtime technology, emphasizing security through fine-grained access controls and modern capabilities like native TypeScript support. Bun has burst onto the scene, releasing version 1.0 in September 2023. Bun sets itself apart with exceptional speed, challenging the performance standards established by its predecessors. Bun's rapid execution capabilities, enabled by just-in-time (JIT) execution, make it a powerful alternative in the runtime environment space. An overview of framework popularity trends The popularity of Node.js has continued to grow over 2023, and I anticipate this will continue into 2024. There has been a slight downtrend in the growth trajectory, which I’d guess is due to the other tooling growing in market share. Deno has seen substantial growth over 2023. If the current trend continues I anticipate Deno to overtake Node.js in popularity in 2024, though it’s worth mentioning that star-based popularity doesn’t reflect usage in the field. Without a doubt, Node.js will retain its position as the lead environment for production systems throughout 2024. Bun has seen the largest growth in this category over the past year. I anticipate that Bun will find a steady foothold and continue its ascent, following the release of version 1.0. It’s early days for this new player, but comparing early-stage growth to others in the category, it’s shaping up to be a high performer. Node.js Node.js, acclaimed as the leading web technology by StackOverflow developers, has been a significant player in the web development world since its inception in 2009. It revolutionized web development by enabling JavaScript for server-side scripting, thus allowing for the creation of complex, backend-driven applications. Advantages Asynchronous and event-driven: Node.js operates on an asynchronous, event-driven architecture, making it efficient for scalable network applications. This model allows Node.js to handle multiple operations concurrently without blocking the main thread. Rich ecosystem: With a diverse and extensive range of tools, resources, and libraries available, Node.js offers developers an incredibly rich ecosystem, supporting a wide array of development needs. Optimized for performance: Node.js is known for its low-latency handling of HTTP requests, which is optimal for web frameworks. It efficiently utilizes system resources, allowing for load balancing and the use of multiple cores through child processes and its cluster module. Disadvantages Learning curve for asynchronous programming: The non-blocking, asynchronous nature of Node.js can be challenging for developers accustomed to linear programming paradigms, leading to a steep learning curve. Callback hell: While manageable, Node.js can lead to complex nested callbacks – often referred to as "callback hell" – which can make code difficult to read and maintain. However, this can be mitigated with modern features like async/await. Deno Deno represents a step forward in JavaScript and TypeScript runtimes, leveraging Google’s V8 engine and built-in Rust for enhanced security and performance. Conceived by Ryan Dahl, the original creator of Node.js, Deno is positioned as a more secure and modern alternative, addressing some of the core issues found in Node.js, particularly around security. Advantages Enhanced security: Deno's secure-by-default approach requires explicit permissions for file, network, and environment access, reducing the risks associated with an all-access runtime. Native TypeScript support: It offers first-class support for TypeScript and TSX, allowing developers to use TypeScript out of the box without additional transpiling steps. Single executable compilation: Deno can compile entire applications into a single, self-contained executable, simplifying deployment and distribution processes. Disadvantages Young ecosystem: Being relatively new compared to Node.js, Deno’s ecosystem is still growing, which may temporarily limit the availability of third-party modules and tools. Adoption barrier: For teams and projects deeply integrated with Node.js, transitioning to Deno can represent a significant change, posing challenges in terms of adoption and migration. Bun Bun emerges as a promising new contender in the JavaScript runtime space, positioning itself as a faster and more efficient alternative to Node.js. Developed using Zig and powered by JavaScriptCore, Bun is designed to deliver significantly quicker startup times and lower memory usage, making it an attractive option for modern web development. Currently, Bun provides a limited, experimental native build for Windows with full support for Linux and macOS. Hopefully, early in 2024, we see full support for Windows released. Advantages High performance: Bun's main draw is its performance, offering faster execution and lower resource usage compared to traditional runtimes, making it particularly suitable for high-efficiency requirements. Integrated development tools: It comes with an integrated suite of tools, including a test runner, script runner, and a Node.js-compatible package manager, all optimized for speed and compatibility with Node.js projects. Evolving ecosystem: Bun is continuously evolving, with a focus on enhancing Node.js compatibility and broadening its integration with various frameworks, signaling its potential as a versatile and adaptable solution for diverse development needs. Disadvantages Relative newness in the market: As a newer player, Bun's ecosystem is not as mature as Node.js, which might pose limitations in terms of available libraries and community support. Compatibility challenges: While efforts are being made to improve compatibility with Node.js, there may still be challenges and growing pains in integrating Bun into existing Node.js-based projects or workflows. Comparing JavaScript Runtimes Installation Each JavaScript runtime has its unique installation process. Here's a brief overview of how to install Node.js, Deno, and Bun: Node.js Download: Visit the Node.js website and download the installer suitable for your operating system. Run installer: Execute the downloaded file and follow the installation prompts. This process will install both Node.js and npm. Verify installation: Open a terminal or command prompt and type node -v and npm -v to check the installed versions of Node.js and npm, respectively. Managing different versions of Node.js has historically been a challenge for developers. To address this issue, tools like NVM (Node Version Manager) and NVM Windows have been developed, greatly simplifying the process of installing and switching between various Node.js versions. Deno Shell Command: You can install Deno using a simple shell command.• Windows: irm https://deno.land/install.ps1 | iex• Linux/macOS: curl -fsSL https://deno.land/x/install/install.sh | sh Alternative methods: Other methods like downloading a binary from the Deno releases page are also available. Verify installation: To ensure Deno is installed correctly, type deno --version in your terminal. Bun Shell Command: Similar to Deno, Bun can be installed using a shell command. For instance, on macOS, Linux, and WSL use the command curl https://bun.sh/install | bash. Alternative methods: For detailed instructions or alternative methods, check the Bun installation guide. Verify installation: After installation, run bun --version in your terminal to verify that Bun is correctly installed. Performance, Stability, and Security In evaluating JavaScript runtimes, performance, stability, and security are the key factors to consider. Mayank Choubey's benchmark studies provide insightful comparisons among Node.js, Deno, and Bun: Node.js vs Deno vs Bun: Express hello world server benchmarking Node.js vs Deno vs Bun: Native HTTP hello world server benchmarking I’d recommend giving the post a read if you’re interested in the specifics. Otherwise, I’ll do my best to summarize the results below. Node.js Historically, Node.js has been known for its efficient handling of asynchronous operations and has set a standard in server-side JavaScript performance. In the benchmark, Node.js displayed solid performance, reflective of its maturity and optimization over the years. However, it didn't lead the pack in terms of raw speed. As Node.js has been around for a long time and has proven its reliability, it wins the category of stability. Deno Deno, being a relatively newer runtime, has shown promising improvements in performance, particularly in the context of security and TypeScript support. The benchmark results for Deno were competitive, showcasing its capability to handle server requests efficiently, though it still trails slightly behind in raw processing speed compared to Bun. Given its emphasis on security features like explicit permissions for file, network, and environment access, Deno excels in the category of security. Bun Bun made a significant impression with its performance in this benchmark. It leverages Zig and JavaScriptCore, which contributes to its faster startup times and lower memory usage. In the "Hello World" server test, Bun outperformed both Node.js and Deno in terms of request handling speed, showcasing its potential as a high-performance JavaScript runtime. With its significant speed improvements, Bun leads in the category of performance. These results suggest that while Node.js remains a reliable and robust choice for many applications, Deno and Bun are catching up, offering competitive and sometimes superior performance metrics. Bun, in particular, demonstrates remarkable speed, which could be a game-changer for performance-critical applications. However, it's important to consider other factors such as stability, community support, and feature completeness when choosing a runtime for your project. Community The community surrounding a JavaScript runtime is vital for its growth and evolution. It shapes development, provides support, and drives innovation. Let's briefly examine the community dynamics for Node.js, Deno, and Bun: Node.js: Node.js has one of the largest, most diverse communities in software development, enriched by a wide array of libraries, tools, and resources. Its community actively contributes to its core and modules, bolstered by global events and forums for learning and networking. Deno: Deno's community is rapidly growing, drawing developers with its modern and security-centric features. It's characterized by active involvement in the runtime’s development and a strong online presence, particularly on platforms like GitHub and Discord. Bun: Although newer, Bun’s community is dynamic and quickly expanding. Early adopters are actively engaged in its development and performance enhancement, with lively discussions and feedback exchanges on online platforms. Each of these communities, from Node.js’s well-established network to the emerging groups around Deno and Bun, plays a crucial role in the adoption and development of these runtimes. For developers, understanding the nuances of these communities can be key to leveraging the full potential of a chosen runtime. Package Managers If you’ve ever worked on the front end of a modern web application or if you're a full-stack node engineer, you’ve likely used a package manager at some point. The package manager is responsible for managing the dependencies of your project, such as libraries, frameworks, and utilities. NPM is the default package manager that comes pre-installed with Node.js. Yarn and PNPM compete to take NPM's spot as the package management tool of choice for developers working in the JavaScript ecosystem. An overview of framework popularity trends NPM Node Package Manager or NPM for short, is the default and most dominant package manager for JavaScript projects. It comes pre-installed with Node.js, providing developers with immediate access to the npm registry, allowing them to install, share, and manage package dependencies right from the start of their project. It was created in 2009 by Isaac Schlueter as a way to share and reuse code for Node.js projects. Since then, it has grown to become a huge repository of packages that can be used for both front-end and back-end development. NPM consists of two main components: NPM CLI (Command Line Interface): This tool is used by developers to install, update, and manage packages (libraries or modules) in their JavaScript projects. It interacts with npm’s online repository, allowing developers to add external packages to their projects easily. NPM registry: An extensive online database of public and private JavaScript packages, the npm Registry is where developers can publish their packages, making them accessible to the wider JavaScript community. It's known for its vast collection of libraries, frameworks, and tools, contributing to the versatility and functionality of JavaScript projects. This star graph doesn’t capture much in terms of the overall popularity of NPM CLI given that this tool comes pre-installed with Node.js. Knowing this, it’s worth also reviewing the overall download count of these packages. NPM currently has 56,205,118,637 weekly downloads Woah, 56.2B! It’s safe to say NPM isn’t going anywhere. From the graphs, we can see a steady incline in the overall popularity of this tool through 2023. I predict this growth will continue through 2024. Yarn Yarn is a well-established open-source package manager created in 2016 by Facebook, Google, Exponent, and Tilde. It was designed to address some of the issues and limitations of NPM, such as speed, correctness, security, and developer experience. To improve these areas, Yarn incorporates a range of innovative features. These include workspaces for managing multiple packages within a single repository, offline caching for faster installs, parallel installations for improved speed, a hardened mode for enhanced security, and interactive commands for a more intuitive user interface. These features collectively contribute to Yarn’s robustness and efficiency. It features a command-line interface that closely resembles NPM's but with several enhancements and differences. It utilizes the same package.json file as NPM for defining project dependencies. Additionally, Yarn introduces the yarn.lock file, which precisely locks down the versions of dependencies, ensuring consistent installs across environments. Like NPM, Yarn also creates a node_modules folder where it installs and organizes the packages for your project. Yarn currently has 4,396,069 weekly downloads Given that Yarn and pnpM require manual installs this does mean the download counts are un-comparable with NPM but it still gives us a glance at the overall trends. In 2023, Yarn appears to have lost some of its growth trajectory but still remains the most popular alternative to NPM for package management. pnpM Performant NPM or pnpM for short, is another alternative package manager for JavaScript that was created in 2016 by Zoltan Kochan. It was designed to be faster, lighter, and more secure than both NPM and Yarn. It excels in saving disk space and speeding up the installation process. Unlike npm, where each project stores separate copies of dependencies, pnpm stores them in a content-addressable store. This approach means if multiple projects use the same dependency, they share a single stored copy, significantly reducing disk usage. When updating dependencies, pnpm only adds changed files instead of duplicating the entire package. The installation process in pnpM is streamlined into three stages: resolving dependencies, calculating the directory structure, and linking dependencies, making it faster than traditional methods. pnpM also creates a unique node_modules directory using symlinks for direct dependencies only, avoiding unnecessary access to indirect dependencies. This approach ensures a cleaner dependency structure, while still offering a traditional flat structure option through its node-linker setting for those who prefer it. pnpM currently has 8,016,757 weekly downloads pnpM's popularity surged in 2023, and I foresee this upward trend extending into 2024, as an increasing number of developers recognize its resource efficiency and streamlined project setup. Bun As Bun comes with an npm-compatible package manager, I felt it was worth mentioning here. I've covered Bun in the "Runtime Environments" section above. What To Choose Choosing the right tool for your project in 2024 depends on a variety of factors including your project's specific requirements, your team's familiarity with the technology, and the particular strengths of each tool. In the dynamic world of JavaScript development, having a clear understanding of these factors is crucial for making an informed decision. For those prioritizing stability and a proven track record, Node.js remains a top recommendation. It's well-established, supported by a vast ecosystem, and continues to be a reliable choice for a wide range of applications. Node.js's maturity makes it a safe bet, especially for projects where long-term viability and extensive community support are essential. On the other hand, if you're inclined towards experimenting with the latest advancements in the field and are operating in a Linux-based environment, Bun presents an exciting opportunity. It stands out for its impressive performance and is ideal for those looking to leverage the bleeding edge of JavaScript runtime technology. Bun’s rapid execution capabilities make it a compelling option for performance-driven projects. When it comes to package management, pnpM is an excellent choice. Its efficient handling of dependencies and disk space makes it ideal for developers managing multiple projects or large dependencies. With its growing popularity and focus on performance, pnpM is well-suited for modern JavaScript development. JavaScript tools in 2024 offer a massive range catered to different needs and preferences. Whether you opt for the stability of Node.js, the cutting-edge performance of Bun, or the efficient dependency management of pnpM, each tool brings unique strengths to the table. Carefully consider your project’s requirements and team’s expertise to make the best choice for your development journey in 2024. Like you, I’m always curious and looking to learn. If I've overlooked a noteworthy tool or if you have any feedback to share, reach out on LinkedIn. By Ollie Bannister Foreign Function and Memory API: Modernizing Native Interfacing in Java 17 Java 17 heralds a new era in Java's evolution, bringing forth the Foreign Function and Memory API as part of its feature set. This API, a cornerstone of Project Panama, is designed to revolutionize the way Java applications interact with native code and memory. Its introduction is a response to the long-standing complexities and inefficiencies associated with the Java Native Interface (JNI), offering a more straightforward, safe, and efficient pathway for Java to interface with non-Java code. This modernization is not just an upgrade but a transformation in how Java developers will approach native interoperability, promising to enhance performance, reduce boilerplate, and minimize error-prone code. Background Traditionally, interfacing Java with native code was predominantly handled through the Java Native Interface (JNI), a framework that allowed Java code to interact with applications and libraries written in other languages like C or C++. However, JNI's steep learning curve, performance overhead, and manual error handling made it less than ideal. The Java Native Access (JNA) library emerged as an alternative, offering easier use but at the cost of performance. Both methods left a gap in the Java ecosystem for a more integrated, efficient, and developer-friendly approach to native interfacing. The Foreign Function and Memory API in Java 17 fills this gap, overcoming the limitations of its predecessors and setting a new standard for native integration. Overview of the Foreign Function and Memory API The Foreign Function and Memory API is a testament to Java's ongoing evolution, designed to provide seamless and efficient interaction with native code and memory. It comprises two main components: the Foreign Function API and the Memory Access API. The Foreign Function API facilitates calling native functions from Java code, addressing type safety and reducing the boilerplate code associated with JNI. The Memory Access API allows for safe and efficient operations on native memory, including allocation, access, and deallocation, mitigating the risks of memory leaks and undefined behavior. Key Features and Advancements The Foreign Function and Memory API introduces several key features that significantly advance Java's native interfacing capabilities: Enhanced Type Safety The API advances type safety in native interactions, addressing the runtime type errors commonly associated with JNI through compile-time type resolution. This is achieved via a combination of method handles and a sophisticated linking mechanism, ensuring a robust match between Java and native types before execution. Linking at compile-time: Employing descriptors for native functions, the API ensures early type resolution, minimizing runtime type discrepancies and enhancing application stability. Utilization of method handles: The adoption of method handles in the API not only enforces strong typing but also introduces flexibility and immutability in native method invocation, elevating the safety and robustness of native calls. Minimized Boilerplate Code Addressing the verbosity inherent in JNI, the Foreign Function and Memory API offers a more concise approach to native method declaration and invocation, significantly reducing the required boilerplate code. Simplified method linking: With straightforward linking descriptors, the API negates the need for verbose JNI-style declarations, streamlining the process of interfacing with native libraries. Streamlined type conversions: The API's automatic mapping for common data types simplifies the translation between Java and native types, extending even to complex structures through direct memory layout descriptions. Streamlined Resource Management The API introduces a robust model for managing native resources, addressing the common pitfalls of memory management in JNI-based applications, such as leaks and manual deallocation. Scoped resource management: Through the concept of resource scopes, the API delineates the lifecycle of native allocations, ensuring automatic cleanup and reducing the likelihood of leaks. Integration with try-with-resources: The compatibility of resource scopes and other native allocations with Java's try-with-resources mechanism facilitates deterministic resource management, further mitigating memory management issues. Enhanced Performance Designed with performance optimization in mind, the Foreign Function and Memory API outperforms its predecessors by reducing call overhead and optimizing memory operations, crucial for high-performance native interactions. Efficient memory operations: The API's Memory Access component optimizes native memory manipulation, offering low-overhead access crucial for applications demanding high throughput or minimal latency. Reduced call overhead: By refining the native call process and minimizing intermediary operations, the API achieves a more efficient execution path for native function invocations compared to JNI. Seamless Java Integration The API is meticulously crafted to complement existing Java features, ensuring a harmonious integration that leverages the strengths of the Java ecosystem. NIO compatibility: The API's synergy with Java NIO enables efficient data exchanges between Java byte buffers and native memory, vital for I/O-centric applications. VarHandle and MethodHandle integration: By embracing VarHandle and MethodHandle, the API offers dynamic and sophisticated means for native memory and function manipulation, enriching the interaction with native code through Java's established handle framework. Practical Examples Simple To illustrate the API's utility, consider a scenario where a Java application needs to call a native library function, int sum(int a, int b), which sums two integers. With the Foreign Function and Memory API, this can be achieved with minimal boilerplate: Java MethodHandle sum = CLinker.getInstance().downcallHandle( LibraryLookup.ofPath("libnative.so").lookup("sum").get(), MethodType.methodType(int.class, int.class, int.class), FunctionDescriptor.of(CLinker.C_INT, CLinker.C_INT, CLinker.C_INT) ); int result = (int) sum.invokeExact(5, 10); System.out.println("The sum is: " + result); This example demonstrates the simplicity and type safety of invoking native functions, contrasting sharply with the more cumbersome and error-prone JNI approach. Calling a Struct-Manipulating Native Function Consider a scenario where you have a native library function that manipulates a C struct. For instance, a function void updatePerson(Person* p, const char* name, int age) that updates a Person struct. With the Memory Access API, you can define and manipulate this struct directly from Java: Java var scope = ResourceScope.newConfinedScope(); var personLayout = MemoryLayout.structLayout( CLinker.C_POINTER.withName("name"), CLinker.C_INT.withName("age") ); var personSegment = MemorySegment.allocateNative(personLayout, scope); var cString = CLinker.toCString("John Doe", scope); CLinker.getInstance().upcallStub( LibraryLookup.ofPath("libperson.so").lookup("updatePerson").get(), MethodType.methodType(void.class, MemoryAddress.class, MemoryAddress.class, int.class), FunctionDescriptor.ofVoid(CLinker.C_POINTER, CLinker.C_POINTER, CLinker.C_INT), personSegment.address(), cString.address(), 30 ); This example illustrates how you can use the Memory Access API to interact with complex data structures expected by native libraries, providing a powerful tool for Java applications that need to work closely with native code. Interfacing With Operating System APIs Another common use case for the Foreign Function and Memory API is interfacing with operating system-level APIs. For example, calling the POSIX getpid function, which returns the calling process's ID, can be done as follows: Java MethodHandle getpid = CLinker.getInstance().downcallHandle( LibraryLookup.ofDefault().lookup("getpid").get(), MethodType.methodType(int.class), FunctionDescriptor.of(CLinker.C_INT) ); int pid = (int) getpid.invokeExact(); System.out.println("Process ID: " + pid); This example demonstrates the ease with which Java applications can now invoke OS-level functions, opening up new possibilities for direct system interactions without relying on Java libraries or external processes. Advanced Memory Access The Memory Access API also allows for more advanced memory operations, such as slicing, dicing, and iterating over memory segments. This is particularly useful for operations on arrays or buffers of native memory. Working With Native Arrays Suppose you need to interact with a native function that expects an array of integers. You can allocate, populate, and pass a native array as follows: Java var intArrayLayout = MemoryLayout.sequenceLayout(10, CLinker.C_INT); try (var scope = ResourceScope.newConfinedScope()) { var intArraySegment = MemorySegment.allocateNative(intArrayLayout, scope); for (int i = 0; i < 10; i++) { CLinker.C_INT.set(intArraySegment.asSlice(i * CLinker.C_INT.byteSize()), i); } // Assuming a native function `void processArray(int* arr, int size)` MethodHandle processArray = CLinker.getInstance().downcallHandle( LibraryLookup.ofPath("libarray.so").lookup("processArray").get(), MethodType.methodType(void.class, MemoryAddress.class, int.class), FunctionDescriptor.ofVoid(CLinker.C_POINTER, CLinker.C_INT) ); processArray.invokeExact(intArraySegment.address(), 10); } This example showcases how to create and manipulate native arrays, enabling Java applications to work with native libraries that process large datasets or perform bulk operations on data. Byte Buffers and Direct Memory The Memory Access API seamlessly integrates with Java's existing NIO buffers, allowing for efficient data transfer between Java and native memory. For instance, transferring data from a ByteBuffer to native memory can be achieved as follows: Java ByteBuffer javaBuffer = ByteBuffer.allocateDirect(100); // Populate the ByteBuffer with data ... try (var scope = ResourceScope.newConfinedScope()) { var nativeBuffer = MemorySegment.allocateNative(100, scope); CLinker.asByteBuffer(nativeBuffer).put(javaBuffer); // Now nativeBuffer contains the data from javaBuffer, ready for native processing } This interoperability with NIO buffers enhances the flexibility and efficiency of data exchange between Java and native code, making it ideal for applications that require high-performance IO operations. Best Practices and Considerations Scalability and Concurrency When working with the Foreign Function and Memory API in concurrent or high-load environments, consider the implications on scalability and resource management. Leveraging ResourceScope effectively can help manage the lifecycle of native resources in complex scenarios. Security Implications Interfacing with native code can introduce security risks, such as buffer overflows or unauthorized memory access. Always validate inputs and outputs when dealing with native functions to mitigate these risks. Debugging and Diagnostics Debugging issues that span Java and native code can be challenging. Utilize Java's built-in diagnostic tools and consider logging or tracing native function calls to simplify debugging. Future Developments and Community Involvement The Foreign Function and Memory API is a living part of the Java ecosystem, with ongoing developments and enhancements influenced by community feedback and use cases. Active involvement in the Java community, through forums, JEP discussions, and contributing to OpenJDK, can help shape the future of this API and ensure it meets the evolving needs of Java developers. Conclusion The Foreign Function and Memory API in Java 17 represents a paradigm shift in Java's capability for native interfacing, offering unprecedented ease of use, safety, and performance. Through practical examples, we've seen how this API simplifies complex native interactions, from manipulating structs and arrays to interfacing with OS-level functions. As Java continues to evolve, the Foreign Function and Memory API stands as a testament to the language's adaptability and its commitment to meeting the needs of modern developers. With this API, the Java ecosystem is better equipped than ever to build high-performance, native-integrated applications, heralding a new era of Java-native interoperability. By Andrei Tuchin CORE Code Complexity in Practice Imagine entering a bustling workshop - not of whirring machines, but of minds collaborating. This is the true essence of software programming at its core: a collective effort where code serves not just as instructions for machines, but as a shared language among developers. However, unlike spoken languages, code can often become an obscure dialect, shrouded in complexity and inaccessible to newcomers. This is where the art of writing code for humans comes into play, transforming cryptic scripts into narratives that others can easily understand. After all, a primary group of users for our code are software engineers; those who are currently working with us or will work on our code in the future. This creates a shift in our software development mindset. Writing code just for the machines to understand and execute is not enough. It's necessary but not sufficient. If our code is easily human-readable and understandable then we've made a sufficient step towards manageable code complexity. This article focuses on how human-centric code can help towards manageable code complexity. There exist a number of best practices but they should be handled with careful thinking and consideration of our context. Finally, the jungle metaphor is used to explain some basic dynamics of code complexity. The Labyrinth of Complexity What is the nemesis of all human-readable code? Complexity. As projects evolve, features multiply, and lines of code snake across the screen, understanding becomes a daunting task. To combat this, developers wield a set of time-tested principles, their weapons against chaos. It is important to keep in mind that complexity is inevitable. It may be minimal complexity or high complexity, but one key takeaway here is that complexity creeps in, but it doesn't have to conquer our code. We must be vigilant and act early so that we can write code that keeps growing and not groaning. Slowing Down By applying good practices like modular design, clear naming conventions, proper documentation, and principles like those mentioned in the next paragraph, we can significantly mitigate the rate at which complexity increases. This makes code easier to understand, maintain, and modify, even as it grows. Breaking Down Complexity We can use techniques like refactoring and code reviews to identify and eliminate unnecessary complexity within existing codebases. This doesn't eliminate all complexity, but it can be significantly reduced. Choosing Better Tools and Approaches Newer programming languages and paradigms often focus on reducing complexity by design. For example, functional programming promotes immutability and modularity, which can lead to less intricate code structures. Complete Elimination of Complexity Slowing down code complexity is one thing, reducing it is another thing and completely eliminating it is something different that is rarely achievable in practice. Time-Tested Principles Below, we can find a sample of principles that may help our battle against complexity. It is by no means an exhaustive list, but it helps to make our point that context is king. While these principles offer valuable guidance, rigid adherence can sometimes backfire. Always consider the specific context of your project. Over-applying principles like Single Responsibility or Interface Segregation can lead to a bloated codebase that obscures core functionality. Don't Make Me Think Strive for code that reads naturally and requires minimal mental effort to grasp. Use clear logic and self-explanatory structures over overly convoluted designs. Make understanding the code as easy as possible for both yourself and others. Encapsulation Group related data and functionalities within classes or modules to promote data hiding and better organization. Loose Coupling Minimize dependencies between different parts of your codebase, making it easier to modify and test individual components. Separation of Concerns Divide your code into distinct layers (e.g., presentation, business logic, data access) for better maintainability and reusability. Readability Use meaningful names, consistent formatting, and comments to explain the "why" behind the code. Design Patterns (Wisely) Understand and apply these common solutions, but avoid forcing their use. For example, the SOLID principles can be summarised as follows: Single Responsibility Principle (SRP) Imagine a Swiss Army knife with a million tools. While cool, it's impractical. Similarly, code should focus on one well-defined task per class. This makes it easier to understand, maintain, and avoid unintended consequences when modifying the code. Open/Closed Principle (OCP) Think of LEGO bricks. You can build countless things without changing the individual bricks themselves. In software, OCP encourages adding new functionality through extensions, leaving the core code untouched. This keeps the code stable and adaptable. fbusin Substitution Principle (LSP) Imagine sending your friend to replace you at work. They might do things slightly differently, but they should fulfill the same role seamlessly. The LSP ensures that subtypes (inheritances) can seamlessly replace their base types without causing errors or unexpected behavior. Interface Segregation Principle (ISP) Imagine a remote with all buttons crammed together. Confusing, right? The ISP advocates for creating smaller, specialized interfaces instead of one giant one. This makes code clearer and easier to use, as different parts only interact with the functionalities they need. Dependency Inversion Principle (DIP) Picture relying on specific tools for every task. Impractical! DIP suggests depending on abstractions (interfaces) instead of concrete implementations. This allows you to easily swap out implementations without affecting the rest of the code, promoting flexibility and testability. Refactoring Regularly revisit and improve the codebase to enhance clarity and efficiency. Simplicity (KISS) Prioritize clear design, avoiding unnecessary features and over-engineering. DRY (Don't Repeat Yourself) Eliminate code duplication by using functions, classes, and modules. Documentation Write clear explanations for both code and software usage, aiding users and future developers. How Misuse Can Backfire While the listed principles aim for clarity and simplicity, their misapplication can lead to the opposite effect. Here are some examples. 1. Overdoing SOLID Strict SRP Imagine splitting a class with several well-defined responsibilities into multiple smaller classes, each handling a single, minuscule task. This can create unnecessary complexity with numerous classes and dependencies, hindering understanding. Obsessive OCP Implementing interfaces for every potential future extension, even for unlikely scenarios, may bloat the codebase with unused abstractions and complicate understanding the actual functionality. 2. Misusing Design Patterns Forced Factory Pattern Applying a factory pattern when simply creating objects directly makes sense, but can introduce unnecessary complexity and abstraction, especially in simpler projects. Overkill Singleton Using a singleton pattern for every service or utility class, even when unnecessary can create global state management issues and tightly coupled code. 3. Excessive Refactoring Refactoring Mania Constantly refactoring without a clear goal or justification can introduce churn, making the codebase unstable and harder to follow for other developers. Premature Optimization Optimizing code for potential future performance bottlenecks prematurely can create complex solutions that may never be needed, adding unnecessary overhead and reducing readability. 4. Misunderstood Encapsulation Data Fortress Overly restrictive encapsulation, hiding all internal data and methods behind complex accessors, can hinder understanding and make code harder to test and modify. 5. Ignoring Context Blindly Applying Principles Rigidly adhering to principles without considering the project's specific needs can lead to solutions that are overly complex and cumbersome for the given context. Remember The goal is to use these principles as guidelines, not strict rules. Simplicity and clarity are paramount, even if it means deviating from a principle in specific situations. Context is king: Adapt your approach based on the project's unique needs and complexity. By understanding these potential pitfalls and applying the principles judiciously, you can use them to write code that is both clear and efficient, avoiding the trap of over-engineering. The Importance of Human-Centric Code Regardless of the primary user, writing clear, understandable code benefits everyone involved. From faster collaboration and knowledge sharing to reduced maintenance and improved software quality. 1. Faster Collaboration and Knowledge Sharing Onboarding becomes a breeze: New developers can quickly grasp the code's structure and intent, reducing the time they spend deciphering cryptic logic. Knowledge flows freely: Clear code fosters open communication and collaboration within teams. Developers can easily share ideas, understand each other's contributions, and build upon previous work. Collective intelligence flourishes: When everyone understands the codebase, diverse perspectives and solutions can emerge, leading to more innovative and robust software. 2. Reduced Future Maintenance Costs Bug fixes become adventures, not nightmares: Debugging is significantly faster when the code is well-structured and easy to navigate. Developers can pinpoint issues quicker, reducing the time and resources spent on troubleshooting. Updates are a breeze, not a burden: Adding new features or modifying existing functionality becomes less daunting when the codebase is clear and understandable. This translates to lower maintenance costs and faster development cycles. Technical debt stays in check: Clear code makes it easier to refactor and improve the codebase over time, preventing technical debt from accumulating and hindering future progress. 3. Improved Overall Software Quality Fewer bugs, more smiles: Clear and well-structured code is less prone to errors, leading to more stable and reliable software. Sustainable projects, not ticking time bombs: Readable code is easier to maintain and evolve, ensuring the software's long-term viability and resilience. Happy developers, happy users: When developers can work on code they understand and enjoy, they're more productive and engaged, leading to better software and ultimately, happier users. Welcome to the Jungle Imagine a small garden, teeming with life and beauty. This is your software codebase, initially small and manageable. As features accumulate and functionality grows, the garden turns into an ever-expanding jungle. Vines of connections intertwine, and dense layers of logic sprout. Complexity, like the jungle, becomes inevitable. But just as skilled explorers can navigate the jungle, understanding its hidden pathways and navigating its obstacles, so too can developers manage code complexity. Again, if careless decisions are made in the jungle, we may endanger ourselves or make our lives miserable. Here are a few things that we can do in the jungle, being aware of what can go wrong: Clearing Paths Refactoring acts like pruning overgrown sections, removing unnecessary code, and streamlining logical flows. This creates well-defined paths, making it easier to traverse the code jungle. However, careless actions can make the situation worse. Overzealous pruning with refactoring might sever crucial connections, creating dead ends and further confusion. Clearing paths needs precision and careful consideration about what paths we need and why. Building Bridges Design patterns can serve as metaphorical bridges, spanning across complex sections and providing clear, standardized ways to access different functionalities. They offer familiar structures within the intricate wilderness. Beware though, that building bridges with ill-suited design patterns or ill-implemented patterns can lead to convoluted detours and hinder efficient navigation. Building bridges requires understanding what needs to be bridged, why, and how. Mapping the Terrain Documentation acts as a detailed map, charting the relationships between different parts of the code. By documenting code clearly, developers have a reference point to navigate the ever-growing jungle. Keep in mind that vague and incomplete documentation becomes a useless map, leaving developers lost in the wilderness. Mapping the terrain demands accuracy and attention to detail. Controlling Growth While the jungle may expand, strategic planning helps manage its complexity. Using modularization, like dividing the jungle into distinct biomes, keeps different functionalities organized and prevents tangled messes. Uncontrolled growth due to poor modularisation may result in code that is impossible to maintain. Controlling growth necessitates strategic foresight. By approaching these tasks with diligence, developers can ensure the code jungle remains explorable, understandable, and maintainable. With tools, mechanisms, and strategies tailored to our specific context and needs, developers can navigate the inevitable complexity. Now, think about the satisfaction of emerging from the dense jungle, having not just tamed it, but having used its complexities to your advantage. That's the true power of managing code complexity in software development. Wrapping Up While completely eliminating complexity might be unrealistic, we can significantly reduce the rate of growth and actively manage complexity through deliberate practices and thoughtful architecture. Ultimately, the goal is to strike a balance between functionality and maintainability. While complexity is unavoidable, it's crucial to implement strategies that prevent it from becoming an obstacle in software development. By Stelios Manioudakis CORE An Approach To Synthetic Transactions With Spring Microservices: Validating Features and Upgrades In fintech application mobile apps or the web, deploying new features in areas like loan applications requires careful validation. Traditional testing with real user data, especially personally identifiable information (PII), presents significant challenges. Synthetic transactions offer a solution, enabling the thorough testing of new functionalities in a secure and controlled environment without compromising sensitive data. By simulating realistic user interactions within the application, synthetic transactions enable developers and QA teams to identify potential issues in a controlled environment. Synthetic transactions help in ensuring that every aspect of a financial application functions correctly after any major updates or new features are rolled out. In this article, we delve into one of the approaches for using synthetic transactions. Synthetic Transactions for Financial Applications Key Business Entity At the heart of every financial application lies a key entity, be it a customer, user, or loan application itself. This entity is often defined by a unique identifier, serving as the cornerstone for transactions and operations within the system. The inception point of this entity, when it is first created, presents a strategic opportunity to categorize it as either synthetic or real. This categorization is critical, as it determines the nature of interactions the entity will undergo. Marking an entity as synthetic or for test purposes from the outset allows for a clear delineation between test and real data within the application's ecosystem. Subsequently, all transactions and operations conducted with this entity can be safely recognized as part of synthetic transactions. This approach ensures that the application's functionality can be thoroughly tested in a realistic environment. Intercepting and Managing Synthetic Transactions A critical component of implementing synthetic transactions lies in the interception and management of these transactions at the HTTP request level. Utilizing Spring's HTTP Interceptor mechanism, we can discern and process synthetic transactions by examining specific HTTP headers. The below visual outlines the coordination between a synthetic HTTP interceptor and a state manager in managing the execution of an HTTP request: Figure 1: Synthetic HTTP interceptor and state manager The SyntheticTransactionInterceptor acts as the primary gatekeeper, ensuring that only transactions identified as synthetic are allowed through the testing pathways. Below is the implementation: Java @Component public class SyntheticTransactionInterceptor implements HandlerInterceptor { protected final Logger logger = LoggerFactory.getLogger(this.getClass()); @Autowired SyntheticTransactionService syntheticTransactionService; @Autowired SyntheticTransactionStateManager syntheticTransactionStateManager; @Override public boolean preHandle(HttpServletRequest request,HttpServletResponse response, Object object) throws Exception { String syntheticTransactionId = request.getHeader("x-synthetic-transaction-uuid"); if (syntheticTransactionId != null && !syntheticTransactionId.isEmpty()){ if (this.syntheticTransactionService.validateTransactionId(syntheticTransactionId)){ logger.info(String.format("Request initiated for synthetic transaction with transaction id:%s", syntheticTransactionId)); this.syntheticTransactionStateManager.setSyntheticTransaction(true); this.syntheticTransactionStateManager.setTransactionId(syntheticTransactionId); } } return true; } } In this implementation, the interceptor looks for a specific HTTP header (x-synthetic-transaction-uuid) carrying a UUID. This UUID is not just any identifier but a validated, time-limited key designated for synthetic transactions. The validation process includes checks on the UUID's validity, its lifespan, and whether it has been previously used, ensuring a level of security and integrity for the synthetic testing process. After a synthetic ID is validated by the SyntheticTransactionInterceptor, the SyntheticTransactionStateManager plays a pivotal role in maintaining the synthetic context for the current request. The SyntheticTransactionStateManager is designed with request scope in mind, meaning its lifecycle is tied to the individual HTTP request. This scoping is essential for preserving the integrity and isolation of synthetic transactions within the application's broader operational context. By tying the state manager to the request scope, the application ensures that synthetic transaction states do not bleed over into unrelated operations or requests. Below is the implementation of the synthetic state manager: Java @Component @RequestScope public class SyntheticTransactionStateManager { private String transactionId; private boolean syntheticTransaction; public String getTransactionId() { return transactionId; } public void setTransactionId(String transactionId) { this.transactionId = transactionId; } public boolean isSyntheticTransaction() { return syntheticTransaction; } public void setSyntheticTransaction(boolean syntheticTransaction) { this.syntheticTransaction = syntheticTransaction; } } When we persist the key entity, be it a customer, user, or loan application—the application's service layer or repository layer consults the SyntheticTransactionStateManager to confirm the transaction's synthetic nature. If the transaction is indeed synthetic, the application proceeds to persist not only the synthetic identifier but also an indicator that the entity itself is synthetic. This sets the foundations for the synthetic transaction flow. This approach ensures that from the moment an entity is marked as synthetic, all related operations and future APIs, whether they involve data processing or business logic execution, are conducted in a controlled manner. For further API calls initiated from the financial application, upon reaching the microservice, we load the application context for that specific request based on the token or entity identifier provided. During the context loading, we ascertain whether the key business entity (e.g., loan application, user/customer) is synthetic. If affirmative, we then set the state manager's syntheticTransaction flag to true and also assign the synthetic transactionId from the application context. This approach negates the need to pass a synthetic transaction ID header for subsequent calls within the application flow. We only need to send a synthetic transaction ID during the initial API call that creates the key business entity. Since this step involves using explicit headers that may not be supported by the financial application, whether it's a mobile or web platform, we can manually make this first API call with Postman or a similar tool. Afterwards, the application can continue with the rest of the flow in the financial application itself. Beyond managing synthetic transactions within the application, it's also crucial to consider how external third-party API calls behave within the context of the synthetic transaction. External Third-Party API Interactions In financial applications handling key entities with personally identifiable information (PII), we conduct validations and fraud checks on user-provided data, often leveraging external third-party services. These services are crucial for tasks such as PII validation and credit bureau report retrieval. However, when dealing with synthetic transactions, we cannot make calls to these third-party services. The solution involves creating mock responses or utilizing stubs for these external services during synthetic transactions. This approach ensures that while synthetic transactions undergo the same processing logic as real transactions, they do so without the need for actual data submission to third-party services. Instead, we simulate the responses that these services would provide if they were called with real data. This allows us to thoroughly test the integration points and data-handling logic of our application. Below is the code snippet for pulling the bureau report. This call happens as part of the API call where the key entity is been created, and then subsequently we pull the applicant's bureau report: Java @Override @Retry(name = "BUREAU_PULL", fallbackMethod = "getBureauReport_Fallback") public CreditBureauReport getBureauReport(SoftPullParams softPullParams, ErrorsI error) { CreditBureauReport result = null; try { Date dt = new Date(); logger.info("UWServiceImpl::getBureauReport method call at :" + dt.toString()); CreditBureauReportRequest request = this.getCreditBureauReportRequest(softPullParams); RestTemplate restTemplate = this.externalApiRestTemplateFactory.getRestTemplate(softPullParams.getUserLoanAccountId(), "BUREAU_PULL", softPullParams.getAccessToken(), "BUREAU_PULL", error); HttpHeaders headers = this.getHttpHeaders(softPullParams); HttpEntity<CreditBureauReportRequest> entity = new HttpEntity<>(request, headers); long startTime = System.currentTimeMillis(); String uwServiceEndPoint = "/transaction"; String bureauCallUrl = String.format("%s%s", appConfig.getUnderwritingTransactionApiPrefix(), uwServiceEndPoint); if (syntheticTransactionStateManager.isSyntheticTransaction()) { result = this.syntheticTransactionService.getPayLoad(syntheticTransactionStateManager.getTransactionId(), "BUREAU_PULL", CreditBureauReportResponse.class); result.setCustomerId(softPullParams.getUserAccountId()); result.setLoanAccountId(softPullParams.getUserLoanAccountId()); } else { ResponseEntity<CreditBureauReportResponse> responseEntity = restTemplate.exchange(bureauCallUrl, HttpMethod.POST, entity, CreditBureauReportResponse.class); result = responseEntity.getBody(); } long endTime = System.currentTimeMillis(); long timeDifference = endTime - startTime; logger.info("Time taken for API call BUREAU_PULL/getBureauReport call 1: " + timeDifference); } catch (HttpClientErrorException exception) { logger.error("HttpClientErrorException occurred while calling BUREAU_PULL API, response string: " + exception.getResponseBodyAsString()); throw exception; } catch (HttpStatusCodeException exception) { logger.error("HttpStatusCodeException occurred while calling BUREAU_PULL API, response string: " + exception.getResponseBodyAsString()); throw exception; } catch (Exception ex) { logger.error("Error occurred in getBureauReport. Detail error:", ex); throw ex; } return result; } The code snippet above is quite elaborate, but we don't need to get into the details of that. What we need to focus on is the code snippet below: Java if (syntheticTransactionStateManager.isSyntheticTransaction()) { result = this.syntheticTransactionService.getPayLoad(syntheticTransactionStateManager.getTransactionId(), "BUREAU_PULL", CreditBureauReportResponse.class); result.setCustomerId(softPullParams.getUserAccountId()); result.setLoanAccountId(softPullParams.getUserLoanAccountId()); } else { ResponseEntity<CreditBureauReportResponse> responseEntity = restTemplate.exchange(bureauCallUrl, HttpMethod.POST, entity, CreditBureauReportResponse.class); result = responseEntity.getBody(); } It checks for the synthetic transaction with the SyntheticTransactionStateManager. If true, then instead of going to a third party, it calls the internal service SyntheticTransactionService to get the Synthetic Bureau report data. Synthetic Data Service Synthetic data service SyntheticTransactionServiceImpl is a general utility service whose responsibility is to pull the synthetic data from the data store, parse it, and convert it to the object type that is been passed as part of the parameter. Below is the implementation for the service: Java @Service @Qualifier("syntheticTransactionServiceImpl") public class SyntheticTransactionServiceImpl implements SyntheticTransactionService { private final Logger logger = LoggerFactory.getLogger(this.getClass()); @Autowired SyntheticTransactionRepository syntheticTransactionRepository; @Override public <T> T getPayLoad(String transactionUuid, String extPartnerServiceType, Class<T> responseType) { T payload = null; try { SyntheticTransactionPayload syntheticTransactionPayload = this.syntheticTransactionRepository.getSyntheticTransactionPayload(transactionUuid, extPartnerServiceType); if (syntheticTransactionPayload != null && syntheticTransactionPayload.getPayload() != null){ ObjectMapper objectMapper = new ObjectMapper() .configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); payload = objectMapper.readValue(syntheticTransactionPayload.getPayload(), responseType); } } catch (Exception ex){ logger.error("An error occurred while getting the synthetic transaction payload, detail error:", ex); } return payload; } @Override public boolean validateTransactionId(String transactionId) { boolean result = false; try{ if (transactionId != null && !transactionId.isEmpty()) { if (UUID.fromString(transactionId).toString().equalsIgnoreCase(transactionId)) { //Removed other validation checks, this could be financial application specific check. } } } catch (Exception ex){ logger.error("SyntheticTransactionServiceImpl::validateTransactionId - An error occurred while validating the synthetic transaction id, detail error:", ex); } return result; } With the generic method getPayLoad(), we provide a high degree of reusability, capable of returning various types of synthetic responses. This reduces the need for multiple, specific mock services for different external interactions. For storing the different payloads for different types of external third-party services, we use a generic table structure as below: MySQL CREATE TABLE synthetic_transaction ( id int NOT NULL AUTO_INCREMENT, transaction_uuid varchar(36) ext_partner_service varchar(30) payload mediumtext create_date datetime DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (id) ); ext_partner_service: This is an external service identifier for which we pull the payload from the table. In this above example for bureau report, it would be BUREAU_PULL. Conclusion In our exploration of synthetic transactions within fintech applications, we've highlighted their role in enhancing the reliability, and integrity of fintech solutions. By leveraging synthetic transactions, we simulate realistic user interactions while circumventing the risks tied to handling real personally identifiable information (PII). This approach enables our developers and QA teams to rigorously test new functionalities and updates in a secure, controlled environment. Moreover, our strategy in integrating synthetic transactions through mechanisms such as HTTP interceptors and state managers showcases a versatile approach applicable across a wide array of applications. This method not only simplifies the incorporation of synthetic transactions but also significantly boosts reusability, alleviating the need to devise unique workflows for each third-party service interaction. This approach significantly enhances the reliability and security of financial application solutions, ensuring that new features can be deployed with confidence. By Amol Gote Essential Relational Database Structures and SQL Tuning Techniques Understanding the structures within a Relational Database Management System (RDBMS) is critical to optimizing performance and managing data effectively. Here's a breakdown of the concepts with examples. RDBMS Structures 1. Partition Partitioning in an RDBMS is a technique to divide a large database table into smaller, more manageable pieces, called partitions, without changing the application's SQL queries. Example Consider a table sales_records that contains sales data over several years. Partitioning this table by year (YEAR column) means that data for each year is stored in a separate partition. This can significantly speed up queries that filter on the partition key, e.g., SELECT * FROM sales_records WHERE YEAR = 2021, as the database only searches the relevant partition. 2. Subpartition Subpartitioning is dividing a partition into smaller pieces, called subpartitions. This is essentially a second level of partitioning and can be used for further organizing data within each partition based on another column. Example Using the sales_records table, you might partition the data by year and then subpartition each year's data by quarter. This way, data for each quarter of each year is stored in its subpartition, potentially improving query performance for searches within a specific quarter of a particular year. 3. Local Index A local index is an index that exists on a partitioned table, where each partition has its independent index. The scope of a local index is limited to its partition, meaning that each index contains only the keys from that partition. Example If the sales_records table is partitioned by year, a local index on the customer_id column will create separate indexes for each year's partition. Queries filtering on both customer_id and year can be very efficient, as the database can quickly locate the partition by year and then use the local index to find records within that partition. 4. Global Index A global index is an index on a partitioned table that is not partition-specific. It includes keys from all partitions of the table, providing a way to search across all partitions quickly. Example A global index on the customer_id column in the sales_records table would enable fast searches for a particular customer's records across all years without needing to access each partition's local index. 5. Create Deterministic Functions for Same Input and Known Output A deterministic function in SQL returns the same result every time it's called with the same input. This consistency can be leveraged for optimization purposes, such as function-based indexes. Function Example CREATE OR REPLACE FUNCTION get_discount_category(price NUMBER) RETURN VARCHAR2 DETERMINISTIC IS BEGIN IF price < 100 THEN RETURN 'Low'; ELSIF price BETWEEN 100 AND 500 THEN RETURN 'Medium'; ELSE RETURN 'High'; END IF; END; This function returns a discount category based on the price. Since it's deterministic, the database can optimize calls to this function within queries. 6. Create Bulk Load for Heavy Datasets Bulk loading is the process of efficiently importing large volumes of data into a database. This is crucial for initializing databases with existing data or integrating large datasets periodically. Example In Oracle, you can use SQL*Loader for bulk-loading data. Here's a simple command to load data from a CSV file into the sales_records table. Bash: Shell sqlldr userid=username/password@database control=load_sales_records.ctl direct=true The control file (load_sales_records.ctl) defines how the data in the CSV file maps to the columns in the sales_records table. The direct=true option specifies that SQL*Loader should use direct path load, which is faster and uses fewer database resources than conventional path load. SQL Tuning Techniques SQL tuning methodologies are essential for optimizing query performance in relational database management systems. Here's an explanation of the methods with examples to illustrate each: 1. Explain Plan Analysis An explain plan shows how the database executes a query, including its paths and methods to access data. Analyzing an explain plan helps identify potential performance issues, such as full table scans or inefficient joins. Example EXPLAIN PLAN FOR SELECT * FROM employees WHERE department_id = 10; Analyzing the output might reveal whether the query uses an index or a full table scan, guiding optimization efforts. 2. Gather Statistics Gathering statistics involves collecting data about table size, column distribution, and other characteristics that the query optimizer uses to determine the most efficient query execution plan. Full statistics: Collect statistics for the entire table Incremental statistics: Collect statistics for the parts of the table that have changed since the last collection Example -- Gather full statistics EXEC DBMS_STATS.GATHER_TABLE_STATS('MY_SCHEMA', 'MY_TABLE'); -- Gather incremental statistics EXEC DBMS_STATS.SET_TABLE_PREFS('MY_SCHEMA', 'MY_TABLE', 'INCREMENTAL', 'TRUE'); EXEC DBMS_STATS.GATHER_TABLE_STATS('MY_SCHEMA', 'MY_TABLE'); 3. Structure Your Queries for Efficient Joins Structuring your SQL queries to take advantage of the most efficient join methods based on your data characteristics and access patterns is critical to query optimization. This strategy involves understanding the nature of your data, the relationships between different data sets, and how your application accesses this data. You can significantly improve query performance by aligning your query design with these factors. Here's a deeper dive into what this entails: Understanding Your Data and Access Patterns Data volume: The size of the data sets you're joining affects which join method will be most efficient. For instance, hash joins might be preferred for joining two large data sets, while nested loops could be more efficient for smaller data sets or when an indexed access path exists. Data distribution and skew: Knowing how your data is distributed and whether there are skewnesses (e.g., some values are far more common than others) can influence join strategy. For skewed data, certain optimizations might be necessary to avoid performance bottlenecks. Indexes: The presence of indexes on the join columns can make nested loop joins more efficient, especially if one of the tables involved in the join is significantly smaller than the other. Choosing the right join type: Use inner joins, outer joins, cross joins, etc., based on the logical requirements of your query and the characteristics of your data. Each join type has its performance implications. Order of tables in the join: In certain databases and scenarios, the order in which tables are joined can influence performance, especially for nested loop joins where the outer table should ideally have fewer rows than the inner table. Filter early: Apply filters as early as possible in your query to reduce the size of the data sets that need to be joined. This can involve subqueries, CTEs (Common Table Expressions), or WHERE clause optimizations to narrow down the data before it is joined. Use indexes effectively: Design your queries to take advantage of indexes on join columns, where possible. This might involve structuring your WHERE clauses or JOIN conditions to use indexed columns efficiently. Practical Examples For large data set joins: If you're joining two large data sets and you know the join will involve scanning large portions of both tables, structuring your query to use a hash join can be beneficial. Ensure that neither table has a filter that could significantly reduce its size before the join, as this could make a nested loops join more efficient if one of the tables becomes much smaller after filtering. For indexed access: If you're joining a small table to a large table and the large table has an index on the join column, structuring your query to encourage a nested loops join can be advantageous. The optimizer will likely pick this join method, but careful query structuring and hinting can ensure it. Join order and filtering: Consider how the join order and placement of filter conditions can impact performance in complex queries involving multiple joins. Placing the most restrictive filters early in the query can reduce the amount of data being joined in later steps. By aligning your query structure with your data's inherent characteristics and your application's specific access patterns, you can guide the SQL optimizer to choose the most efficient execution paths. This often involves a deep understanding of both the theoretical aspects of how different join methods work and practical knowledge gained from observing the performance of your queries on your specific data sets. Continuous monitoring and tuning are essential for maintaining optimal performance based on changing data volumes and usage patterns. Example: If you're joining a large table with a small table and there's an index on the join column of the large table, structuring the query to ensure the optimizer chooses a nested loop join can be more efficient. 4. Use Common Table Expressions (CTEs) CTEs make your queries more readable and can improve performance by breaking down complex queries into simpler parts. Example SQL WITH RegionalSales AS ( SELECT region, SUM(sales) AS total_sales FROM sales GROUP BY region ) SELECT * FROM RegionalSales WHERE total_sales > 1000000; 5. Use Global Temporary Tables and Indexes Global temporary tables store intermediate results for the duration of a session or transaction, which can be indexed for faster access. Example SQL CREATE GLOBAL TEMPORARY TABLE temp_sales AS SELECT * FROM sales WHERE year = 2021; CREATE INDEX idx_temp_sales ON temp_sales(sales_id); 6. Multiple Indexes With Different Column Ordering Creating multiple indexes on the same set of columns but in different orders can optimize different query patterns. Example SQL CREATE INDEX idx_col1_col2 ON my_table(col1, col2); CREATE INDEX idx_col2_col1 ON my_table(col2, col1); 7. Use Hints Hints are instructions embedded in SQL statements that guide the optimizer to choose a particular execution plan. Example SQL SELECT /*+ INDEX(my_table my_index) */ * FROM my_table WHERE col1 = 'value'; 8. Joins Using Numeric Values Numeric joins are generally faster than string joins because numeric comparisons are faster than string comparisons. Example Instead of joining on string columns, if possible, join on numeric columns like IDs that represent the same data. 9. Full Table Scan vs. Partition Pruning Use a full table scan when you need to access a significant portion of the table or when there's no suitable index. Use partition pruning when you're querying partitioned tables and your query can be limited to specific partitions. Example -- Likely results in partition pruning SELECT * FROM sales_partitioned WHERE sale_date BETWEEN '2021-01-01' AND '2021-01-31'; 10. SQL Tuning Advisor The SQL Tuning Advisor analyzes SQL statements and provides recommendations for improving performance, such as creating indexes, restructuring the query, or gathering statistics. Example In Oracle, you can use the DBMS_SQLTUNE package to run the SQL Tuning Advisor: SQL DECLARE l_tune_task_id VARCHAR2(100); BEGIN l_tune_task_id := DBMS_SQLTUNE.create_tuning_task(sql_id => 'your_sql_id_here'); DBMS_SQLTUNE.execute_tuning_task(task_name => l_tune_task_id); DBMS_OUTPUT.put_line(DBMS_SQLTUNE.report_tuning_task(l_tune_task_id)); END; Conclusion Each of these structures and techniques optimizes data storage, retrieval, and manipulation in an RDBMS, enabling efficient handling of large datasets and complex queries. Each of these tuning methodologies targets specific aspects of SQL performance, from how queries are structured to how the database's optimizer interprets and executes them. By applying these techniques, you can significantly improve the efficiency and speed of your database operations. By Anandaganesh Balakrishnan Architectural Insights: Designing Efficient Multi-Layered Caching With Instagram Example Caching is a critical technique for optimizing application performance by temporarily storing frequently accessed data, allowing for faster retrieval during subsequent requests. Multi-layered caching involves using multiple levels of cache to store and retrieve data. Leveraging this hierarchical structure can significantly reduce latency and improve overall performance. This article will explore the concept of multi-layered caching from both architectural and development perspectives, focusing on real-world applications like Instagram, and provide insights into designing and implementing an efficient multi-layered cache system. Understanding Multi-Layered Cache in Real-World Applications: Instagram Example Instagram, a popular photo and video-sharing social media platform, handles vast amounts of data and numerous user requests daily. To maintain optimal performance and provide a seamless user experience, Instagram employs an efficient multi-layered caching strategy that includes in-memory caches, distributed caches, and Content Delivery Networks (CDNs). 1. In-Memory Cache Instagram uses in-memory caching systems like Memcached and Redis to store frequently accessed data, such as user profiles, posts, and comments. These caches are incredibly fast since they store data in the system's RAM, offering low-latency access to hot data. 2. Distributed Cache To handle the massive amount of user-generated data, Instagram also employs distributed caching systems. These systems store data across multiple nodes, ensuring scalability and fault tolerance. Distributed caches like Cassandra and Amazon DynamoDB are used to manage large-scale data storage while maintaining high availability and low latency. 3. Content Delivery Network (CDN) Instagram leverages CDNs to cache and serve static content more quickly to users. This reduces latency by serving content from the server closest to the user. CDNs like Akamai, Cloudflare, and Amazon CloudFront help distribute static assets such as images, videos, and JavaScript files to edge servers worldwide. Architectural and Development Insights for Designing and Implementing a Multi-Layered Cache System When designing and implementing a multi-layered cache system, consider the following factors: 1. Data Access Patterns Analyze the application's data access patterns to determine the most suitable caching strategy. Consider factors such as data size, frequency of access, and data volatility. For instance, frequently accessed and rarely modified data can benefit from aggressive caching, while volatile data may require a more conservative approach. 2. Cache Eviction Policies Choose appropriate cache eviction policies for each cache layer based on data access patterns and business requirements. Common eviction policies include Least Recently Used (LRU), First In First Out (FIFO), and Time To Live (TTL). Each policy has its trade-offs, and selecting the right one can significantly impact cache performance. 3. Scalability and Fault Tolerance Design the cache system to be scalable and fault-tolerant. Distributed caches can help achieve this by partitioning data across multiple nodes and replicating data for redundancy. When selecting a distributed cache solution, consider factors such as consistency, partition tolerance, and availability. 4. Monitoring and Observability Implement monitoring and observability tools to track cache performance, hit rates, and resource utilization. This enables developers to identify potential bottlenecks, optimize cache settings, and ensure that the caching system is operating efficiently. 5. Cache Invalidation Design a robust cache invalidation strategy to keep cached data consistent with the underlying data source. Techniques such as write-through caching, cache-aside, and event-driven invalidation can help maintain data consistency across cache layers. 6. Development Considerations Choose appropriate caching libraries and tools for your application's tech stack. For Java applications, consider using Google's Guava or Caffeine for in-memory caching. For distributed caching, consider using Redis, Memcached, or Amazon DynamoDB. Ensure that your caching implementation is modular and extensible, allowing for easy integration with different caching technologies. Example Below is a code snippet to demonstrate a simple implementation of a multi-layered caching system using Python and Redis for the distributed cache layer. First, you'll need to install the redis package: Shell pip install redis Next, create a Python script with the following code: Python import redis import time class InMemoryCache: def __init__(self, ttl=60): self.cache = {} self.ttl = ttl def get(self, key): data = self.cache.get(key) if data and data['expire'] > time.time(): return data['value'] return None def put(self, key, value): self.cache[key] = {'value': value, 'expire': time.time() + self.ttl} class DistributedCache: def __init__(self, host='localhost', port=6379, ttl=300): self.r = redis.Redis(host=host, port=port) self.ttl = ttl def get(self, key): return self.r.get(key) def put(self, key, value): self.r.setex(key, self.ttl, value) class MultiLayeredCache: def __init__(self, in_memory_cache, distributed_cache): self.in_memory_cache = in_memory_cache self.distributed_cache = distributed_cache def get(self, key): value = self.in_memory_cache.get(key) if value is None: value = self.distributed_cache.get(key) if value is not None: self.in_memory_cache.put(key, value) return value def put(self, key, value): self.in_memory_cache.put(key, value) self.distributed_cache.put(key, value) # Usage example in_memory_cache = InMemoryCache() distributed_cache = DistributedCache() multi_layered_cache = MultiLayeredCache(in_memory_cache, distributed_cache) key, value = 'example_key', 'example_value' multi_layered_cache.put(key, value) print(multi_layered_cache.get(key)) This example demonstrates a simple multi-layered cache using an in-memory cache and Redis as a distributed cache. The InMemoryCache class uses a Python dictionary to store cached values with a time-to-live (TTL). The DistributedCache class uses Redis for distributed caching with a separate TTL. The MultiLayeredCache class combines both layers and handles data fetching and storage across the two layers. Note: You should have a Redis server running on your localhost. Conclusion Multi-layered caching is a powerful technique for improving application performance by efficiently utilizing resources and reducing latency. Real-world applications like Instagram demonstrate the value of multi-layered caching in handling massive amounts of data and traffic while maintaining smooth user experiences. By understanding the architectural and development insights provided in this article, developers can design and implement multi-layered caching systems in their projects, optimizing applications for faster, more responsive experiences. Whether working with hardware or software-based caching systems, multi-layered caching is a valuable tool in a developer's arsenal. By Arun Pandey CORE From Batch ML To Real-Time ML Real-time machine learning refers to the application of machine learning algorithms that continuously learn from incoming data and make predictions or decisions in real-time. Unlike batch machine learning, where data is collected over a period and processed in batches offline, real-time ML operates instantaneously on streaming data, allowing for immediate responses to changes or events. Common use cases include fraud detection in financial transactions, predictive maintenance in manufacturing, recommendation systems in e-commerce, and personalized content delivery in media. Challenges in building real-time ML capabilities include managing high volumes of streaming data efficiently, ensuring low latency for timely responses, maintaining model accuracy and performance over time, and addressing privacy and security concerns associated with real-time data processing. This article delves into these concepts and provides insights into how organizations can overcome these challenges to deploy effective real-time ML systems. Use Cases Now we have explained the difference between batch ML and real-time ML, it's worth mentioning that in real-life use cases, you can have batch ML, real-time ML, or in between batch and real-time. For example, you can have scenarios where you have real-time inference with batch features, real-time inference with real-time features, or real-time inference with batch features and real-time features. Continuous machine learning is beyond the scope of this article, but you can apply real-time feature solutions to continuous machine learning (CML) too. Hybrid approaches that combine real-time and batch-learning aspects offer a flexible solution to address various requirements and constraints in different applications. Here are some expanded examples: Uses Cases Batch Real-Time Fraud detection in banking Initially, a fraud detection model can be trained offline using a large historical dataset of transactions. This batch training allows the model to learn complex patterns of fraudulent behavior over time, leveraging the entirety of available historical data. Once the model is deployed, it continues to learn in real-time as new transactions occur. Each transaction is processed in real-time, and the model is updated periodically (e.g., hourly or daily) using batches of recent transaction data. This real-time updating ensures that the model can quickly adapt to emerging fraud patterns without sacrificing computational efficiency. Recommendation systems in e-commerce A recommendation system may be initially trained offline using a batch of historical user interaction data, such as past purchases, clicks, and ratings. This batch training allows the model to learn user preferences and item similarities effectively. Once the model is deployed, it can be fine-tuned in real-time as users interact with the system. For example, when a user makes a purchase or provides feedback on a product, the model can be updated immediately to adjust future recommendations for that user. This real-time personalization enhances user experience and engagement without requiring retraining the entire model with each interaction. Natural Language Processing (NLP) applications NLP models, such as sentiment analysis or language translation models, can be trained offline using large corpora of text data. Batch training allows the model to learn semantic representations and language structures from diverse text sources. Once deployed, the model can be fine-tuned in real-time using user-generated text data, such as customer reviews or live chat interactions. Real-time fine-tuning enables the model to adapt to domain-specific or user-specific language nuances and evolving trends without requiring retraining from scratch. In each of these examples, the hybrid approach combines the depth of analysis provided by batch learning with the adaptability of real-time learning, resulting in more robust and responsive machine learning systems. The choice between real-time and batch-learning elements depends on the specific requirements of the application, such as data volume, latency constraints, and the need for continuous adaptation. What Are the Main Components of a Real-Time ML Pipeline? A real-time machine learning (ML) pipeline typically consists of several components working together to enable the continuous processing of data and the deployment of ML models with minimal latency. Here are the main components of such a pipeline: 1. Data Ingestion This component is responsible for collecting data from various sources in real-time. It could involve streaming data from sensors, databases, APIs, or other sources. 2. Streaming Data Processing and Feature Engineering Once the data is ingested, it needs to be processed in real-time. This component involves streaming data processing frameworks that handle the data streams efficiently. Features extracted from raw data are crucial for building ML models. This component involves transforming the raw data into meaningful features that can be used by the ML models. Feature engineering might include techniques like normalization, encoding categorical variables, and creating new features. 3. Model Training Training typically occurs at regular intervals, with the frequency varying between near-real-time, which involves more frequent time frames than batch training, or online-real-time training. 4. Model Inference This component involves deploying the ML models and making predictions in real time. The deployed models should be optimized for low latency inference, and they need to scale well to handle varying loads. 5. Scalability and Fault Tolerance Real-time ML pipelines must be scalable to handle large volumes of data and fault-tolerant to withstand failures gracefully. This often involves deploying the pipeline across distributed systems and implementing mechanisms for fault recovery and data replication. Challenges for Building Real-Time ML Pipelines Low Latency Requirement Real-time pipelines must process data and make predictions within strict time constraints, often in milliseconds. Achieving low latency requires optimizing every component of the pipeline, including data ingestion, pre-processing, model inference, and output delivery. Scalability Real-time pipelines must handle varying workloads and scale to accommodate increasing data volumes and computational demands. Designing scalable architectures involves choosing appropriate technologies and distributed computing strategies to ensure efficient resource utilization and horizontal scalability. Feature Engineering Generating features in real time from streaming data can be complex and resource-intensive. Designing efficient feature extraction and transformation pipelines that adapt to changing data distributions and maintain model accuracy over time is a key challenge. Security Robust authentication, authorization, and secure communication mechanisms are essential for real-time ML. Having effective incident response and monitoring capabilities enables organizations to detect and respond to security incidents promptly, bolstering the overall resilience of real-time ML pipelines against security threats. By addressing these security considerations comprehensively, organizations can build secure real-time ML pipelines that protect sensitive data and assets effectively. Cost Optimization Building and operating real-time ML pipelines can be costly, especially when using cloud-based infrastructure or third-party services. Optimizing resource utilization, selecting cost-effective technologies, and implementing auto-scaling and resource provisioning strategies are essential for controlling operational expenses. Robustness and Fault Tolerance Real-time pipelines must be resilient to failures and ensure continuous operation under adverse conditions. Implementing fault tolerance mechanisms, such as data replication, checkpointing, and automatic failover, is critical for maintaining system reliability and availability. Integration with Existing Systems Integrating real-time ML pipelines with existing IT infrastructure, data sources, and downstream applications requires careful planning and coordination. Ensuring compatibility, interoperability, and seamless data flow between different components of the system is essential for successful deployment and adoption. Addressing these challenges requires a combination of domain expertise, software engineering skills, and knowledge of distributed systems, machine learning algorithms, and cloud computing technologies. Opting for solutions that streamline operations by minimizing the number of tools involved can be a game-changer. This approach not only slashes integration efforts but also trims down maintenance costs and operational overheads while ushering in lower latency—a crucial factor in real-time ML applications. By consolidating feature processing and storage into a single, high-speed key-value store, with a real-time ML model serving, Hazelcast simplifies the AI landscape, reducing complexity and ensuring seamless data flow. The Future of Real-Time ML The future of real-time machine learning (ML) is closely intertwined with advancements in vector databases and the emergence of Relative Attribute Graphs (RAG). Vector databases provide efficient storage and querying capabilities for high-dimensional data, making them well-suited for managing the large feature spaces common in ML applications. Relative Attribute Graphs, on the other hand, offer a novel approach to representing and reasoning about complex relationships in data, enabling more sophisticated analysis and decision-making in real-time ML pipelines. In the context of finance and fintech, the integration of vector databases and RAGs holds significant promise for enhancing various aspects of real-time ML applications. One example is in fraud detection and prevention. Financial institutions must constantly monitor transactions and identify suspicious activities to mitigate fraud risk. By leveraging vector databases to store and query high-dimensional transaction data efficiently, combined with RAGs to model intricate relationships between transactions, real-time ML algorithms can detect anomalies and fraudulent patterns in real time with greater accuracy and speed. Another application area is in personalized financial recommendations and portfolio management. Traditional recommendation systems often struggle to capture the nuanced preferences and goals of individual users. However, by leveraging vector representations of user preferences and financial assets stored in vector databases, and utilizing RAGs to model the relative attributes and interdependencies between different investment options, real-time ML algorithms can generate personalized recommendations that better align with users' financial objectives and risk profiles. For example, a real-time ML system could analyze a user's financial history, risk tolerance, and market conditions to dynamically adjust their investment portfolio in response to changing market conditions and personal preferences. Furthermore, in algorithmic trading, real-time ML models powered by vector databases and RAGs can enable more sophisticated trading strategies that adapt to evolving market dynamics and exploit complex interrelationships between different financial instruments. By analyzing historical market data stored in vector databases and incorporating real-time market signals represented as RAGs, algorithmic trading systems can make more informed and timely trading decisions, optimizing trading performance and risk management. Overall, the future of real-time ML in finance and fintech is poised to benefit significantly from advancements in vector databases and RAGs. By leveraging these technologies, organizations can build more intelligent, adaptive, and efficient real-time ML pipelines that enable enhanced fraud detection, personalized financial services, and algorithmic trading strategies. By Fawaz Ghali, PhD CORE Best Practices To Secure Stateless REST Applications Statelessness in RESTful applications poses challenges and opportunities, influencing how we manage fundamental security aspects such as authentication and authorization. This blog aims to delve into this topic, explore its impact, and offer insights into the best practices for handling stateless REST applications. Understanding Statelessness in REST REST, or REpresentational State Transfer, is an architectural style that defines a set of constraints for creating web services. One of its core principles is statelessness, which means that each request from a client to a server must contain all the information needed to understand and process the request. This model stands in contrast to stateful approaches, where the server stores user session data between requests. The stateless nature of REST brings significant benefits, particularly in terms of scalability and reliability. By not maintaining a state between requests, RESTful services can handle requests independently, allowing for more efficient load balancing and reduced server memory requirements. However, this approach introduces complexities in managing user authentication and authorization. Authentication in Stateless REST Applications Token-Based Authentication The most common approach to handling authentication in stateless REST applications is through token-based methods, like JSON Web Tokens (JWT). In this model, the server generates a token that encapsulates user identity and attributes when they log in. This token is then sent to the client, which will include it in the HTTP header of subsequent requests. Upon receiving a request, the server decodes the token to verify user identity. Finally, the authorization service can make decisions based on the user permissions. // Example of a JWT token in an HTTP header Authorization: Bearer <token> OAuth 2.0 Another widely used framework is OAuth 2.0, particularly for applications requiring third-party access. OAuth 2.0 allows users to grant limited access to their resources from another service without exposing their credentials. It uses access tokens, providing layered security and enabling scenarios where an application needs to act on behalf of the user. Authorization in Stateless REST Applications Once authentication is established, the next challenge is authorization — checking the user has permission to perform the relevant actions on resources. Keeping REST applications stateless requires decoupling policy and code. In traditional stateful applications, authorization decisions are made in imperative code statements that clutter the application logic and rely on the state of the request. In a stateless application, policy logic should be separated from the application code and be defined separately as policy code (using policy as code engines and languages), thus keeping the application logic stateless. Here are some examples of stateless implementation of common policy models: Role-Based Access Control (RBAC) Role-Based Access Control (RBAC) is a common pattern where users are assigned roles that dictate the access level a user has to resources. When decoupling policy from the code, the engine syncs the user roles from the identity provider. By providing the JWT with the identity, the policy engine can return a decision on whether a role is allowed to perform the action or not. Attribute-Based Access Control (ABAC) A more dynamic approach is Attribute-Based Access Control (ABAC), which evaluates a set of policies against the attributes of users, resources, and the environment. This model offers more granular control and flexibility, which is particularly useful in complex systems with varying access requirements. To keep REST applications stateless, it is necessary to declare these policies in a separate code base as well as ensure that the data synchronization with the engine is stateless. Relationship-Based Access Control (ReBAC) In applications where data privacy is of top importance, and users can have ownership of their data by declaring relationships, Using a centralized graph outside of the REST application is necessary to maintain the statelessness of the application logic. A well-crafted implementation of an authorization service will have the application throw a stateless check function with the identity and resource instance. Then, the authorization service will analyze it based on the stateful graph separated from the application. Security Considerations in Stateless Authentication and Authorization Handling Token Security In stateless REST applications, token security is critical, and developers must ensure that tokens are encrypted and transmitted securely. The use of HTTPS is mandatory to prevent token interception. Additionally, token expiration mechanisms must be implemented to reduce the risk of token hijacking. It’s a common practice to have short-lived access tokens and longer-lived refresh tokens to balance security and user convenience. Preventing CSRF and XSS Attacks Cross-Site Request Forgery (CSRF) and Cross-Site Scripting (XSS) are two prevalent security threats in web applications. Using tokens instead of cookies in stateless REST APIs can inherently mitigate CSRF attacks, as the browser does not automatically send the token. However, developers must still be vigilant about XSS attacks, which can compromise token security. Implementing Content Security Policy (CSP) headers and sanitizing user input are effective strategies against XSS. Performance Implications Caching Strategies Statelessness in REST APIs poses unique challenges for caching, as user-specific data cannot be stored on the server. Leveraging HTTP cache headers effectively allows clients to cache responses appropriately, reducing the load on the server and improving response times. ETag headers and conditional requests can optimize bandwidth usage and enhance overall application performance. Load Balancing and Scalability Stateless applications are inherently more scalable as they allow for straightforward load balancing. Since there’s no session state tied to a specific server, any server can handle any request. This property enables seamless horizontal scaling, which is essential for applications anticipating high traffic volumes. Conclusion: Balancing Statelessness With Practicality Implementing authentication and authorization in stateless REST applications involves a careful balance between security, performance, and usability. While statelessness offers numerous advantages in terms of scalability and simplicity, it also necessitates robust security measures and thoughtful system design. The implications of token-based authentication, access control mechanisms, security threats, and performance strategies must be considered to build effective and secure RESTful services. By Gabriel L. Manor Architecture: Software Cost Estimation Estimating workloads is crucial in mastering software development. This can be achieved either as an ongoing development part of agile teams or in response to tenders as a cost estimate before migration, among other ways. The team responsible for producing the estimate regularly encounters a considerable workload, which can lead to significant time consumption if the costing is not conducted using the correct methodology. The measurement figures generated may significantly differ based on the efficiency of the technique employed. Additionally, misconceptions regarding validity requirements and their extent exist. This paper presents a novel hybrid method for software cost estimation that discretizes software into smaller tasks and uses both expert judgment and algorithmic techniques. By using a two-factor qualification system based on volumetry and complexity, we present a more adaptive and scalable model for estimating software project duration, with particular emphasis on large legacy migration projects. Table Of Contents Introduction Survey of Existing SCE2.1. Algorithmic Methods2.2. Non-algorithmic Methods2.3. AI-based Methods2.4. Agile Estimation Techniques Hybrid Model Approach3.1. Discretization3.2. Dual-factor Qualification System and Effort Calculation Task3.3. Abacus System Specific Use Case in Large Legacy Migration Projects4.1. Importance of SCE in Legacy Migration4.2. Application of the Hybrid Model4.3. Results and Findings Conclusion Introduction Software Cost Estimation (SCE) is a systematic and quantitative process within the field of software engineering that involves analyzing, predicting, and allocating the financial, temporal, and resource investments required for the development, maintenance, and management of software systems. This vital effort uses different methods, models, and techniques to offer stakeholders knowledgeable evaluations of the expected financial, time, and resource requirements for successful software project execution. It is an essential part of project planning, allowing for a logical distribution of resources and supporting risk assessment and management during the software development life cycle. Survey of Existing SCE Algorithmic Methods COCOMO Within the field of software engineering and cost estimation, the Constructive Cost Model, commonly referred to as COCOMO, is a well-established and highly regarded concept. Developed by Dr Barry Boehm, COCOMO examines the interplay between software attributes and development costs. The model operates on a hierarchy of levels, ranging from basic to detailed, with each level providing varying degrees of granularity [1]. The model carefully uses factors such as lines of code and other project details, aligning them with empirical cost estimation data. Nonetheless, COCOMO is not a stagnant vestige of the past. It has progressed over the years, with COCOMO II encompassing the intricacies of contemporary software development practices, notably amid constantly evolving paradigms like object-oriented programming and agile methodologies [2]. However, though COCOMO’s empirical and methodical approach provides credibility, its use of lines of code as a primary metric attracts criticism. This is particularly true for projects where functional attributes are of greater importance. Function Point Analysis (FPA) Navigating away from the strict confines of code metrics, Function Point Analysis (FPA) emerges as a holistic method for evaluating software from a functional perspective. Introduced by Allan Albrecht at IBM in the late 1970s, FPA aims to measure software by its functionality and the value it provides to users, rather than the number of lines of code. By categorizing and evaluating different user features — such as inputs, outputs, inquiries, and interfaces — FPA simplifies software complexity into measurable function points [3]. This methodology is particularly effective in projects where the functional output is of greater importance than the underlying code. FPA, which takes a user-focused approach, aligns well with customer demands and offers a concrete metric that appeals to developers and stakeholders alike. However, it is important to note that the effectiveness of FPA depends on a thorough comprehension of user needs, and uncertainties could lead to discrepancies in estimation. SLIM (Software Life Cycle Management) Rooted in the philosophy of probabilistic modeling, SLIM — an acronym for Software Life Cycle Management — is a multifaceted tool designed by Lawrence Putnam [4]. SLIM’s essence revolves around a set of non-linear equations that, when woven together, trace the trajectory of software development projects from inception to completion. Leveraging a combination of historical data and project specifics, SLIM presents a probabilistic landscape that provides insights regarding project timelines, costs, and potential risks. What distinguishes SLIM is its capability to adapt and reconfigure as projects progress. By persistently absorbing project feedback, SLIM dynamically refines its estimates to ensure they remain grounded in project actualities. This continuous recalibration is both SLIM’s greatest asset and its primary obstacle. While it provides flexible adaptability, it also requires detailed data recording and tracking, which requires a disciplined approach from project teams. Non-Algorithmic Methods Expert Judgement Treading the venerable corridors of software estimation methodologies, one cannot overlook the enduring wisdom of Expert Judgment [5]. Avoiding the rigorous algorithms and formalities of other techniques, Expert Judgment instead draws upon the accumulated experience and intuitive prowess of industry veterans. These experienced practitioners, with their wealth of insights gathered from a multitude of projects, have an innate ability to assess the scope, intricacy, and possible difficulties of new ventures. Their nuanced comprehension can bridge gaps left by more strictly data-driven models. Expert Judgment captures the intangible subtleties of a project artfully, encapsulating the software development craft in ways quantitative metrics may overlook. However, like any art form, Expert Judgment is subject to the quirks of its practitioners. It is vulnerable to personal biases and the innate variability of human judgment. Analogous Estimation (or Historical Data) Historical Data estimation, also known as Analogous Estimation, is a technique used to inform estimates for future projects by reviewing past ones. It is akin to gazing in the rearview mirror to navigate the path ahead. This method involves extrapolating experiences and outcomes of similar previous projects and comparing them to the current one. By doing so, it provides a grounded perspective tempered by real-world outcomes to inform estimates. Its effectiveness rests on its empirical grounding, with past events often offering reliable predictors for future undertakings. Nevertheless, the quality and relevance of historical data at hand are crucial factors. A mismatched comparison or outdated data can lead projects astray, underscoring the importance of careful data curation and prudent implementation [6]. Delphi Technique The method draws its name from the ancient Oracle of Delphi, and it orchestrates a harmonious confluence of experts. The Delphi Technique is a method that aims to reach a consensus by gathering anonymous insights and projections from a group of experts [7]. This approach facilitates a symposium of collective wisdom rather than relying on a singular perspective. Through iterative rounds of feedback, the estimates are refined and recalibrated based on the collective input. The Delphi Technique is a structured yet dynamic process that filters out outliers and converges towards a more balanced, collective judgment. It is iterative in nature and emphasizes anonymity to curtail the potential pitfalls of groupthink and influential biases. This offers a milieu where each expert’s voice finds its rightful resonance. However, the Delphi Technique requires meticulous facilitation and patience, as it journeys through multiple rounds of deliberation before arriving at a consensus. AI-Based Methods Machine Learning in SCE Within the rapidly evolving landscape of Software Cost Estimation, Machine Learning (ML) emerges as a formidable harbinger of change [8]. Unshackling from the deterministic confines of traditional methods, ML delves into probabilistic realms, harnessing vast swaths of historical data to unearth hidden patterns and correlations. By training on diverse project datasets, ML algorithms refine their predictive prowess, adapting to nuances often overlooked by rigid, rule-based systems. This adaptability positions ML as a particularly potent tool in dynamic software ecosystems, where project scopes and challenges continually morph. However, the effectiveness of ML in SCE hinges on the quality and comprehensiveness of the training data. Sparse or biased datasets can lead the algorithms astray, underlining the importance of robust data curation and validation. Neural Networks Venturing deeper into the intricate neural pathways of computational modeling, Neural Networks (NN) stand as a testament to the biomimetic aspirations of artificial intelligence. Structured to mimic the neuronal intricacies of the human brain, NNs deploy layered architectures of nodes and connections to process and interpret information. In the realm of Software Cost Estimation, Neural Networks weave intricate patterns from historical data, capturing nonlinear relationships often elusive to traditional models [9], [10]. Their capacity for deep learning, especially with the advent of multi-layered architectures, holds immense promise for SCE’s complex datasets. Yet, the very depth that lends NNs their power can sometimes shroud them in opacity. Their "black box" nature, combined with susceptibility to overfitting, necessitates meticulous training and validation to ensure reliable estimations. Also, the recent discovery of ”Grokking” suggests that this field could yield fascinating new findings [11]. Genetic Algorithms Drawing inspiration from the very fabric of life, Genetic Algorithms (GAs) transpose the principles of evolution onto computational canvases. GAs approach Software Cost Estimation as an optimization puzzle, seeking the fittest solutions through processes mimicking natural selection, crossover, and mutation. By initiating with a diverse population of estimation strategies and iteratively refining them through evolutionary cycles, GAs converge towards more optimal estimation models. Their inherent adaptability and explorative nature make them well-suited for SCE landscapes riddled with local optima [12]. However, the stochastic essence of GAs means that their results, while generally robust, may not always guarantee absolute consistency across runs. Calibration of their evolutionary parameters remains crucial to strike a balance between exploration and exploitation. Agile Estimation Techniques Agile methodologies, originally formulated to address the challenges of traditional software development processes, introduced a paradigm shift in how projects are managed and products are delivered. Integral to this approach is the iterative nature of development and the emphasis on collaboration among cross-functional teams. This collaborative approach extends to the estimation processes in Agile. Instead of trying to foresee the entirety of a project’s complexity at its outset, Agile estimation techniques are designed to evolve, adapting as the team gathers more information. Story Points Instead of estimating tasks in hours or days, many Agile teams use story points to estimate the relative effort required for user stories. Story points consider the complexity, risk, and effort of the task. By focusing on relative effort rather than absolute time, teams avoid the pitfalls of under or over-estimating due to unforeseen challenges or dependencies. Over several iterations, teams develop a sense of their ”velocity” — the average number of story points they complete in an iteration — which aids in forecasting [13]. Planning Poker One of the most popular Agile estimation techniques is Planning Poker. Team members, often inclusive of developers, testers, and product owners, collaboratively estimate the effort required for specific tasks or user stories. Using a set of cards with pre-defined values (often Fibonacci sequence numbers), each member selects a card representing their estimate. After revealing their cards simultaneously, discrepancies in estimates are discussed, leading to consensus [14], [15]. The beauty of Planning Poker lies in its ability to combine individual expert opinions and arrive at an estimate that reflects the collective wisdom of the entire team. The process also uncovers potential challenges or uncertainties, leading to more informed decision-making. Continuous Reevaluation A hallmark of Agile estimation is its iterative nature. As teams proceed through sprints or iterations, they continually reassess and adjust their estimates based on new learnings and the actual effort expended in previous cycles. This iterative feedback loop allows for more accurate forecasting as the project progresses [14]. Hybrid Model Approach We aim to present a novel model that incorporates both expert judgment and algorithmic approaches. While considering the expert approach, it is worth noting that it may involve subjective evaluations, possibly exhibiting inconsistencies amongst different experts. Besides, its dependence on the experience and availability of experts has the potential to introduce biases due to cognitive heuristics and over-reliance on recent experiences. On the other hand, an algorithmic approach may require significant expertise to be applied correctly and may focus on certain parameters, such as the number of lines of code, which may not be relevant. Therefore, the aim here is to propose a model that is independent of the programming language and considers multiple factors, such as project, hardware, and personnel attributes. Task Discretization In the constantly evolving field of software engineering, the practice of task discretization has become firmly established as a mainstay [16]. This approach stresses the importance of breaking down larger software objectives into manageable, bite-sized units. By acknowledging the inherent discreetness of software components — from screens and APIs to SQL scripts — a methodical breakdown emerges as a practical requirement [17]. Such an approach allows you to define your software in consistent modules, composed of consistent elements. It is crucial to have homogeneous elements for estimation, to enable the estimation team to easily understand what they are estimating and avoid the need to adapt to the elements. Those elements will be referred to as ”tasks” throughout the paper. Also, addressing tasks at an individual level improves accuracy, while the detail it provides promotes flexibility, allowing for iterative adjustments that accommodate a project’s changing requirements. Such an approach guarantees that each component’s distinct characteristics are appropriately and independently considered. This method of discretization has several advantages. Addressing tasks at an individual level enhances accuracy, while the granularity it brings forth promotes flexibility, enabling iterative adjustments that accommodate a project’s fluid requirements. Conversely, a detailed comprehension of each task’s complexities enables the prudent allocation of resources and fine-tuning skills to where they are most necessary. Nevertheless, this level of detail is not without drawbacks. Despite providing accuracy, deconstructing tasks into their constituent parts may result in administrative challenges, particularly in extensive projects. The possibility of neglecting certain tasks, albeit minor, is ever-present. Moreover, an excessively detailed approach can sometimes obscure wider project aims, resulting in decision-making delays, which is often referred to as ”analysis paralysis”. Dual-Factor Qualification System and Effort Calculation Task Discretization With the delineation of tasks exhibiting homogeneous attributes, it becomes imperative to pinpoint generic determinants for allocating appropriate effort. Upon meticulous scrutiny, two pivotal factors have been discerned: Complexity and Volumetry [1], [18]. Complexity serves as a metric to gauge the requisite technical acumen for task execution. For instance, within a user interface, the incorporation of a dynamic table may warrant the classification of the task as possessing high complexity due to its intricate requirements. Volumetry delineates the volume or quantum of work involved. To illustrate, in the context of a user interface, the presence of an extensive forty-field form might be indicative of a task with significant volumetry due to the sheer magnitude of its components. Both Complexity and Volumetry are in the interval [1 − 5] and must be integers. Now we will define the Effort (E), which is calculated as follows: E = C ∗V Where C is the Complexity and V the Volumetry. We utilize multiplication in this calculation in order to establish a connection between high Complexity and high Volumetry. This enables us to account for potential risks when both evaluation criteria increase simultaneously while maintaining accuracy for tasks with lower coefficients. By using a simple product of the two intervals of C and V, we obtain the following possibilities for E: [1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 15, 16, 20, 25] Abacus System Now that an effort has been obtained, the corresponding number of days can be identified for each effort value. This stage is critical and requires the intervention of an expert with knowledge of the target architecture and technologies. However, the model permits this crucial resource to intervene only once when establishing these values. Use of an Algorithm To establish these values, we propose using an algorithm to enhance accuracy and prevent errors.It can be utilized to simulate data sets using three distinct models and two starting criteria: The maximal number of days (which is linked with an effort of 25) The gap padding between values We utilized three distinct models to enable the experts and estimation team to select from different curve profiles that may yield varied characteristics, such as precision, risk assessment, and padding size, for ideal adaptation to the requirements. Three distinct mathematical models were hypothesized to explicate the relationship: linear, quadratic, and exponential. Each model postulates a unique behavior of effort-to-days transformation: The Linear Model postulates a direct proportionality between effort and days. The Quadratic Model envisages an accelerated growth rate, invoking polynomial mathematics. The Exponential Model projects an exponential surge, signifying steep escalation for higher effort values. Those models can be adjusted to more accurately meet estimation requirements. Finally, we obtain the following code: Python import numpy as np import matplotlib.pyplot as plt import pandas as pd # Importing pandas for tabular display # Fixed effort values efforts = np.array([1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 15, 16, 20, 25]) # Parameters Max_days = 25 Step_Days = 0.25 def linear_model(effort, max_effort, max_days, step_days): slope = (max_days - step_days) / max_effort return slope * effort + step_days - slope def quadratic_model(effort, max_effort, max_days, step_days): scale = (max_days - step_days) / (max_effort + 0.05 * max_effort**2) return scale * (effort + 0.05 * effort**2) def exponential_model(effort, max_effort, max_days, step_days): adjusted_max_days = max_days - step_days + 1 base = np.exp(np.log(adjusted_max_days) / max_effort) return step_days + base ** effort - 1 def logarithmic_model(effort, max_effort, max_days, step_days): scale = (max_days - step_days) / np.log(max_effort + 1) return scale * np.log(effort + 1) # Rounding to nearest step def round_to_step(value, step): return round(value / step) * step linear_days = np.array([round_to_step(linear_model(e, efforts[-1], Max_days, Step_Days), Step_Days) for e in efforts]) quadratic_days = np.array([round_to_step(quadratic_model(e, efforts[-1], Max_days, Step_Days), Step_Days) for e in efforts]) exponential_days = np.array([round_to_step(exponential_model(e, efforts[-1], Max_days, Step_Days), Step_Days) for e in efforts]) logarithmic_days = np.array([round_to_step(logarithmic_model(e, efforts[-1], Max_days, Step_Days), Step_Days) for e in efforts]) # Plot plt.figure(figsize=(10,6)) plt.plot(efforts, linear_days, label="Linear Model", marker='o') plt.plot(efforts, quadratic_days, label="Quadratic Model", marker='x') plt.plot(efforts, exponential_days, label="Exponential Model", marker='.') plt.plot(efforts, logarithmic_days, label="Logarithmic Model", marker='+') plt.xlabel("Effort") plt.ylabel("Days") plt.title("Effort to Days Estimation Models") plt.legend() plt.grid(True) plt.show() # Displaying data in table format df = pd.DataFrame({ 'Effort': efforts, 'Linear Model (Days)': linear_days, 'Quadratic Model (Days)': quadratic_days, 'Exponential Model (Days)': exponential_days, 'Logarithmic Model (Days)': logarithmic_days }) display(df) Listing 1. Days generation model code, Python Simulations Let us now examine a practical example of chart generation. As previously stated in the code, the essential parameters ”Step Days” and ”Max days” have been set to 0.25 and 25, respectively. The results generated by the three models using these parameters are presented below. Figure 1: Effort to days estimation models - Data Below is a graphical representation of these results: Figure 2: Effort to days estimation models — graphical representation The graph enables us to distinguish the variation in ”compressions” amongst the three models, which will yield distinct traits, including accuracy in minimal forces or strong association among values. Specific Use Case in Large Legacy Migration Projects Now that the model has been described, a specific application will be proposed in the context of a migration project. It is believed that this model is well-suited to projects of this kind, where teams are confronted with a situation that appears unsuited to the existing standard model, as explained in the first part. Importance of SCE in Legacy Migration Often, migration projects are influenced by their cost. The need to migrate is typically caused by factors including: More frequent regressions and side effects Difficulty in locating new resources for outdated technologies Specialist knowledge concentration Complexity in integrating new features Performance issues All potential causes listed above increase cost and/or risk. It may be necessary to consider the migration of the problematic technological building block(s). Implementation depends mainly on the cost incurred, necessitating an accurate estimate [19]. However, it is important to acknowledge that during an organization’s migration process, technical changes must be accompanied by human and organizational adjustments. Frequently, after defining the target architecture and technologies, the organization might lack the necessary experts in these fields. This can complicate the ”Expert Judgement” approach. Algorithmic approaches do not appear to be suitable either, as they necessitate knowledge and mastery but also do not necessarily consider all the subtleties that migrations may require in terms of redrawing the components to be migrated. Additionally, the number of initial lines of code is not consistently a reliable criterion. Finally, AI-based methodologies seem to still be in their formative stages and may be challenging to implement and master for these organizations. That is why our model appears suitable, as it enables present teams to quantify the effort and then seek advice from an expert in the target technologies to create the estimate, thus obtaining an accurate figure. It is worth noting that this estimation merely encompasses the development itself and disregards the specification stages and associated infrastructurecosts. Application of the Hybrid Model We shall outline the entire procedure for implementing our estimation model. The process comprises three phases: Initialization, Estimation, and Finalization. Initialization During this phase, the technology building block to be estimated must first be deconstructed. It needs to be broken down into sets of unified tasks. For example, an application with a GWT front-end calling an AS400 database could be broken down into two main sets: Frontend: Tasks are represented by screens. Backend: Tasks are represented by APIs. We can then put together the estimation team. It does not need to be a technical expert in the target technology but should be made up of resources from the existing project, preferably a technical/functional pair, who can assess the complementarity of each task with the two visions. This team will be able to start listing the tasks for the main assemblies identified during the discretization process. Estimation We now have a team ready to assign Complexity and Volumetry values to the set of tasks to be identified. In parallel with this association work, we can begin to set values for the days to be associated with the effort. This work may require an expert in the target technologies and also members of the estimation team to quantify some benchmark values on the basis of which the expert can take a critical look and extend the results to the whole chart. At the end of this phase, we have a days/effort correspondence abacus and a list of tasks with an associated effort value. Finalization The final step is to calculate the conversion between effort and days using the abacus to obtain a total number of days. Once the list of effort values has been obtained, a risk analysis canbe carried out using the following criteria: The standard deviation of the probability density curve of efforts Analysis of whether certain ”zones” of the components concentrate high effort values The number of tasks with an effort value greater than 16 Depending on these criteria, specific measures can be taken in restricted areas. Results and Findings Finally, we arrive at the following process, which provides a hybrid formalization between expert judgment and algorithmic analysis. The method seems particularly well suited to the needs of migration projects, drawing on accessible resources and not requiring a high level of expertise. Figure 3: Complete process of the hybrid model Another representation, based on the nature of elements, could be the following: Figure 4: Complete process of the hybrid model Conclusion In conclusion, our model presents a practical and flexible approach for estimating the costs involved in large legacy migration projects. By combining elements of expert judgment with a structured, algorithmic analysis, this model addresses the unique challenges that come with migrating outdated or complex systems. It recognizes the importance of accurately gauging the effort and costs, considering not just the technical aspects but also the human and organizational shifts required. The three-phase process — Initialization, Estimation, and Finalization — ensures a comprehensive evaluation, from breaking down the project into manageable tasks to conducting a detailed risk analysis. This hybrid model is especially beneficial for teams facing the daunting task of migration, providing a pathway to make informed decisions and prepare effectively for the transition. Through this approach, organizations can navigate the intricacies of migration, ensuring a smoother transition to modern, more efficient systems. In light of the presented discussions and findings, it becomes evident that legacy migration projects present a unique set of challenges that can’t be addressed by conventional software cost estimation methods alone. The hybrid model as proposed serves as a promising bridge between the more heuristic expert judgment approach and the more structured algorithmic analysis, offering a balanced and adaptive solution. The primary strength of this model lies in its adaptability and its capacity to leverage both institutional knowledge and specific expertise in target technologies. Furthermore, the model’s ability to deconstruct a problem into sets of unified tasks and estimate with an appropriate level of granularity ensures its relevance across a variety of application scenarios. While the current implementation of the hybrid model shows potential, future research and improvements can drive its utility even further: Empirical validation: As with all models, empirical validation on a diverse set of migration projects is crucial. This would not only validate its effectiveness but also refine its accuracy.(We are already working on it.) Integration with AI: Although AI-based methodologies for software cost estimation are still nascent, their potential cannot be overlooked. Future iterations of the hybrid model could integrate machine learning for enhanced predictions, especially when large datasets from past projects are available. Improved risk analysis: The proposed risk analysis criteria provide a solid starting point. However, more sophisticated risk models, which factor in unforeseen complexities and uncertainties inherent to migration projects, could be integrated into the model. Tooling and automation: Developing tools that can semiautomate the process described would make the model more accessible and easier to adopt by organizations. In conclusion, the hybrid model presents a notable advancement in the realm of software cost estimation, especially for legacy migration projects. However, as with all models, it’s an evolving entity, and continued refinement will only enhance its applicability and effectiveness. References [1] Barry W. Boehm. Software engineering economics. IEEE Transactions on Software Engineering, SE-7(1):4–21, 1981. [2] Barry W. Boehm, Chris Abts, A. Winsor Brown, Sunita Chulani, Bradford K. Clark, Ellis Horowitz, Ray Madachy, Donald J. Reifer, and Bert Steece. Cost models for future software life cycle processes: Cocomo 2.0. Annals of Software Engineering, 1(1):57–94, 2000. [3] International Function Point Users Group (IFPUG). Function Point Counting Practices Manual. IFPUG, 2000. FPCPM. [4] L.H. Putnam. A general empirical solution to the macro software sizing and estimating problem. IEEE Transactions on Software Engineering, 4:345–361, 1978. [5] R.T. Hughes. Expert judgment as an estimating method. Information and Software Technology, 38(2):67–75, 1996. [6] Christopher Rush and Rajkumar Roy. Expert judgment in cost estimating: Modelling the reasoning process. Unknown Journal Name. [7] N. Dalkey. An experimental study of group opinion: the Delphi method. Futures, 1(5):408–426, 1969. [8] Yibeltal Assefa, Fekerte Berhanu, Asnakech Tilahun, and Esubalew Alemneh. Software effort estimation using machine learning algorithm. In 2022 International Conference on Information and Communication Technology for Development for Africa (ICT4DA), pages 163–168, 2022. [9] A. Venkatachalam. Software cost estimation using artificial neural networks. In Proc. Int. Conf. Neural Netw. (IJCNN-93-Nagoya Japan), volume 1, pages 987–990, Oct 1993. [10] R. Poonam and S. Jain. Enhanced software effort estimation using multi-layered feed forward artificial neural network technique. Procedia Computer Science, 89:307–312, 2016. [11] Alethea Power, Yuri Burda, Harri Edwards, et al. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022. [12] B.K. Singh and A.K. Misra. Software effort estimation by genetic algorithm tuned parameters of modified constructive cost model for NASA software projects. International Journal of Computer Applications, 59:22–26, 2012. [13] K. Hrvoje and S. Gotovac. Estimating software development effort using Bayesian networks. In 2015 23rd International Conference on Software, Telecommunications and Computer Networks, pages 229–233, Split, Croatia, September 16–18 2015. [14] M. Cohn. Agile Estimating and Planning. Prentice Hall PTR, 2005. [15] Saurabh Bilgaiyan, Santwana Sagnika, Samaresh Mishra, and Madhabananda Das. A systematic review on software cost estimation in agile software development. Journal of Engineering Science and Technology Review, 10(4):51–64, 2017. [16] S. McConnell. Software Estimation: Demystifying the Black Art. Microsoft Press, 2006. [17] C. Szyperski. Component Software: Beyond Object-Oriented Programming. Addison-Wesley, 2nd edition, 2002. [18] N. E. Fenton and S. L. Pfleeger. Software Metrics: A Rigorous and Practical Approach. PWS Publishing Co., 1997. [19] Harry M. Sneed and Chris Verhoef. Cost-driven software migration: An experience report. Software: Practice and Experience, 2020. By Pier-Jean MALANDRINO Culture and Methodologies Agile Career Development Methodologies Team Management Culture, Community, Creation, Calling March 5, 2024 by Leon Adato Elevate Your Terminal Game: Hacks for a Productive Workspace March 1, 2024 by Pradeep Gopalgowda Operational Testing Tutorial: Comprehensive Guide With Best Practices May 16, 2023 by Harshit Paul Data Engineering AI/ML Big Data Databases IoT Unveiling the Clever Way: Converting XML to Relational Data March 5, 2024 by Manas Sadangi CORE Unpacking Our Findings From Assessing Numerous Infrastructures (Part 2) March 5, 2024 by Komal J Prabhakar Explainable AI: Making the Black Box Transparent May 16, 2023 by Yifei Wang Software Design and Architecture Cloud Architecture Integration Microservices Performance Ensuring Security and Compliance: A Detailed Guide to Testing the OAuth 2.0 Authorization Flow in Python Web Applications March 5, 2024 by Vijay Panwar Unpacking Our Findings From Assessing Numerous Infrastructures (Part 2) March 5, 2024 by Komal J Prabhakar Low Code vs. Traditional Development: A Comprehensive Comparison May 16, 2023 by Tien Nguyen Coding Frameworks Java JavaScript Languages Tools Unveiling the Clever Way: Converting XML to Relational Data March 5, 2024 by Manas Sadangi CORE Ensuring Security and Compliance: A Detailed Guide to Testing the OAuth 2.0 Authorization Flow in Python Web Applications March 5, 2024 by Vijay Panwar Scaling Event-Driven Applications Made Easy With Sveltos Cross-Cluster Configuration May 15, 2023 by Gianluca Mardente Testing, Deployment, and Maintenance Deployment DevOps and CI/CD Maintenance Monitoring and Observability Unpacking Our Findings From Assessing Numerous Infrastructures (Part 2) March 5, 2024 by Komal J Prabhakar Automating Django Deployments: Integrating CI/CD With GitHub Actions and Heroku March 4, 2024 by Vijay Panwar Low Code vs. Traditional Development: A Comprehensive Comparison May 16, 2023 by Tien Nguyen Popular AI/ML Java JavaScript Open Source O11y Guide, Cloud Native Observability Pitfalls: The Protocol Jungle March 4, 2024 by Eric D. Schabell CORE Angular's Evolution: Embracing Change in the Web Development Landscape March 4, 2024 by deji adesoga CORE Five IntelliJ Idea Plugins That Will Change the Way You Code May 15, 2023 by Toxic Dev ABOUT US About DZone Send feedback Careers Sitemap ADVERTISE Advertise with DZone CONTRIBUTE ON DZONE Article Submission Guidelines Become a Contributor Core Program Visit the Writers' Zone LEGAL Terms of Service Privacy Policy CONTACT US 3343 Perimeter Hill Drive Suite 100 Nashville, TN 37211 support@dzone.com Let's be friends: